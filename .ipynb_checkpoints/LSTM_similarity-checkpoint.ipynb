{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GeForce GTX 980 Ti (CNMeM is disabled, CuDNN 4007)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.preprocessing import text\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "token_pattern=r\"(?u)\\b\\w\\w+\\b\"\n",
    "def build_tokenizer():\n",
    "    \"\"\"Return a function that splits a string into a sequence of tokens\"\"\"\n",
    "    pattern = re.compile(token_pattern)\n",
    "    return lambda doc: pattern.findall(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readData(src):\n",
    "    b1 = []\n",
    "    b2 = []\n",
    "    with open(src) as p:\n",
    "        for i, line in enumerate(p):\n",
    "            s = line.split('\\t')\n",
    "            if len(s) == 2:\n",
    "                b1.append(s[0])\n",
    "                b2.append(s[1][:-1]) #remove \\n\n",
    "                lines = i + 1\n",
    "    return b1, b2, lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readGs(src):\n",
    "    b = []\n",
    "    with open(src) as p:\n",
    "        for i, line in enumerate(p):\n",
    "            b.append(round(float(line),0))\n",
    "            lines = i + 1\n",
    "    return b, lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read 2012 trainning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750\n",
      "750\n",
      "734\n",
      "2234\n"
     ]
    }
   ],
   "source": [
    "msr = './dataset/STS2012-train/STS.input.MSRpar.txt'\n",
    "msrvid = './dataset/STS2012-train/STS.input.MSRvid.txt'\n",
    "smt = './dataset/STS2012-train/STS.input.SMTeuroparl.txt'\n",
    "b1_12_1, b2_12_1, l_12_1 = readData(msr)\n",
    "print l_12_1\n",
    "b1_12_2, b2_12_2, l_12_2 = readData(msrvid)\n",
    "print l_12_2\n",
    "b1_12_3, b2_12_3, l_12_3 = readData(smt)\n",
    "print l_12_3\n",
    "lines_12 = l_12_1 + l_12_2 + l_12_3\n",
    "b1_12_train = b1_12_1 + b1_12_2 + b1_12_3\n",
    "b2_12_train = b2_12_1 + b2_12_2 + b2_12_3\n",
    "print lines_12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "msr_gs = './dataset/STS2012-train/STS.gs.MSRpar.txt'\n",
    "msr_gs_vid = './dataset/STS2012-train/STS.gs.MSRvid.txt'\n",
    "smt_gs = './dataset/STS2012-train/STS.gs.SMTeuroparl.txt'\n",
    "b_12_train = readGs(msr_gs)[0]\n",
    "b_12_train = b_12_train + readGs(msr_gs_vid)[0]\n",
    "b_12_train = b_12_train + readGs(smt_gs)[0]\n",
    "print len(b_12_train) == len(b1_12_train) == len(b2_12_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read 2012 test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750\n",
      "750\n",
      "459\n",
      "750\n",
      "399\n",
      "3108\n"
     ]
    }
   ],
   "source": [
    "msr_test = './dataset/STS2012-test/STS.input.MSRpar.txt'\n",
    "vid_test = './dataset/STS2012-test/STS.input.MSRvid.txt'\n",
    "smt_test = './dataset/STS2012-test/STS.input.SMTeuroparl.txt'\n",
    "surprise_test = './dataset/STS2012-test/STS.input.surprise.OnWN.txt'\n",
    "surprise2_test = './dataset/STS2012-test/STS.input.surprise.SMTnews.txt'\n",
    "b1_12_1t, b2_12_1t, l_12_1t = readData(msr_test)\n",
    "print l_12_1t\n",
    "b1_12_2t, b2_12_2t, l_12_2t = readData(vid_test)\n",
    "print l_12_2t\n",
    "b1_12_3t, b2_12_3t, l_12_3t = readData(smt_test)\n",
    "print l_12_3t\n",
    "b1_12_4t, b2_12_4t, l_12_4t = readData(surprise_test)\n",
    "print l_12_4t\n",
    "b1_12_5t, b2_12_5t, l_12_5t = readData(surprise2_test)\n",
    "print l_12_5t\n",
    "lines = l_12_1t + l_12_2t + l_12_3t + l_12_4t + l_12_5t\n",
    "b1_12_test = b1_12_1t + b1_12_2t + b1_12_3t + b1_12_4t + b1_12_5t\n",
    "b2_12_test = b2_12_1t + b2_12_2t + b2_12_3t + b2_12_4t + b2_12_5t\n",
    "print lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "msr_test_gs = './dataset/STS2012-test/STS.gs.MSRpar.txt'\n",
    "vid_test_gs = './dataset/STS2012-test/STS.gs.MSRvid.txt'\n",
    "smt_test_gs = './dataset/STS2012-test/STS.gs.SMTeuroparl.txt'\n",
    "surprise_test_gs = './dataset/STS2012-test/STS.gs.surprise.OnWN.txt'\n",
    "surprise2_test_gs = './dataset/STS2012-test/STS.gs.surprise.SMTnews.txt'\n",
    "b_12_test = readGs(msr_test_gs)[0]\n",
    "b_12_test = b_12_test + readGs(vid_test_gs)[0]\n",
    "b_12_test = b_12_test + readGs(smt_test_gs)[0]\n",
    "b_12_test = b_12_test + readGs(surprise_test_gs)[0]\n",
    "b_12_test = b_12_test + readGs(surprise2_test_gs)[0]\n",
    "print len(b_12_test) == len(b1_12_test) == len(b2_12_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read 2014 test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450\n",
      "300\n",
      "750\n",
      "750\n",
      "750\n",
      "750\n",
      "3750\n"
     ]
    }
   ],
   "source": [
    "t14_f = './dataset/STS2014-test/STS.input.deft-forum.txt'\n",
    "t14_n = './dataset/STS2014-test/STS.input.deft-news.txt'\n",
    "t14_h = './dataset/STS2014-test/STS.input.headlines.txt'\n",
    "t14_i = './dataset/STS2014-test/STS.input.images.txt'\n",
    "t14_o = './dataset/STS2014-test/STS.input.OnWN.txt'\n",
    "t14_t = './dataset/STS2014-test/STS.input.tweet-news.txt'\n",
    "b1_14_1t, b2_14_1t, l_14_1t = readData(t14_f)\n",
    "print l_14_1t\n",
    "b1_14_2t, b2_14_2t, l_14_2t = readData(t14_n)\n",
    "print l_14_2t\n",
    "b1_14_3t, b2_14_3t, l_14_3t = readData(t14_h)\n",
    "print l_14_3t\n",
    "b1_14_4t, b2_14_4t, l_14_4t = readData(t14_i)\n",
    "print l_14_4t\n",
    "b1_14_5t, b2_14_5t, l_14_5t = readData(t14_o)\n",
    "print l_14_5t\n",
    "b1_14_6t, b2_14_6t, l_14_6t = readData(t14_t)\n",
    "print l_14_6t\n",
    "b1_14_test = b1_14_1t + b1_14_2t + b1_14_3t + b1_14_4t + b1_14_5t + b1_14_6t\n",
    "b2_14_test = b2_14_1t + b2_14_2t + b2_14_3t + b2_14_4t + b2_14_5t + b2_14_6t\n",
    "lines = l_14_1t + l_14_2t + l_14_3t + l_14_4t + l_14_5t + l_14_6t\n",
    "print lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "t14_f_gs = './dataset/STS2014-test/STS.gs.deft-forum.txt'\n",
    "t14_n_gs = './dataset/STS2014-test/STS.gs.deft-news.txt'\n",
    "t14_h_gs = './dataset/STS2014-test/STS.gs.headlines.txt'\n",
    "t14_i_gs = './dataset/STS2014-test/STS.gs.images.txt'\n",
    "t14_o_gs = './dataset/STS2014-test/STS.gs.OnWN.txt'\n",
    "t14_t_gs = './dataset/STS2014-test/STS.gs.tweet-news.txt'\n",
    "b_14_test = readGs(t14_f_gs)[0]\n",
    "b_14_test = b_14_test + readGs(t14_n_gs)[0]\n",
    "b_14_test = b_14_test + readGs(t14_h_gs)[0]\n",
    "b_14_test = b_14_test + readGs(t14_i_gs)[0]\n",
    "b_14_test = b_14_test + readGs(t14_o_gs)[0]\n",
    "b_14_test = b_14_test + readGs(t14_t_gs)[0]\n",
    "print len(b_14_test) == len(b1_14_test) == len(b2_14_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add all years train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "b1 = b1_12_train + b1_12_test + b1_14_test\n",
    "b2 = b2_12_train + b2_12_test + b2_14_test\n",
    "y_train = b_12_train + b_12_test + b_14_test\n",
    "print len(b1) == len(b2) == len(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read 2015 train data and 2013 test data as validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1570\n"
     ]
    }
   ],
   "source": [
    "val_f = './dataset/STS2015-train/STS.input.answers-forum.txt'\n",
    "val_s = './dataset/STS2015-train/STS.input.answers-students.txt'\n",
    "val_b = './dataset/STS2015-train/STS.input.belief.txt'\n",
    "val_h = './dataset/STS2015-train/STS.input.headlines.txt'\n",
    "val_i = './dataset/STS2015-train/STS.input.images.txt'\n",
    "val_fn = './dataset/STS2013-test/STS.input.FNWN.txt'\n",
    "val_he = './dataset/STS2013-test/STS.input.headlines.txt'\n",
    "val_on = './dataset/STS2013-test/STS.input.OnWN.txt'\n",
    "v1_15_1, v2_15_1, l_15_1 = readData(val_f)\n",
    "v1_15_2, v2_15_2, l_15_2 = readData(val_s)\n",
    "v1_15_3, v2_15_3, l_15_3 = readData(val_b)\n",
    "v1_15_4, v2_15_4, l_15_4 = readData(val_h)\n",
    "v1_15_5, v2_15_5, l_15_5 = readData(val_i)\n",
    "v1_13_1, v2_13_1, l_13_1 = readData(val_fn)\n",
    "v1_13_2, v2_13_2, l_13_2 = readData(val_he)\n",
    "v1_13_3, v2_13_3, l_13_3 = readData(val_on)\n",
    "lines = l_15_1 + l_15_2 + l_15_3 + l_15_4 + l_15_5 + l_13_1 + l_13_2 + l_13_3\n",
    "v1 = v1_15_1 + v1_15_2 + v1_15_3 + v1_15_4 + v1_15_5 + v1_13_1 + v1_13_2 + v1_13_3\n",
    "v2 = v2_15_1 + v2_15_2 + v2_15_3 + v2_15_4 + v2_15_5 + v2_13_1 + v2_13_2 + v2_13_3\n",
    "print lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1570\n"
     ]
    }
   ],
   "source": [
    "val_gs_f = './dataset/STS2015-train/STS.gs.answers-forum.txt'\n",
    "val_gs_s = './dataset/STS2015-train/STS.gs.answers-students.txt'\n",
    "val_gs_b = './dataset/STS2015-train/STS.gs.belief.txt'\n",
    "val_gs_h = './dataset/STS2015-train/STS.gs.headlines.txt'\n",
    "val_gs_i = './dataset/STS2015-train/STS.gs.images.txt'\n",
    "val_gs_fn = './dataset/STS2013-test/STS.gs.FNWN.txt'\n",
    "val_gs_he = './dataset/STS2013-test/STS.gs.headlines.txt'\n",
    "val_gs_on = './dataset/STS2013-test/STS.gs.OnWN.txt'\n",
    "y_val = readGs(val_gs_f)[0]\n",
    "y_val = y_val + readGs(val_gs_s)[0]\n",
    "y_val = y_val + readGs(val_gs_b)[0]\n",
    "y_val = y_val + readGs(val_gs_h)[0]\n",
    "y_val = y_val + readGs(val_gs_i)[0]\n",
    "y_val = y_val + readGs(val_gs_fn)[0]\n",
    "y_val = y_val + readGs(val_gs_he)[0]\n",
    "y_val = y_val + readGs(val_gs_on)[0]\n",
    "print len(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read 2015 test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "1500\n",
      "2000\n",
      "1500\n",
      "1500\n"
     ]
    }
   ],
   "source": [
    "test_f = './dataset/STS2015-test/STS.input.answers-forums.txt'\n",
    "b1_test_f, b2_test_f, lines_f = readData(test_f)\n",
    "test_s = './dataset/STS2015-test/STS.input.answers-students.txt'\n",
    "b1_test_s, b2_test_s, lines_s = readData(test_s)\n",
    "test_b = './dataset/STS2015-test/STS.input.belief.txt'\n",
    "b1_test_b, b2_test_b, lines_b = readData(test_b)\n",
    "test_h = './dataset/STS2015-test/STS.input.headlines.txt'\n",
    "b1_test_h, b2_test_h, lines_h = readData(test_h)\n",
    "test_i = './dataset/STS2015-test/STS.input.images.txt'\n",
    "b1_test_i, b2_test_i, lines_i = readData(test_i)\n",
    "print lines_f\n",
    "print lines_s\n",
    "print lines_b\n",
    "print lines_h\n",
    "print lines_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14758\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectors = vectorizer.fit_transform(b1 + b2)\n",
    "# vectors_test = vectorizer.transform(b1_test + b2_test)\n",
    "vectors.shape\n",
    "vocab = vectorizer.get_feature_names()\n",
    "print len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:7: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:13: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    }
   ],
   "source": [
    "tokenize = build_tokenizer()\n",
    "X_train1 = []\n",
    "X_train2 = []\n",
    "for seq in b1:\n",
    "    s = []\n",
    "    for word in tokenize(seq):\n",
    "        if word in vocab:\n",
    "            s.append(vectorizer.vocabulary_[word] + 1)\n",
    "    X_train1.append(s)\n",
    "for seq in b2:\n",
    "    s = []\n",
    "    for word in tokenize(seq):\n",
    "        if word in vocab:\n",
    "            s.append(vectorizer.vocabulary_[word] + 1)\n",
    "    X_train2.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9092\n",
      "9092\n"
     ]
    }
   ],
   "source": [
    "print len(X_train1)\n",
    "print len(X_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:6: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:12: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:34: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:40: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:48: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:54: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    }
   ],
   "source": [
    "X_test1_f = []\n",
    "X_test2_f = []\n",
    "for seq in b1_test_f:\n",
    "    s = []\n",
    "    for word in tokenize(seq):\n",
    "        if word in vocab:\n",
    "            s.append(vectorizer.vocabulary_[word] + 1)\n",
    "    X_test1_f.append(s)\n",
    "for seq in b2_test_f:\n",
    "    s = []\n",
    "    for word in tokenize(seq):\n",
    "        if word in vocab:\n",
    "            s.append(vectorizer.vocabulary_[word] + 1)\n",
    "    X_test2_f.append(s)\n",
    "X_test1_s = []\n",
    "X_test2_s = []\n",
    "for seq in b1_test_s:\n",
    "    s = []\n",
    "    for word in tokenize(seq):\n",
    "        if word in vocab:\n",
    "            s.append(vectorizer.vocabulary_[word] + 1)\n",
    "    X_test1_s.append(s)\n",
    "for seq in b2_test_s:\n",
    "    s = []\n",
    "    for word in tokenize(seq):\n",
    "        if word in vocab:\n",
    "            s.append(vectorizer.vocabulary_[word] + 1)\n",
    "    X_test2_s.append(s)\n",
    "X_test1_b = []\n",
    "X_test2_b = []\n",
    "for seq in b1_test_b:\n",
    "    s = []\n",
    "    for word in tokenize(seq):\n",
    "        if word in vocab:\n",
    "            s.append(vectorizer.vocabulary_[word] + 1)\n",
    "    X_test1_b.append(s)\n",
    "for seq in b2_test_b:\n",
    "    s = []\n",
    "    for word in tokenize(seq):\n",
    "        if word in vocab:\n",
    "            s.append(vectorizer.vocabulary_[word] + 1)\n",
    "    X_test2_b.append(s)\n",
    "X_test1_h = []\n",
    "X_test2_h = []\n",
    "for seq in b1_test_h:\n",
    "    s = []\n",
    "    for word in tokenize(seq):\n",
    "        if word in vocab:\n",
    "            s.append(vectorizer.vocabulary_[word] + 1)\n",
    "    X_test1_h.append(s)\n",
    "for seq in b2_test_h:\n",
    "    s = []\n",
    "    for word in tokenize(seq):\n",
    "        if word in vocab:\n",
    "            s.append(vectorizer.vocabulary_[word] + 1)\n",
    "    X_test2_h.append(s)\n",
    "X_test1_i = []\n",
    "X_test2_i = []\n",
    "for seq in b1_test_i:\n",
    "    s = []\n",
    "    for word in tokenize(seq):\n",
    "        if word in vocab:\n",
    "            s.append(vectorizer.vocabulary_[word] + 1)\n",
    "    X_test1_i.append(s)\n",
    "for seq in b2_test_i:\n",
    "    s = []\n",
    "    for word in tokenize(seq):\n",
    "        if word in vocab:\n",
    "            s.append(vectorizer.vocabulary_[word] + 1)\n",
    "    X_test2_i.append(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "1500\n",
      "2000\n",
      "1500\n",
      "1500\n"
     ]
    }
   ],
   "source": [
    "print len(X_test1_f)\n",
    "print len(X_test2_s)\n",
    "print len(X_test1_b)\n",
    "print len(X_test2_h)\n",
    "print len(X_test1_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1570\n",
      "1570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:6: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:12: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    }
   ],
   "source": [
    "X_val1 = []\n",
    "X_val2 = []\n",
    "for seq in v1:\n",
    "    s = []\n",
    "    for word in tokenize(seq):\n",
    "        if word in vocab:\n",
    "            s.append(vectorizer.vocabulary_[word] + 1)\n",
    "    X_val1.append(s)\n",
    "for seq in v2:\n",
    "    s = []\n",
    "    for word in tokenize(seq):\n",
    "        if word in vocab:\n",
    "            s.append(vectorizer.vocabulary_[word] + 1)\n",
    "    X_val2.append(s)\n",
    "print len(X_val1)\n",
    "print len(X_val2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('X_train shape:', (9092, 25))\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 25\n",
    "X_train1 = sequence.pad_sequences(X_train1, maxlen=MAX_LEN)\n",
    "X_train2 = sequence.pad_sequences(X_train2, maxlen=MAX_LEN)\n",
    "\n",
    "print('X_train shape:', X_train1.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('X_val shape:', (1570, 25))\n"
     ]
    }
   ],
   "source": [
    "X_val1 = sequence.pad_sequences(X_val1, maxlen=MAX_LEN)\n",
    "X_val2 = sequence.pad_sequences(X_val2, maxlen=MAX_LEN)\n",
    "\n",
    "print('X_val shape:', X_val1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('X_test1_f shape:', (2000, 25))\n",
      "('X_test2_f shape:', (2000, 25))\n"
     ]
    }
   ],
   "source": [
    "X_test1_f = sequence.pad_sequences(X_test1_f,  maxlen=MAX_LEN)\n",
    "X_test2_f = sequence.pad_sequences(X_test2_f,  maxlen=MAX_LEN)\n",
    "print('X_test1_f shape:', X_test1_f.shape)\n",
    "print('X_test2_f shape:', X_test2_f.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_train, y_val = [np_utils.to_categorical(x) for x in (y_train, y_val)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use pre_trained word2vec embedding for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done loading\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "wv = Word2Vec.load_word2vec_format(\"/home/tong/Documents/python/GoogleNews-vectors-negative300.bin.gz\", binary = True)\n",
    "print \"done\" + \" loading\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab_dim = 300 # dimensionality of your word vectors\n",
    "n_symbols = len(vocab) + 1 # adding 1 to account for 0th index (for masking)\n",
    "embedding_weights = np.random.rand(n_symbols,vocab_dim)\n",
    "for word in vocab:\n",
    "    if word in wv:\n",
    "        embedding_weights[vectorizer.vocabulary_[word] + 1,:] = wv[word]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14570\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print vectorizer.vocabulary_[\"woman\"] + 1\n",
    "print np.array_equal(wv['woman'], embedding_weights[vectorizer.vocabulary_[\"woman\"] + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Build complete\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Merge\n",
    "print('Build model...')\n",
    "encoder_a = Sequential()\n",
    "encoder_a.add(Embedding(n_symbols, vocab_dim, input_length=MAX_LEN, weights=[embedding_weights]))\n",
    "encoder_a.add(LSTM(vocab_dim, dropout_W=0.6, dropout_U=0.3))  # try using a GRU instead, for fun\n",
    "encoder_a.add(Dropout(0.6))\n",
    "\n",
    "encoder_b = Sequential()\n",
    "encoder_b.add(Embedding(n_symbols, vocab_dim, input_length=MAX_LEN, weights=[embedding_weights]))\n",
    "encoder_b.add(LSTM(vocab_dim, dropout_W=0.6, dropout_U=0.3)) \n",
    "encoder_b.add(Dropout(0.6))\n",
    "\n",
    "decoder = Sequential()\n",
    "decoder.add(Merge([encoder_a, encoder_b], mode='concat'))\n",
    "decoder.add(Dense(6, activation='softmax'))\n",
    "decoder.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "print('Build complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Layer is not connected. Did you forget to set \"input_shape\"?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-9b8d84c8d145>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mforwards_a\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout_W\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout_U\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mbackwards_a\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout_W\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout_U\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgo_backwards\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mmerged_a\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mforwards_a\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackwards_a\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'concat'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconcat_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mencoder_a\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmerged_a\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mencoder_a\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/keras/layers/core.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, layers, mode, concat_axis, dot_axes)\u001b[0m\n\u001b[0;32m    488\u001b[0m             \u001b[0minput_shapes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    489\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 490\u001b[1;33m                 \u001b[0moshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    491\u001b[0m                 \u001b[0moshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconcat_axis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m                 \u001b[0moshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/keras/layers/recurrent.pyc\u001b[0m in \u001b[0;36moutput_shape\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0moutput_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m         \u001b[0minput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreturn_sequences\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/keras/layers/core.pyc\u001b[0m in \u001b[0;36minput_shape\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    208\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_input_shape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 210\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Layer is not connected. Did you forget to set \"input_shape\"?'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mset_input_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: Layer is not connected. Did you forget to set \"input_shape\"?"
     ]
    }
   ],
   "source": [
    "from keras.layers import Merge\n",
    "print('Build model...')\n",
    "encoder_a = Sequential()\n",
    "encoder_a.add(Embedding(n_symbols, vocab_dim, input_length=MAX_LEN, weights=[embedding_weights]))\n",
    "forwards_a = LSTM(vocab_dim, dropout_W=0.5, dropout_U=0.1)\n",
    "backwards_a = LSTM(vocab_dim, dropout_W=0.5, dropout_U=0.1, go_backwards=True)\n",
    "merged_a = Merge([forwards_a, backwards_a], mode='concat', concat_axis=-1)\n",
    "encoder_a.add(merged_a)\n",
    "encoder_a.add(Dropout(0.5))\n",
    "\n",
    "encoder_b = Sequential()\n",
    "encoder_b.add(Embedding(n_symbols, vocab_dim, input_length=MAX_LEN, weights=[embedding_weights]))\n",
    "forwards_b = LSTM(vocab_dim, dropout_W=0.5, dropout_U=0.1)\n",
    "backwards_b = LSTM(vocab_dim, dropout_W=0.5, dropout_U=0.1, go_backwards=True)\n",
    "encoder_b.add(Merge([forwards_b, backwards_b], mode='concat', concat_axis=-1))\n",
    "encoder_b.add(Dropout(0.5))\n",
    "\n",
    "decoder = Sequential()\n",
    "decoder.add(Merge([encoder_a, encoder_b], mode='concat'))\n",
    "decoder.add(Dense(6, activation='softmax'))\n",
    "decoder.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "print('Build complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1570\n"
     ]
    }
   ],
   "source": [
    "print len(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 9092 samples, validate on 1570 samples\n",
      "Epoch 1/100\n",
      "9092/9092 [==============================] - 32s - loss: 1.7148 - acc: 0.2687 - val_loss: 1.8960 - val_acc: 0.2242\n",
      "Epoch 2/100\n",
      "9092/9092 [==============================] - 32s - loss: 1.6147 - acc: 0.3003 - val_loss: 1.8424 - val_acc: 0.2752\n",
      "Epoch 3/100\n",
      "9092/9092 [==============================] - 31s - loss: 1.5698 - acc: 0.3286 - val_loss: 1.8798 - val_acc: 0.2592\n",
      "Epoch 4/100\n",
      "9092/9092 [==============================] - 31s - loss: 1.5417 - acc: 0.3389 - val_loss: 1.9129 - val_acc: 0.2599\n",
      "Epoch 5/100\n",
      "9092/9092 [==============================] - 31s - loss: 1.5098 - acc: 0.3586 - val_loss: 1.8761 - val_acc: 0.2306\n",
      "Epoch 6/100\n",
      "9092/9092 [==============================] - 31s - loss: 1.4765 - acc: 0.3768 - val_loss: 1.7950 - val_acc: 0.2720\n",
      "Epoch 7/100\n",
      "9092/9092 [==============================] - 32s - loss: 1.4560 - acc: 0.3930 - val_loss: 1.9352 - val_acc: 0.2478\n",
      "Epoch 8/100\n",
      "9092/9092 [==============================] - 31s - loss: 1.4314 - acc: 0.4067 - val_loss: 1.8682 - val_acc: 0.2834\n",
      "Epoch 9/100\n",
      "9092/9092 [==============================] - 31s - loss: 1.4098 - acc: 0.4164 - val_loss: 1.8816 - val_acc: 0.2834\n",
      "Epoch 10/100\n",
      "9092/9092 [==============================] - 31s - loss: 1.3814 - acc: 0.4305 - val_loss: 1.8964 - val_acc: 0.2828\n",
      "Epoch 11/100\n",
      "9092/9092 [==============================] - 31s - loss: 1.3652 - acc: 0.4391 - val_loss: 1.9357 - val_acc: 0.2605\n",
      "Epoch 12/100\n",
      "9092/9092 [==============================] - 31s - loss: 1.3354 - acc: 0.4572 - val_loss: 1.8923 - val_acc: 0.2471\n",
      "Epoch 13/100\n",
      "9092/9092 [==============================] - 31s - loss: 1.3019 - acc: 0.4768 - val_loss: 1.9161 - val_acc: 0.2828\n",
      "Epoch 14/100\n",
      "9092/9092 [==============================] - 31s - loss: 1.2776 - acc: 0.4899 - val_loss: 1.9279 - val_acc: 0.2503\n",
      "Epoch 15/100\n",
      "9092/9092 [==============================] - 31s - loss: 1.2571 - acc: 0.5021 - val_loss: 1.9673 - val_acc: 0.2554\n",
      "Epoch 16/100\n",
      "9092/9092 [==============================] - 31s - loss: 1.2246 - acc: 0.5150 - val_loss: 1.9648 - val_acc: 0.2694\n",
      "Epoch 17/100\n",
      "9092/9092 [==============================] - 31s - loss: 1.1934 - acc: 0.5232 - val_loss: 2.0224 - val_acc: 0.2484\n",
      "Epoch 18/100\n",
      "9092/9092 [==============================] - 31s - loss: 1.1703 - acc: 0.5363 - val_loss: 2.0069 - val_acc: 0.2580\n",
      "Epoch 19/100\n",
      "9092/9092 [==============================] - 31s - loss: 1.1366 - acc: 0.5499 - val_loss: 2.0353 - val_acc: 0.2484\n",
      "Epoch 20/100\n",
      "9092/9092 [==============================] - 31s - loss: 1.1038 - acc: 0.5631 - val_loss: 2.0557 - val_acc: 0.2618\n",
      "Epoch 21/100\n",
      "9092/9092 [==============================] - 31s - loss: 1.0800 - acc: 0.5739 - val_loss: 2.1654 - val_acc: 0.2548\n",
      "Epoch 22/100\n",
      "9092/9092 [==============================] - 31s - loss: 1.0521 - acc: 0.5883 - val_loss: 2.1641 - val_acc: 0.2427\n",
      "Epoch 23/100\n",
      "9092/9092 [==============================] - 31s - loss: 1.0353 - acc: 0.5926 - val_loss: 2.2088 - val_acc: 0.2561\n",
      "Epoch 24/100\n",
      "9092/9092 [==============================] - 32s - loss: 0.9977 - acc: 0.6164 - val_loss: 2.3310 - val_acc: 0.2503\n",
      "Epoch 25/100\n",
      "9092/9092 [==============================] - 31s - loss: 0.9565 - acc: 0.6308 - val_loss: 2.1752 - val_acc: 0.2471\n",
      "Epoch 26/100\n",
      "9092/9092 [==============================] - 31s - loss: 0.9387 - acc: 0.6411 - val_loss: 2.3425 - val_acc: 0.2446\n",
      "Epoch 27/100\n",
      "9092/9092 [==============================] - 32s - loss: 0.9160 - acc: 0.6526 - val_loss: 2.3666 - val_acc: 0.2516\n",
      "Epoch 28/100\n",
      "9092/9092 [==============================] - 31s - loss: 0.8792 - acc: 0.6697 - val_loss: 2.3903 - val_acc: 0.2535\n",
      "Epoch 29/100\n",
      "9092/9092 [==============================] - 31s - loss: 0.8531 - acc: 0.6777 - val_loss: 2.5369 - val_acc: 0.2497\n",
      "Epoch 30/100\n",
      "9092/9092 [==============================] - 31s - loss: 0.8134 - acc: 0.6941 - val_loss: 2.4821 - val_acc: 0.2414\n",
      "Epoch 31/100\n",
      "9092/9092 [==============================] - 31s - loss: 0.8031 - acc: 0.6960 - val_loss: 2.5278 - val_acc: 0.2433\n",
      "Epoch 32/100\n",
      "9092/9092 [==============================] - 31s - loss: 0.7546 - acc: 0.7199 - val_loss: 2.6168 - val_acc: 0.2408\n",
      "Epoch 33/100\n",
      "9092/9092 [==============================] - 31s - loss: 0.7558 - acc: 0.7155 - val_loss: 2.6649 - val_acc: 0.2471\n",
      "Epoch 34/100\n",
      "9092/9092 [==============================] - 31s - loss: 0.7241 - acc: 0.7282 - val_loss: 2.7896 - val_acc: 0.2484\n",
      "Epoch 35/100\n",
      "9092/9092 [==============================] - 31s - loss: 0.7063 - acc: 0.7336 - val_loss: 2.8473 - val_acc: 0.2306\n",
      "Epoch 36/100\n",
      "9092/9092 [==============================] - 31s - loss: 0.6818 - acc: 0.7468 - val_loss: 2.8749 - val_acc: 0.2420\n",
      "Epoch 37/100\n",
      "9092/9092 [==============================] - 31s - loss: 0.6626 - acc: 0.7545 - val_loss: 2.8434 - val_acc: 0.2395\n",
      "Epoch 38/100\n",
      "9092/9092 [==============================] - 31s - loss: 0.6322 - acc: 0.7626 - val_loss: 2.8728 - val_acc: 0.2439\n",
      "Epoch 39/100\n",
      "9092/9092 [==============================] - 31s - loss: 0.6326 - acc: 0.7658 - val_loss: 2.9085 - val_acc: 0.2395\n",
      "Epoch 40/100\n",
      "9092/9092 [==============================] - 31s - loss: 0.6008 - acc: 0.7775 - val_loss: 3.0328 - val_acc: 0.2395\n",
      "Epoch 41/100\n",
      "9092/9092 [==============================] - 31s - loss: 0.5641 - acc: 0.7942 - val_loss: 3.1308 - val_acc: 0.2414\n",
      "Epoch 42/100\n",
      "9092/9092 [==============================] - 31s - loss: 0.5620 - acc: 0.7956 - val_loss: 3.2065 - val_acc: 0.2395\n",
      "Epoch 43/100\n",
      "9092/9092 [==============================] - 31s - loss: 0.5482 - acc: 0.7977 - val_loss: 3.2212 - val_acc: 0.2490\n",
      "Epoch 44/100\n",
      "9092/9092 [==============================] - 31s - loss: 0.5218 - acc: 0.8069 - val_loss: 3.3845 - val_acc: 0.2389\n",
      "Epoch 45/100\n",
      "9092/9092 [==============================] - 31s - loss: 0.5041 - acc: 0.8184 - val_loss: 3.3556 - val_acc: 0.2369\n",
      "Epoch 46/100\n",
      "9092/9092 [==============================] - 31s - loss: 0.4827 - acc: 0.8184 - val_loss: 3.4363 - val_acc: 0.2363\n",
      "Epoch 47/100\n",
      "9092/9092 [==============================] - 31s - loss: 0.4745 - acc: 0.8272 - val_loss: 3.5702 - val_acc: 0.2433\n",
      "Epoch 48/100\n",
      "9092/9092 [==============================] - 31s - loss: 0.4665 - acc: 0.8304 - val_loss: 3.5507 - val_acc: 0.2312\n",
      "Epoch 49/100\n",
      "9092/9092 [==============================] - 31s - loss: 0.4576 - acc: 0.8288 - val_loss: 3.6745 - val_acc: 0.2357\n",
      "Epoch 50/100\n",
      "9092/9092 [==============================] - 31s - loss: 0.4492 - acc: 0.8362 - val_loss: 3.5831 - val_acc: 0.2344\n",
      "Epoch 51/100\n",
      "9092/9092 [==============================] - 31s - loss: 0.4361 - acc: 0.8395 - val_loss: 3.7590 - val_acc: 0.2280\n",
      "Epoch 52/100\n",
      "9092/9092 [==============================] - 31s - loss: 0.4180 - acc: 0.8493 - val_loss: 3.7045 - val_acc: 0.2261\n",
      "Epoch 53/100\n",
      "9092/9092 [==============================] - 31s - loss: 0.4241 - acc: 0.8460 - val_loss: 3.7011 - val_acc: 0.2350\n",
      "Epoch 54/100\n",
      "9092/9092 [==============================] - 31s - loss: 0.4023 - acc: 0.8507 - val_loss: 3.8720 - val_acc: 0.2338\n",
      "Epoch 55/100\n",
      "9092/9092 [==============================] - 31s - loss: 0.3923 - acc: 0.8535 - val_loss: 4.0394 - val_acc: 0.2318\n",
      "Epoch 56/100\n",
      "9092/9092 [==============================] - 31s - loss: 0.3824 - acc: 0.8606 - val_loss: 3.9408 - val_acc: 0.2344\n",
      "Epoch 57/100\n",
      "9092/9092 [==============================] - 31s - loss: 0.3758 - acc: 0.8621 - val_loss: 3.9992 - val_acc: 0.2312\n",
      "Epoch 58/100\n",
      "9092/9092 [==============================] - 31s - loss: 0.3734 - acc: 0.8583 - val_loss: 4.0992 - val_acc: 0.2204\n",
      "Epoch 59/100\n",
      "9092/9092 [==============================] - 31s - loss: 0.3559 - acc: 0.8692 - val_loss: 4.0800 - val_acc: 0.2242\n",
      "Epoch 60/100\n",
      "9092/9092 [==============================] - 31s - loss: 0.3585 - acc: 0.8711 - val_loss: 4.1323 - val_acc: 0.2338\n",
      "Epoch 61/100\n",
      "9092/9092 [==============================] - 31s - loss: 0.3270 - acc: 0.8778 - val_loss: 4.1446 - val_acc: 0.2293\n",
      "Epoch 62/100\n",
      "9092/9092 [==============================] - 31s - loss: 0.3347 - acc: 0.8800 - val_loss: 4.3770 - val_acc: 0.2306\n",
      "Epoch 63/100\n",
      "9092/9092 [==============================] - 31s - loss: 0.3259 - acc: 0.8810 - val_loss: 4.3700 - val_acc: 0.2236\n",
      "Epoch 64/100\n",
      "9092/9092 [==============================] - 31s - loss: 0.3136 - acc: 0.8875 - val_loss: 4.3886 - val_acc: 0.2229\n",
      "Epoch 65/100\n",
      "9092/9092 [==============================] - 31s - loss: 0.3083 - acc: 0.8857 - val_loss: 4.4409 - val_acc: 0.2293\n",
      "Epoch 66/100\n",
      "9092/9092 [==============================] - 31s - loss: 0.3061 - acc: 0.8883 - val_loss: 4.4377 - val_acc: 0.2236\n",
      "Epoch 67/100\n",
      "9092/9092 [==============================] - 31s - loss: 0.2994 - acc: 0.8945 - val_loss: 4.4787 - val_acc: 0.2248\n",
      "Epoch 68/100\n",
      "9092/9092 [==============================] - 31s - loss: 0.2778 - acc: 0.8985 - val_loss: 4.6500 - val_acc: 0.2236\n",
      "Epoch 69/100\n",
      "9092/9092 [==============================] - 31s - loss: 0.2725 - acc: 0.9031 - val_loss: 4.6610 - val_acc: 0.2312\n",
      "Epoch 70/100\n",
      "9092/9092 [==============================] - 31s - loss: 0.2765 - acc: 0.8976 - val_loss: 4.6406 - val_acc: 0.2306\n",
      "Epoch 71/100\n",
      "9092/9092 [==============================] - 31s - loss: 0.2636 - acc: 0.9013 - val_loss: 4.7542 - val_acc: 0.2236\n",
      "Epoch 72/100\n",
      "9092/9092 [==============================] - 31s - loss: 0.2681 - acc: 0.9027 - val_loss: 4.7520 - val_acc: 0.2210\n",
      "Epoch 73/100\n",
      "9092/9092 [==============================] - 31s - loss: 0.2591 - acc: 0.9093 - val_loss: 4.7083 - val_acc: 0.2217\n",
      "Epoch 74/100\n",
      "9092/9092 [==============================] - 31s - loss: 0.2431 - acc: 0.9121 - val_loss: 5.0024 - val_acc: 0.2191\n",
      "Epoch 75/100\n",
      "9092/9092 [==============================] - 31s - loss: 0.2462 - acc: 0.9093 - val_loss: 4.9729 - val_acc: 0.2178\n",
      "Epoch 76/100\n",
      "9092/9092 [==============================] - 31s - loss: 0.2429 - acc: 0.9150 - val_loss: 5.0204 - val_acc: 0.2197\n",
      "Epoch 77/100\n",
      "9092/9092 [==============================] - 31s - loss: 0.2357 - acc: 0.9143 - val_loss: 5.1294 - val_acc: 0.2159\n",
      "Epoch 78/100\n",
      "9092/9092 [==============================] - 31s - loss: 0.2417 - acc: 0.9150 - val_loss: 5.1423 - val_acc: 0.2096\n",
      "Epoch 79/100\n",
      "9092/9092 [==============================] - 31s - loss: 0.2269 - acc: 0.9179 - val_loss: 5.1980 - val_acc: 0.2153\n",
      "Epoch 80/100\n",
      "9092/9092 [==============================] - 32s - loss: 0.2228 - acc: 0.9171 - val_loss: 5.1923 - val_acc: 0.2166\n",
      "Epoch 81/100\n",
      "9092/9092 [==============================] - 32s - loss: 0.2188 - acc: 0.9206 - val_loss: 5.2550 - val_acc: 0.2134\n",
      "Epoch 82/100\n",
      "9092/9092 [==============================] - 31s - loss: 0.2111 - acc: 0.9256 - val_loss: 5.2396 - val_acc: 0.2185\n",
      "Epoch 83/100\n",
      "9092/9092 [==============================] - 31s - loss: 0.2194 - acc: 0.9210 - val_loss: 5.3035 - val_acc: 0.2191\n",
      "Epoch 84/100\n",
      "9092/9092 [==============================] - 31s - loss: 0.2091 - acc: 0.9254 - val_loss: 5.4200 - val_acc: 0.2217\n",
      "Epoch 85/100\n",
      "9092/9092 [==============================] - 31s - loss: 0.2064 - acc: 0.9241 - val_loss: 5.3858 - val_acc: 0.2191\n",
      "Epoch 86/100\n",
      "9092/9092 [==============================] - 31s - loss: 0.2050 - acc: 0.9245 - val_loss: 5.3959 - val_acc: 0.2197\n",
      "Epoch 87/100\n",
      "9092/9092 [==============================] - 31s - loss: 0.2075 - acc: 0.9239 - val_loss: 5.4723 - val_acc: 0.2153\n",
      "Epoch 88/100\n",
      "9092/9092 [==============================] - 31s - loss: 0.1971 - acc: 0.9261 - val_loss: 5.5313 - val_acc: 0.2197\n",
      "Epoch 89/100\n",
      "9092/9092 [==============================] - 31s - loss: 0.1963 - acc: 0.9262 - val_loss: 5.5230 - val_acc: 0.2166\n",
      "Epoch 90/100\n",
      "9092/9092 [==============================] - 31s - loss: 0.1942 - acc: 0.9310 - val_loss: 5.6210 - val_acc: 0.2210\n",
      "Epoch 91/100\n",
      "9092/9092 [==============================] - 31s - loss: 0.2020 - acc: 0.9288 - val_loss: 5.5712 - val_acc: 0.2172\n",
      "Epoch 92/100\n",
      "9092/9092 [==============================] - 31s - loss: 0.1840 - acc: 0.9310 - val_loss: 5.6445 - val_acc: 0.2229\n",
      "Epoch 93/100\n",
      "9092/9092 [==============================] - 31s - loss: 0.1900 - acc: 0.9306 - val_loss: 5.6234 - val_acc: 0.2172\n",
      "Epoch 94/100\n",
      "9092/9092 [==============================] - 31s - loss: 0.1882 - acc: 0.9284 - val_loss: 5.6572 - val_acc: 0.2223\n",
      "Epoch 95/100\n",
      "9092/9092 [==============================] - 31s - loss: 0.1812 - acc: 0.9341 - val_loss: 5.7105 - val_acc: 0.2146\n",
      "Epoch 96/100\n",
      "9092/9092 [==============================] - 31s - loss: 0.1720 - acc: 0.9392 - val_loss: 5.7704 - val_acc: 0.2204\n",
      "Epoch 97/100\n",
      "9092/9092 [==============================] - 31s - loss: 0.1701 - acc: 0.9388 - val_loss: 5.7591 - val_acc: 0.2197\n",
      "Epoch 98/100\n",
      "9092/9092 [==============================] - 31s - loss: 0.1799 - acc: 0.9359 - val_loss: 5.8133 - val_acc: 0.2236\n",
      "Epoch 99/100\n",
      "9092/9092 [==============================] - 31s - loss: 0.1778 - acc: 0.9359 - val_loss: 5.7995 - val_acc: 0.2223\n",
      "Epoch 100/100\n",
      "9092/9092 [==============================] - 31s - loss: 0.1759 - acc: 0.9371 - val_loss: 5.7707 - val_acc: 0.2236\n"
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "\n",
    "print('Train...')\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=4)\n",
    "hist = decoder.fit([X_train1, X_train2], y_train, batch_size=batch_size, nb_epoch=100, show_accuracy=True, \n",
    "            validation_data=([X_val1, X_val2], y_val), \n",
    "#                    callbacks=[early_stopping]\n",
    "                  )\n",
    "# print(hist.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1413 samples, validate on 157 samples\n",
      "Epoch 1/100\n",
      "1413/1413 [==============================] - 5s - loss: 1.8650 - acc: 0.2130 - val_loss: 1.7859 - val_acc: 0.1720\n",
      "Epoch 2/100\n",
      "1413/1413 [==============================] - 4s - loss: 1.7492 - acc: 0.2604 - val_loss: 1.8563 - val_acc: 0.2038\n",
      "Epoch 3/100\n",
      "1413/1413 [==============================] - 4s - loss: 1.7125 - acc: 0.2626 - val_loss: 1.7463 - val_acc: 0.2102\n",
      "Epoch 4/100\n",
      "1413/1413 [==============================] - 4s - loss: 1.6761 - acc: 0.2831 - val_loss: 1.6164 - val_acc: 0.4395\n",
      "Epoch 5/100\n",
      "1413/1413 [==============================] - 4s - loss: 1.6379 - acc: 0.3128 - val_loss: 1.6528 - val_acc: 0.2293\n",
      "Epoch 6/100\n",
      "1413/1413 [==============================] - 4s - loss: 1.6165 - acc: 0.3383 - val_loss: 1.6205 - val_acc: 0.2930\n",
      "Epoch 7/100\n",
      "1413/1413 [==============================] - 4s - loss: 1.5898 - acc: 0.3510 - val_loss: 1.5786 - val_acc: 0.4140\n",
      "Epoch 8/100\n",
      "1413/1413 [==============================] - 4s - loss: 1.5707 - acc: 0.3595 - val_loss: 1.6784 - val_acc: 0.3376\n",
      "Epoch 9/100\n",
      "1413/1413 [==============================] - 4s - loss: 1.5485 - acc: 0.3737 - val_loss: 1.4964 - val_acc: 0.4204\n",
      "Epoch 10/100\n",
      "1413/1413 [==============================] - 4s - loss: 1.5308 - acc: 0.3687 - val_loss: 1.5467 - val_acc: 0.3949\n",
      "Epoch 11/100\n",
      "1413/1413 [==============================] - 4s - loss: 1.5007 - acc: 0.3942 - val_loss: 1.5459 - val_acc: 0.4013\n",
      "Epoch 12/100\n",
      "1413/1413 [==============================] - 5s - loss: 1.4803 - acc: 0.4062 - val_loss: 1.5443 - val_acc: 0.3949\n",
      "Epoch 13/100\n",
      "1413/1413 [==============================] - 4s - loss: 1.4515 - acc: 0.4168 - val_loss: 1.5406 - val_acc: 0.3885\n",
      "Epoch 14/100\n",
      "1413/1413 [==============================] - 4s - loss: 1.4089 - acc: 0.4508 - val_loss: 1.5963 - val_acc: 0.3885\n",
      "Epoch 15/100\n",
      "1413/1413 [==============================] - 4s - loss: 1.3733 - acc: 0.4558 - val_loss: 1.6341 - val_acc: 0.3758\n",
      "Epoch 16/100\n",
      "1413/1413 [==============================] - 4s - loss: 1.3181 - acc: 0.4968 - val_loss: 1.7041 - val_acc: 0.3376\n",
      "Epoch 17/100\n",
      "1413/1413 [==============================] - 4s - loss: 1.2917 - acc: 0.5074 - val_loss: 1.6022 - val_acc: 0.4331\n",
      "Epoch 18/100\n",
      "1413/1413 [==============================] - 4s - loss: 1.2374 - acc: 0.5131 - val_loss: 1.6140 - val_acc: 0.3439\n",
      "Epoch 19/100\n",
      "1413/1413 [==============================] - 4s - loss: 1.1975 - acc: 0.5456 - val_loss: 1.7165 - val_acc: 0.3121\n",
      "Epoch 20/100\n",
      "1413/1413 [==============================] - 4s - loss: 1.1649 - acc: 0.5541 - val_loss: 1.6309 - val_acc: 0.3758\n",
      "Epoch 21/100\n",
      "1413/1413 [==============================] - 5s - loss: 1.0870 - acc: 0.5895 - val_loss: 1.6602 - val_acc: 0.2930\n",
      "Epoch 22/100\n",
      "1413/1413 [==============================] - 5s - loss: 1.0281 - acc: 0.6270 - val_loss: 1.7677 - val_acc: 0.2803\n",
      "Epoch 23/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.9878 - acc: 0.6447 - val_loss: 1.6845 - val_acc: 0.3822\n",
      "Epoch 24/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.9070 - acc: 0.6794 - val_loss: 1.7967 - val_acc: 0.3121\n",
      "Epoch 25/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.8482 - acc: 0.6985 - val_loss: 1.9505 - val_acc: 0.3439\n",
      "Epoch 26/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.8075 - acc: 0.7254 - val_loss: 1.9031 - val_acc: 0.2803\n",
      "Epoch 27/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.7609 - acc: 0.7233 - val_loss: 2.0296 - val_acc: 0.2739\n",
      "Epoch 28/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.7213 - acc: 0.7523 - val_loss: 2.1121 - val_acc: 0.3376\n",
      "Epoch 29/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.6609 - acc: 0.7665 - val_loss: 1.9909 - val_acc: 0.3376\n",
      "Epoch 30/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.5986 - acc: 0.7976 - val_loss: 2.0612 - val_acc: 0.3312\n",
      "Epoch 31/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.5689 - acc: 0.8160 - val_loss: 2.2446 - val_acc: 0.3439\n",
      "Epoch 32/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.5058 - acc: 0.8337 - val_loss: 2.3571 - val_acc: 0.2866\n",
      "Epoch 33/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.5100 - acc: 0.8259 - val_loss: 2.4788 - val_acc: 0.2866\n",
      "Epoch 34/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.4566 - acc: 0.8436 - val_loss: 2.7126 - val_acc: 0.2739\n",
      "Epoch 35/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.4500 - acc: 0.8471 - val_loss: 2.4363 - val_acc: 0.3121\n",
      "Epoch 36/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.4193 - acc: 0.8627 - val_loss: 2.4571 - val_acc: 0.3567\n",
      "Epoch 37/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.3669 - acc: 0.8790 - val_loss: 2.5672 - val_acc: 0.3439\n",
      "Epoch 38/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.3512 - acc: 0.8825 - val_loss: 2.7846 - val_acc: 0.3885\n",
      "Epoch 39/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.3357 - acc: 0.8960 - val_loss: 2.8504 - val_acc: 0.3121\n",
      "Epoch 40/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.3263 - acc: 0.8953 - val_loss: 2.7285 - val_acc: 0.3312\n",
      "Epoch 41/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.2757 - acc: 0.9087 - val_loss: 2.8559 - val_acc: 0.3312\n",
      "Epoch 42/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.2794 - acc: 0.9094 - val_loss: 3.0356 - val_acc: 0.3312\n",
      "Epoch 43/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.2729 - acc: 0.9144 - val_loss: 2.9578 - val_acc: 0.3121\n",
      "Epoch 44/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.2608 - acc: 0.9179 - val_loss: 3.3252 - val_acc: 0.2803\n",
      "Epoch 45/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.2536 - acc: 0.9130 - val_loss: 3.1893 - val_acc: 0.2866\n",
      "Epoch 46/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.2183 - acc: 0.9335 - val_loss: 3.0771 - val_acc: 0.3312\n",
      "Epoch 47/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.1992 - acc: 0.9398 - val_loss: 3.3509 - val_acc: 0.3248\n",
      "Epoch 48/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.2053 - acc: 0.9349 - val_loss: 3.2183 - val_acc: 0.2994\n",
      "Epoch 49/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.2066 - acc: 0.9356 - val_loss: 3.1724 - val_acc: 0.3312\n",
      "Epoch 50/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.2088 - acc: 0.9278 - val_loss: 3.2967 - val_acc: 0.3121\n",
      "Epoch 51/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.1969 - acc: 0.9377 - val_loss: 3.3220 - val_acc: 0.2994\n",
      "Epoch 52/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.1768 - acc: 0.9441 - val_loss: 3.4794 - val_acc: 0.3185\n",
      "Epoch 53/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.1711 - acc: 0.9427 - val_loss: 3.4100 - val_acc: 0.3439\n",
      "Epoch 54/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.1662 - acc: 0.9420 - val_loss: 3.5260 - val_acc: 0.3057\n",
      "Epoch 55/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.1675 - acc: 0.9441 - val_loss: 3.5902 - val_acc: 0.3376\n",
      "Epoch 56/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.1429 - acc: 0.9483 - val_loss: 3.5998 - val_acc: 0.3567\n",
      "Epoch 57/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.1536 - acc: 0.9519 - val_loss: 3.5960 - val_acc: 0.3567\n",
      "Epoch 58/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.1446 - acc: 0.9455 - val_loss: 3.6684 - val_acc: 0.2803\n",
      "Epoch 59/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.1160 - acc: 0.9689 - val_loss: 3.8264 - val_acc: 0.3057\n",
      "Epoch 60/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.1308 - acc: 0.9590 - val_loss: 3.8092 - val_acc: 0.3439\n",
      "Epoch 61/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.1282 - acc: 0.9618 - val_loss: 3.7316 - val_acc: 0.3248\n",
      "Epoch 62/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.1129 - acc: 0.9611 - val_loss: 3.7225 - val_acc: 0.3248\n",
      "Epoch 63/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.1173 - acc: 0.9618 - val_loss: 4.0660 - val_acc: 0.2866\n",
      "Epoch 64/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.1074 - acc: 0.9660 - val_loss: 3.7886 - val_acc: 0.2994\n",
      "Epoch 65/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.1038 - acc: 0.9660 - val_loss: 3.9751 - val_acc: 0.3057\n",
      "Epoch 66/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.1099 - acc: 0.9611 - val_loss: 4.0022 - val_acc: 0.3057\n",
      "Epoch 67/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.1087 - acc: 0.9667 - val_loss: 4.0484 - val_acc: 0.3121\n",
      "Epoch 68/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.0927 - acc: 0.9696 - val_loss: 4.0106 - val_acc: 0.3185\n",
      "Epoch 69/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.1035 - acc: 0.9660 - val_loss: 4.0493 - val_acc: 0.3248\n",
      "Epoch 70/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.1023 - acc: 0.9660 - val_loss: 4.1186 - val_acc: 0.2866\n",
      "Epoch 71/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.0893 - acc: 0.9710 - val_loss: 4.0839 - val_acc: 0.3503\n",
      "Epoch 72/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.0984 - acc: 0.9667 - val_loss: 4.1363 - val_acc: 0.3503\n",
      "Epoch 73/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.0832 - acc: 0.9745 - val_loss: 4.1467 - val_acc: 0.3185\n",
      "Epoch 74/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.0976 - acc: 0.9696 - val_loss: 4.3497 - val_acc: 0.2866\n",
      "Epoch 75/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.0874 - acc: 0.9696 - val_loss: 4.2429 - val_acc: 0.2994\n",
      "Epoch 76/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.0789 - acc: 0.9724 - val_loss: 4.6248 - val_acc: 0.2803\n",
      "Epoch 77/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.0830 - acc: 0.9696 - val_loss: 4.1941 - val_acc: 0.3376\n",
      "Epoch 78/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.0768 - acc: 0.9724 - val_loss: 4.6176 - val_acc: 0.2994\n",
      "Epoch 79/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.0830 - acc: 0.9710 - val_loss: 4.7412 - val_acc: 0.2930\n",
      "Epoch 80/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.0710 - acc: 0.9795 - val_loss: 4.3868 - val_acc: 0.3376\n",
      "Epoch 81/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.0753 - acc: 0.9781 - val_loss: 4.5251 - val_acc: 0.3439\n",
      "Epoch 82/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.0601 - acc: 0.9844 - val_loss: 4.5361 - val_acc: 0.3185\n",
      "Epoch 83/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.0632 - acc: 0.9795 - val_loss: 4.6108 - val_acc: 0.3248\n",
      "Epoch 84/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.0913 - acc: 0.9696 - val_loss: 4.5547 - val_acc: 0.2866\n",
      "Epoch 85/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.0714 - acc: 0.9809 - val_loss: 4.5034 - val_acc: 0.3185\n",
      "Epoch 86/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.0703 - acc: 0.9731 - val_loss: 4.5861 - val_acc: 0.3248\n",
      "Epoch 87/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.0654 - acc: 0.9788 - val_loss: 4.5523 - val_acc: 0.3503\n",
      "Epoch 88/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.0618 - acc: 0.9830 - val_loss: 4.5395 - val_acc: 0.3567\n",
      "Epoch 89/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.0611 - acc: 0.9851 - val_loss: 4.6799 - val_acc: 0.3376\n",
      "Epoch 90/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.0659 - acc: 0.9795 - val_loss: 4.6002 - val_acc: 0.3248\n",
      "Epoch 91/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.0641 - acc: 0.9788 - val_loss: 4.8258 - val_acc: 0.3248\n",
      "Epoch 92/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.0593 - acc: 0.9830 - val_loss: 4.6893 - val_acc: 0.3376\n",
      "Epoch 93/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.0680 - acc: 0.9731 - val_loss: 4.9048 - val_acc: 0.3185\n",
      "Epoch 94/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.0581 - acc: 0.9837 - val_loss: 4.6501 - val_acc: 0.3503\n",
      "Epoch 95/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.0620 - acc: 0.9816 - val_loss: 4.6882 - val_acc: 0.3439\n",
      "Epoch 96/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.0591 - acc: 0.9823 - val_loss: 4.7803 - val_acc: 0.3631\n",
      "Epoch 97/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.0577 - acc: 0.9795 - val_loss: 4.8381 - val_acc: 0.3185\n",
      "Epoch 98/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.0531 - acc: 0.9823 - val_loss: 4.8868 - val_acc: 0.2994\n",
      "Epoch 99/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.0634 - acc: 0.9774 - val_loss: 4.7720 - val_acc: 0.3567\n",
      "Epoch 100/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.0548 - acc: 0.9802 - val_loss: 4.8976 - val_acc: 0.3312\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fceea5ffd50>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder.fit([X_val1, X_val2], y_val, batch_size=batch_size, nb_epoch=100, show_accuracy=True, \n",
    "            validation_split = 0.1, shuffle=True, \n",
    "#                    callbacks=[early_stopping]\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predict score of 2015 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "All input arrays and the target array must have the same number of samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-114-f056a2cd9e2f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m score, acc = decoder.evaluate([X_test1_f, X_test2_f], y_test,\n\u001b[0;32m      2\u001b[0m                             \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m                             show_accuracy=True)\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Test score:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Test accuracy:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/keras/models.pyc\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, X, y, batch_size, show_accuracy, verbose, sample_weight)\u001b[0m\n\u001b[0;32m    765\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 767\u001b[1;33m                 raise Exception('All input arrays and the target array must '\n\u001b[0m\u001b[0;32m    768\u001b[0m                                 'have the same number of samples.')\n\u001b[0;32m    769\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: All input arrays and the target array must have the same number of samples."
     ]
    }
   ],
   "source": [
    "score, acc = decoder.evaluate([X_test1_f, X_test2_f], y_test,\n",
    "                            batch_size=batch_size,\n",
    "                            show_accuracy=True)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def writeRes(dst, res):\n",
    "    with open(dst, 'w') as thefile:\n",
    "        thefile.write(\"\\n\".join(str(i) for i in res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 0s     \n"
     ]
    }
   ],
   "source": [
    "res = decoder.predict_classes([X_test1_f, X_test2_f]) \n",
    "# res = [r[0] for r in res]\n",
    "# writeRes('./dataset/STS2015-test/sys.forum', res)\n",
    "np.savetxt('./dataset/STS2015-test/sys.forum', res, newline='\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
