{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GeForce GTX 980 Ti (CNMeM is disabled, CuDNN 4007)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM, GRU\n",
    "from keras.preprocessing import text\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import EarlyStopping\n",
    "from __future__ import print_function\n",
    "from keras.layers.core import Activation, TimeDistributedDense, RepeatVector\n",
    "from keras.layers import recurrent\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CharacterTable(object):\n",
    "    def __init__(self, vocab, maxlen):\n",
    "        self.vocab = vocab\n",
    "        self.maxlen = maxlen\n",
    "    \n",
    "    def encode(self, C, maxlen=None):\n",
    "        maxlen = maxlen if maxlen else self.maxlen\n",
    "        X = np.zeros((maxlen, len(self.vocab)))\n",
    "        for i, c in enumerate(C):\n",
    "            X[i, c] = 1\n",
    "        return X\n",
    "    \n",
    "    def decode(self, X, calc_argmax=True):\n",
    "        if calc_argmax:\n",
    "            X = X.argmax(axis=-1)\n",
    "        return ','.join(x for x in X)\n",
    "    \n",
    "def generateRandSeq(min, max, len):\n",
    "    return [np.random.randint(min, max) for _ in range(len)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TRAINING_SIZE = 150000\n",
    "TEST_SIZE = 10000\n",
    "DIGITS = 25\n",
    "MAXLEN = DIGITS\n",
    "voc = list(xrange(1000))\n",
    "ctable = CharacterTable(voc, MAXLEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating data...\n",
      "[353, 115, 172, 440, 52, 411, 662, 130, 443, 675, 95, 744, 166, 31, 985, 198, 932, 467, 201, 371, 433, 675, 206, 677, 443]\n",
      "[198, 185, 172, 171, 166, 153, 144, 132, 130, 115, 95, 77, 75, 75, 67, 62, 52, 43, 43, 40, 33, 31, 11, 6, 1]\n"
     ]
    }
   ],
   "source": [
    "inputs = []\n",
    "outputs = []\n",
    "inputs_t = []\n",
    "outputs_t = []\n",
    "print('Generating data...')\n",
    "while len(inputs) < TRAINING_SIZE:\n",
    "    s = generateRandSeq(0, len(voc), DIGITS)\n",
    "    inputs.append(s)\n",
    "    s2 = [i%200 for i in s]\n",
    "    outputs.append(sorted(s2)[::-1])\n",
    "\n",
    "while len(inputs_t) < TEST_SIZE:\n",
    "    s = generateRandSeq(0, len(voc), DIGITS)\n",
    "    inputs_t.append(s)\n",
    "    s2 = [i%200 for i in s]\n",
    "    outputs_t.append(sorted(s2)[::-1])\n",
    "print(inputs[12])\n",
    "print(outputs[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization...\n",
      "(150000, 25)\n",
      "(150000, 25, 1000)\n",
      "(10000, 25)\n",
      "(10000, 25, 1000)\n"
     ]
    }
   ],
   "source": [
    "print('Vectorization...')\n",
    "X = np.zeros((len(inputs), MAXLEN), dtype=np.int32)\n",
    "# y = np.zeros((len(outputs), MAXLEN), dtype=np.int32)\n",
    "y = np.zeros((len(outputs), MAXLEN, len(voc)), dtype=np.bool)\n",
    "for i, sentence in enumerate(inputs):\n",
    "    X[i] = inputs[i]\n",
    "\n",
    "# for i, sentence in enumerate(outputs):\n",
    "#     y[i] = outputs[i]\n",
    "for i, sentence in enumerate(outputs):\n",
    "    y[i] = ctable.encode(sentence, maxlen=MAXLEN)\n",
    "\n",
    "X_test = np.zeros((len(inputs_t), MAXLEN), dtype=np.int32)\n",
    "# y_test = np.zeros((len(outputs_t), MAXLEN), dtype=np.int32)\n",
    "y_test = np.zeros((len(outputs_t), MAXLEN, len(voc)), dtype=np.bool)\n",
    "for i, sentence in enumerate(inputs_t):\n",
    "    X_test[i] = inputs_t[i]\n",
    "\n",
    "# for i, sentence in enumerate(outputs_t):\n",
    "#     y_test[i] = outputs_t[i]\n",
    "for i, sentence in enumerate(outputs_t):\n",
    "    y_test[i] = ctable.encode(sentence, maxlen=MAXLEN)\n",
    "    \n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "HIDDEN_SIZE = 256\n",
    "BATCH_SIZE = 200\n",
    "LAYERS = 2\n",
    "'''\n",
    "Hey guys, I also met this problem and I found this thread. Basically, \n",
    "the error info can happen when the dimension of the input data (X_train or Y_train) doesn't match with the \n",
    "model's input shape.\n",
    "\n",
    "In my case (and @LeavesBreathe 's case I guess), the problem is that \n",
    "the model is expecting the Y_train to be a 3d tensor. Because of the embedding layer, \n",
    "the 2d tensor X_train of size (n_batch, sequence_length) will be eventually converted to a 3d tensor of size \n",
    "(n_batch, sequence_length, embedding_size) and will be processed by the succeeding LSTM layer. However, \n",
    "the 2d tensor Y_train of size (n_sample, sequence_length) is not converted to 3d, \n",
    "which is needed by the decoder LSTM.\n",
    "\n",
    "To fix this problem, what I did is to convert Y_train into a 3d binary tensor (binary one-hot coding) and it worked.\n",
    "'''\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(voc), 300, input_length = MAXLEN))\n",
    "model.add(LSTM(HIDDEN_SIZE, return_sequences=True))\n",
    "for _ in range(LAYERS - 2):\n",
    "    model.add(LSTM(HIDDEN_SIZE, return_sequences=True))\n",
    "\n",
    "model.add(LSTM(HIDDEN_SIZE))\n",
    "model.add(RepeatVector(MAXLEN))\n",
    "for _ in range(LAYERS):\n",
    "    model.add(LSTM(HIDDEN_SIZE, return_sequences=True))\n",
    "\n",
    "model.add(TimeDistributedDense(input_dim=HIDDEN_SIZE, output_dim=300))\n",
    "model.add(Activation('tanh'))\n",
    "model.add(TimeDistributedDense(input_dim=300, output_dim=len(voc)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(optimizer='RMSprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 135000 samples, validate on 15000 samples\n",
      "Epoch 1/150\n",
      "135000/135000 [==============================] - 545s - loss: 4.1375 - acc: 0.0297 - val_loss: 3.9156 - val_acc: 0.0338\n",
      "Epoch 2/150\n",
      "135000/135000 [==============================] - 603s - loss: 3.7152 - acc: 0.0415 - val_loss: 3.5557 - val_acc: 0.0469\n",
      "Epoch 3/150\n",
      "135000/135000 [==============================] - 610s - loss: 3.4321 - acc: 0.0543 - val_loss: 3.3369 - val_acc: 0.0571\n",
      "Epoch 4/150\n",
      "135000/135000 [==============================] - 588s - loss: 3.2718 - acc: 0.0632 - val_loss: 3.1051 - val_acc: 0.0733\n",
      "Epoch 5/150\n",
      "135000/135000 [==============================] - 585s - loss: 3.0382 - acc: 0.0795 - val_loss: 2.9223 - val_acc: 0.0903\n",
      "Epoch 6/150\n",
      "135000/135000 [==============================] - 478s - loss: 2.8638 - acc: 0.0947 - val_loss: 2.8184 - val_acc: 0.0986\n",
      "Epoch 7/150\n",
      "135000/135000 [==============================] - 475s - loss: 2.7041 - acc: 0.1108 - val_loss: 2.6392 - val_acc: 0.1181\n",
      "Epoch 8/150\n",
      "135000/135000 [==============================] - 480s - loss: 2.5317 - acc: 0.1321 - val_loss: 2.4133 - val_acc: 0.1465\n",
      "Epoch 9/150\n",
      "135000/135000 [==============================] - 477s - loss: 2.3979 - acc: 0.1512 - val_loss: 2.3255 - val_acc: 0.1575\n",
      "Epoch 10/150\n",
      "135000/135000 [==============================] - 480s - loss: 2.2698 - acc: 0.1710 - val_loss: 2.2773 - val_acc: 0.1671\n",
      "Epoch 11/150\n",
      "135000/135000 [==============================] - 481s - loss: 2.1565 - acc: 0.1910 - val_loss: 2.1549 - val_acc: 0.1935\n",
      "Epoch 12/150\n",
      "135000/135000 [==============================] - 481s - loss: 2.0383 - acc: 0.2155 - val_loss: 2.0364 - val_acc: 0.2148\n",
      "Epoch 13/150\n",
      "135000/135000 [==============================] - 566s - loss: 1.9363 - acc: 0.2388 - val_loss: 1.8607 - val_acc: 0.2530\n",
      "Epoch 14/150\n",
      "135000/135000 [==============================] - 601s - loss: 1.8424 - acc: 0.2628 - val_loss: 1.7664 - val_acc: 0.2740\n",
      "Epoch 15/150\n",
      "135000/135000 [==============================] - 556s - loss: 1.7442 - acc: 0.2905 - val_loss: 1.6162 - val_acc: 0.3231\n",
      "Epoch 16/150\n",
      "135000/135000 [==============================] - 480s - loss: 1.6451 - acc: 0.3195 - val_loss: 1.6541 - val_acc: 0.3251\n",
      "Epoch 17/150\n",
      "135000/135000 [==============================] - 477s - loss: 1.5594 - acc: 0.3479 - val_loss: 1.4858 - val_acc: 0.3685\n",
      "Epoch 18/150\n",
      "135000/135000 [==============================] - 561s - loss: 1.4765 - acc: 0.3764 - val_loss: 1.5302 - val_acc: 0.3633\n",
      "Epoch 19/150\n",
      "135000/135000 [==============================] - 407s - loss: 1.3961 - acc: 0.4071 - val_loss: 1.3050 - val_acc: 0.4352\n",
      "Epoch 20/150\n",
      "135000/135000 [==============================] - 377s - loss: 1.3199 - acc: 0.4362 - val_loss: 1.3808 - val_acc: 0.4080\n",
      "Epoch 21/150\n",
      "135000/135000 [==============================] - 379s - loss: 1.2455 - acc: 0.4658 - val_loss: 1.2052 - val_acc: 0.4821\n",
      "Epoch 22/150\n",
      "135000/135000 [==============================] - 376s - loss: 1.1815 - acc: 0.4947 - val_loss: 1.1110 - val_acc: 0.5184\n",
      "Epoch 23/150\n",
      "135000/135000 [==============================] - 384s - loss: 1.1196 - acc: 0.5213 - val_loss: 1.2339 - val_acc: 0.4820\n",
      "Epoch 24/150\n",
      "135000/135000 [==============================] - 436s - loss: 1.0574 - acc: 0.5489 - val_loss: 1.0049 - val_acc: 0.5653\n",
      "Epoch 25/150\n",
      "135000/135000 [==============================] - 378s - loss: 0.9999 - acc: 0.5749 - val_loss: 0.8764 - val_acc: 0.6301\n",
      "Epoch 26/150\n",
      "135000/135000 [==============================] - 376s - loss: 0.9444 - acc: 0.6017 - val_loss: 0.9110 - val_acc: 0.6109\n",
      "Epoch 27/150\n",
      "135000/135000 [==============================] - 375s - loss: 0.8953 - acc: 0.6238 - val_loss: 0.9350 - val_acc: 0.6027\n",
      "Epoch 28/150\n",
      "135000/135000 [==============================] - 373s - loss: 0.8463 - acc: 0.6477 - val_loss: 0.7304 - val_acc: 0.7000\n",
      "Epoch 29/150\n",
      "135000/135000 [==============================] - 373s - loss: 0.7938 - acc: 0.6703 - val_loss: 0.7404 - val_acc: 0.6924\n",
      "Epoch 30/150\n",
      "135000/135000 [==============================] - 372s - loss: 0.7530 - acc: 0.6903 - val_loss: 0.7975 - val_acc: 0.6664\n",
      "Epoch 31/150\n",
      "135000/135000 [==============================] - 372s - loss: 0.7135 - acc: 0.7076 - val_loss: 0.6711 - val_acc: 0.7233\n",
      "Epoch 32/150\n",
      "135000/135000 [==============================] - 374s - loss: 0.6742 - acc: 0.7258 - val_loss: 0.6458 - val_acc: 0.7310\n",
      "Epoch 33/150\n",
      "135000/135000 [==============================] - 373s - loss: 0.6391 - acc: 0.7423 - val_loss: 0.5890 - val_acc: 0.7602\n",
      "Epoch 34/150\n",
      "135000/135000 [==============================] - 372s - loss: 0.6018 - acc: 0.7566 - val_loss: 0.5767 - val_acc: 0.7640\n",
      "Epoch 35/150\n",
      "135000/135000 [==============================] - 376s - loss: 0.5701 - acc: 0.7733 - val_loss: 0.5664 - val_acc: 0.7664\n",
      "Epoch 36/150\n",
      "135000/135000 [==============================] - 459s - loss: 0.5412 - acc: 0.7846 - val_loss: 0.5209 - val_acc: 0.7865\n",
      "Epoch 37/150\n",
      "135000/135000 [==============================] - 493s - loss: 0.5105 - acc: 0.7969 - val_loss: 0.4851 - val_acc: 0.8039\n",
      "Epoch 38/150\n",
      "135000/135000 [==============================] - 378s - loss: 0.4809 - acc: 0.8095 - val_loss: 0.4871 - val_acc: 0.8024\n",
      "Epoch 39/150\n",
      "135000/135000 [==============================] - 376s - loss: 0.4528 - acc: 0.8222 - val_loss: 0.5256 - val_acc: 0.7921\n",
      "Epoch 40/150\n",
      "135000/135000 [==============================] - 375s - loss: 0.4237 - acc: 0.8345 - val_loss: 0.4794 - val_acc: 0.8088\n",
      "Epoch 41/150\n",
      "135000/135000 [==============================] - 375s - loss: 0.3994 - acc: 0.8456 - val_loss: 0.3838 - val_acc: 0.8483\n",
      "Epoch 42/150\n",
      "135000/135000 [==============================] - 375s - loss: 0.3750 - acc: 0.8553 - val_loss: 0.7066 - val_acc: 0.7713\n",
      "Epoch 43/150\n",
      "135000/135000 [==============================] - 373s - loss: 0.3532 - acc: 0.8646 - val_loss: 0.5279 - val_acc: 0.8098\n",
      "Epoch 44/150\n",
      "135000/135000 [==============================] - 376s - loss: 0.3352 - acc: 0.8736 - val_loss: 0.3177 - val_acc: 0.8758\n",
      "Epoch 45/150\n",
      "135000/135000 [==============================] - 374s - loss: 0.3172 - acc: 0.8798 - val_loss: 0.2757 - val_acc: 0.8948\n",
      "Epoch 46/150\n",
      "135000/135000 [==============================] - 376s - loss: 0.3002 - acc: 0.8876 - val_loss: 0.3690 - val_acc: 0.8646\n",
      "Epoch 47/150\n",
      "135000/135000 [==============================] - 377s - loss: 0.2862 - acc: 0.8940 - val_loss: 0.6808 - val_acc: 0.8064\n",
      "Epoch 48/150\n",
      "135000/135000 [==============================] - 380s - loss: 0.2685 - acc: 0.8988 - val_loss: 0.2807 - val_acc: 0.8906\n",
      "Epoch 49/150\n",
      "135000/135000 [==============================] - 375s - loss: 0.2574 - acc: 0.9048 - val_loss: 0.2795 - val_acc: 0.8931\n",
      "Epoch 50/150\n",
      "135000/135000 [==============================] - 375s - loss: 0.2432 - acc: 0.9097 - val_loss: 0.4212 - val_acc: 0.8545\n",
      "Epoch 51/150\n",
      "135000/135000 [==============================] - 375s - loss: 0.2334 - acc: 0.9152 - val_loss: 0.2551 - val_acc: 0.9025\n",
      "Epoch 52/150\n",
      "135000/135000 [==============================] - 378s - loss: 0.2192 - acc: 0.9191 - val_loss: 0.2534 - val_acc: 0.9036\n",
      "Epoch 53/150\n",
      "135000/135000 [==============================] - 377s - loss: 0.2109 - acc: 0.9235 - val_loss: 0.2800 - val_acc: 0.8947\n",
      "Epoch 54/150\n",
      "135000/135000 [==============================] - 439s - loss: 0.1995 - acc: 0.9275 - val_loss: 0.2266 - val_acc: 0.9133\n",
      "Epoch 55/150\n",
      "135000/135000 [==============================] - 489s - loss: 0.1934 - acc: 0.9306 - val_loss: 0.1866 - val_acc: 0.9296\n",
      "Epoch 56/150\n",
      "135000/135000 [==============================] - 490s - loss: 0.1798 - acc: 0.9346 - val_loss: 0.3240 - val_acc: 0.8838\n",
      "Epoch 57/150\n",
      "135000/135000 [==============================] - 461s - loss: 0.1739 - acc: 0.9377 - val_loss: 0.2297 - val_acc: 0.9186\n",
      "Epoch 58/150\n",
      "135000/135000 [==============================] - 374s - loss: 0.1676 - acc: 0.9405 - val_loss: 0.1644 - val_acc: 0.9388\n",
      "Epoch 59/150\n",
      "135000/135000 [==============================] - 374s - loss: 0.1586 - acc: 0.9433 - val_loss: 0.2308 - val_acc: 0.9174\n",
      "Epoch 60/150\n",
      "135000/135000 [==============================] - 374s - loss: 0.1540 - acc: 0.9460 - val_loss: 0.2254 - val_acc: 0.9179\n",
      "Epoch 61/150\n",
      "135000/135000 [==============================] - 374s - loss: 0.1482 - acc: 0.9481 - val_loss: 0.1677 - val_acc: 0.9375\n",
      "Epoch 62/150\n",
      "135000/135000 [==============================] - 374s - loss: 0.1415 - acc: 0.9508 - val_loss: 0.1750 - val_acc: 0.9349\n",
      "Epoch 63/150\n",
      "135000/135000 [==============================] - 374s - loss: 0.1379 - acc: 0.9524 - val_loss: 0.1413 - val_acc: 0.9479\n",
      "Epoch 64/150\n",
      "135000/135000 [==============================] - 374s - loss: 0.1310 - acc: 0.9546 - val_loss: 0.2346 - val_acc: 0.9177\n",
      "Epoch 65/150\n",
      "135000/135000 [==============================] - 374s - loss: 0.1248 - acc: 0.9562 - val_loss: 0.1423 - val_acc: 0.9478\n",
      "Epoch 66/150\n",
      "135000/135000 [==============================] - 374s - loss: 0.1224 - acc: 0.9582 - val_loss: 0.2978 - val_acc: 0.9060\n",
      "Epoch 67/150\n",
      "135000/135000 [==============================] - 374s - loss: 0.1179 - acc: 0.9597 - val_loss: 0.1366 - val_acc: 0.9500\n",
      "Epoch 68/150\n",
      "135000/135000 [==============================] - 374s - loss: 0.1119 - acc: 0.9612 - val_loss: 0.1447 - val_acc: 0.9477\n",
      "Epoch 69/150\n",
      "135000/135000 [==============================] - 374s - loss: 0.1099 - acc: 0.9626 - val_loss: 0.1762 - val_acc: 0.9377\n",
      "Epoch 70/150\n",
      "135000/135000 [==============================] - 374s - loss: 0.1042 - acc: 0.9642 - val_loss: 0.2121 - val_acc: 0.9281\n",
      "Epoch 71/150\n",
      "135000/135000 [==============================] - 374s - loss: 0.1021 - acc: 0.9658 - val_loss: 0.1285 - val_acc: 0.9534\n",
      "Epoch 72/150\n",
      "135000/135000 [==============================] - 375s - loss: 0.0985 - acc: 0.9667 - val_loss: 0.1357 - val_acc: 0.9509\n",
      "Epoch 73/150\n",
      "135000/135000 [==============================] - 374s - loss: 0.0950 - acc: 0.9678 - val_loss: 0.1203 - val_acc: 0.9568\n",
      "Epoch 74/150\n",
      "135000/135000 [==============================] - 374s - loss: 0.0920 - acc: 0.9690 - val_loss: 0.1207 - val_acc: 0.9571\n",
      "Epoch 75/150\n",
      "135000/135000 [==============================] - 374s - loss: 0.0877 - acc: 0.9702 - val_loss: 0.1460 - val_acc: 0.9480\n",
      "Epoch 76/150\n",
      "135000/135000 [==============================] - 374s - loss: 0.0869 - acc: 0.9711 - val_loss: 0.2813 - val_acc: 0.9129\n",
      "Epoch 77/150\n",
      "135000/135000 [==============================] - 374s - loss: 0.0850 - acc: 0.9720 - val_loss: 0.1514 - val_acc: 0.9470\n",
      "Epoch 78/150\n",
      "135000/135000 [==============================] - 374s - loss: 0.0797 - acc: 0.9728 - val_loss: 0.1394 - val_acc: 0.9520\n",
      "Epoch 79/150\n",
      "135000/135000 [==============================] - 374s - loss: 0.0803 - acc: 0.9739 - val_loss: 0.1326 - val_acc: 0.9544\n",
      "Epoch 80/150\n",
      "135000/135000 [==============================] - 374s - loss: 0.0770 - acc: 0.9747 - val_loss: 0.1155 - val_acc: 0.9596\n",
      "Epoch 81/150\n",
      "135000/135000 [==============================] - 374s - loss: 0.0748 - acc: 0.9756 - val_loss: 0.1182 - val_acc: 0.9588\n",
      "Epoch 82/150\n",
      "135000/135000 [==============================] - 374s - loss: 0.0712 - acc: 0.9762 - val_loss: 0.1442 - val_acc: 0.9523\n",
      "Epoch 83/150\n",
      "135000/135000 [==============================] - 374s - loss: 0.0713 - acc: 0.9770 - val_loss: 0.1341 - val_acc: 0.9552\n",
      "Epoch 84/150\n",
      "135000/135000 [==============================] - 375s - loss: 0.0684 - acc: 0.9775 - val_loss: 0.1118 - val_acc: 0.9616\n",
      "Epoch 85/150\n",
      "135000/135000 [==============================] - 374s - loss: 0.0674 - acc: 0.9780 - val_loss: 0.1279 - val_acc: 0.9578\n",
      "Epoch 86/150\n",
      "135000/135000 [==============================] - 374s - loss: 0.0638 - acc: 0.9789 - val_loss: 0.1208 - val_acc: 0.9585\n",
      "Epoch 87/150\n",
      "135000/135000 [==============================] - 374s - loss: 0.0657 - acc: 0.9796 - val_loss: 0.1094 - val_acc: 0.9630\n",
      "Epoch 88/150\n",
      "135000/135000 [==============================] - 374s - loss: 0.0624 - acc: 0.9801 - val_loss: 0.1194 - val_acc: 0.9599\n",
      "Epoch 89/150\n",
      "135000/135000 [==============================] - 382s - loss: 0.0599 - acc: 0.9805 - val_loss: 0.1568 - val_acc: 0.9489\n",
      "Epoch 90/150\n",
      "135000/135000 [==============================] - 385s - loss: 0.0589 - acc: 0.9812 - val_loss: 0.1110 - val_acc: 0.9627\n",
      "Epoch 91/150\n",
      "135000/135000 [==============================] - 385s - loss: 0.0565 - acc: 0.9814 - val_loss: 0.1112 - val_acc: 0.9626\n",
      "Epoch 92/150\n",
      "135000/135000 [==============================] - 384s - loss: 0.0573 - acc: 0.9818 - val_loss: 0.1032 - val_acc: 0.9654\n",
      "Epoch 93/150\n",
      "135000/135000 [==============================] - 379s - loss: 0.0564 - acc: 0.9825 - val_loss: 0.1438 - val_acc: 0.9543\n",
      "Epoch 94/150\n",
      "135000/135000 [==============================] - 375s - loss: 0.0545 - acc: 0.9828 - val_loss: 0.1022 - val_acc: 0.9659\n",
      "Epoch 95/150\n",
      "135000/135000 [==============================] - 376s - loss: 0.0535 - acc: 0.9834 - val_loss: 0.2060 - val_acc: 0.9410\n",
      "Epoch 96/150\n",
      "135000/135000 [==============================] - 377s - loss: 0.0511 - acc: 0.9835 - val_loss: 0.1492 - val_acc: 0.9545\n",
      "Epoch 97/150\n",
      "135000/135000 [==============================] - 376s - loss: 0.0510 - acc: 0.9841 - val_loss: 0.1131 - val_acc: 0.9634\n",
      "Epoch 98/150\n",
      "135000/135000 [==============================] - 376s - loss: 0.0487 - acc: 0.9843 - val_loss: 0.1019 - val_acc: 0.9665\n",
      "Epoch 99/150\n",
      "135000/135000 [==============================] - 375s - loss: 0.0483 - acc: 0.9847 - val_loss: 0.1002 - val_acc: 0.9672\n",
      "Epoch 100/150\n",
      "135000/135000 [==============================] - 376s - loss: 0.0487 - acc: 0.9848 - val_loss: 0.0913 - val_acc: 0.9701\n",
      "Epoch 101/150\n",
      "135000/135000 [==============================] - 375s - loss: 0.0464 - acc: 0.9855 - val_loss: 0.1094 - val_acc: 0.9644\n",
      "Epoch 102/150\n",
      "135000/135000 [==============================] - 376s - loss: 0.0450 - acc: 0.9856 - val_loss: 0.0969 - val_acc: 0.9685\n",
      "Epoch 103/150\n",
      "135000/135000 [==============================] - 376s - loss: 0.0460 - acc: 0.9861 - val_loss: 0.1050 - val_acc: 0.9668\n",
      "Epoch 104/150\n",
      "135000/135000 [==============================] - 375s - loss: 0.0439 - acc: 0.9862 - val_loss: 0.1352 - val_acc: 0.9600\n",
      "Epoch 105/150\n",
      "135000/135000 [==============================] - 375s - loss: 0.0431 - acc: 0.9865 - val_loss: 0.1189 - val_acc: 0.9631\n",
      "Epoch 106/150\n",
      "135000/135000 [==============================] - 375s - loss: 0.0418 - acc: 0.9866 - val_loss: 0.0942 - val_acc: 0.9698\n",
      "Epoch 107/150\n",
      "135000/135000 [==============================] - 375s - loss: 0.0413 - acc: 0.9870 - val_loss: 0.0916 - val_acc: 0.9705\n",
      "10000/10000 [==============================] - 13s    \n",
      "Test score: 0.0915344823897\n",
      "Test accuracy: 0.970488011837\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=6)\n",
    "hist = model.fit(X, y, batch_size=BATCH_SIZE, nb_epoch=150, \n",
    "                 callbacks=[early_stopping],\n",
    "          validation_split = 0.1, shuffle=True)\n",
    "\n",
    "score, acc = model.evaluate(X_test, y_test,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            show_accuracy=True)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
