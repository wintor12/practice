{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GeForce GTX 980 Ti (CNMeM is disabled, CuDNN 4007)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM, GRU\n",
    "from keras.preprocessing import text\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import EarlyStopping\n",
    "from __future__ import print_function\n",
    "from keras.layers.core import Activation, TimeDistributedDense, RepeatVector\n",
    "from keras.layers import recurrent\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done loading\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "wv = Word2Vec.load_word2vec_format(\"/home/tong/Documents/python/GoogleNews-vectors-negative300.bin.gz\", binary = True)\n",
    "print(\"done\" + \" loading\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CharacterTable(object):\n",
    "    '''\n",
    "        Given a set of characters:\n",
    "        + Encode them to a one hot integer representation\n",
    "        + Decode the one hot integer representation to their character output\n",
    "        + Decode a vector of probabilties to their character output\n",
    "        '''\n",
    "    def __init__(self, vocab, maxlen, wv):\n",
    "        self.vocab = vocab\n",
    "        self.char_indices = dict((c, i) for i, c in enumerate(self.vocab))\n",
    "        self.indices_char = dict((i, c) for i, c in enumerate(self.vocab))\n",
    "        self.maxlen = maxlen\n",
    "        self.wv = wv\n",
    "        self.embedding = {}\n",
    "        for i, c in enumerate(self.vocab):\n",
    "            if c in wv:\n",
    "                self.embedding[c] = wv[c]\n",
    "            else:\n",
    "                self.embedding[c] = np.random.rand(300)\n",
    "    \n",
    "    def encode_onehot(self, C, maxlen=None):\n",
    "        maxlen = maxlen if maxlen else self.maxlen\n",
    "        X = np.zeros((maxlen, len(self.vocab)))\n",
    "        for i, c in enumerate(C):\n",
    "            try:\n",
    "                X[i, self.char_indices[c]] = 1\n",
    "            except KeyError:\n",
    "                X[i, self.char_indices[' ']] = 1\n",
    "        return X\n",
    "    \n",
    "    def encode(self, C, maxlen=None):\n",
    "        maxlen = maxlen if maxlen else self.maxlen\n",
    "        X = np.zeros((maxlen, 300))\n",
    "        for i, c in enumerate(C):\n",
    "            try:\n",
    "                X[i] = self.embedding[c]\n",
    "            except KeyError:\n",
    "                X[i] = np.random.rand(300)\n",
    "        return X\n",
    "    \n",
    "    def decode(self, X, calc_argmax=True):\n",
    "        if calc_argmax:\n",
    "            X = X.argmax(axis=-1)\n",
    "        return ' '.join(self.indices_char[x] for x in X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "token_pattern=r\"(?u)\\b\\w\\w+\\b\"\n",
    "def build_tokenizer():\n",
    "    \"\"\"Return a function that splits a string into a sequence of tokens\"\"\"\n",
    "    pattern = re.compile(token_pattern)\n",
    "    return lambda doc: pattern.findall(doc)\n",
    "\n",
    "\n",
    "def readData(src):\n",
    "    b1 = []\n",
    "    b2 = []\n",
    "    with open(src) as p:\n",
    "        for i, line in enumerate(p):\n",
    "            s = line.split('\\t')\n",
    "            if len(s) == 2:\n",
    "                b1.append(s[0])\n",
    "                b2.append(s[1][:-1]) #remove \\n\n",
    "                lines = i + 1\n",
    "    return b1, b2, lines\n",
    "\n",
    "\n",
    "def readGs(src):\n",
    "    b = []\n",
    "    with open(src) as p:\n",
    "        for i, line in enumerate(p):\n",
    "            b.append(round(float(line),0))\n",
    "            lines = i + 1\n",
    "    return b, lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9092"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msr = './dataset/STS2012-train/STS.input.MSRpar.txt'\n",
    "msrvid = './dataset/STS2012-train/STS.input.MSRvid.txt'\n",
    "smt = './dataset/STS2012-train/STS.input.SMTeuroparl.txt'\n",
    "b1_12_1, b2_12_1, l_12_1 = readData(msr)\n",
    "b1_12_2, b2_12_2, l_12_2 = readData(msrvid)\n",
    "b1_12_3, b2_12_3, l_12_3 = readData(smt)\n",
    "lines_12 = l_12_1 + l_12_2 + l_12_3\n",
    "b1_12_train = b1_12_1 + b1_12_2 + b1_12_3\n",
    "b2_12_train = b2_12_1 + b2_12_2 + b2_12_3\n",
    "\n",
    "\n",
    "msr_gs = './dataset/STS2012-train/STS.gs.MSRpar.txt'\n",
    "msr_gs_vid = './dataset/STS2012-train/STS.gs.MSRvid.txt'\n",
    "smt_gs = './dataset/STS2012-train/STS.gs.SMTeuroparl.txt'\n",
    "b_12_train = readGs(msr_gs)[0]\n",
    "b_12_train = b_12_train + readGs(msr_gs_vid)[0]\n",
    "b_12_train = b_12_train + readGs(smt_gs)[0]\n",
    "print(len(b_12_train) == len(b1_12_train) == len(b2_12_train))\n",
    "\n",
    "msr_test = './dataset/STS2012-test/STS.input.MSRpar.txt'\n",
    "vid_test = './dataset/STS2012-test/STS.input.MSRvid.txt'\n",
    "smt_test = './dataset/STS2012-test/STS.input.SMTeuroparl.txt'\n",
    "surprise_test = './dataset/STS2012-test/STS.input.surprise.OnWN.txt'\n",
    "surprise2_test = './dataset/STS2012-test/STS.input.surprise.SMTnews.txt'\n",
    "b1_12_1t, b2_12_1t, l_12_1t = readData(msr_test)\n",
    "\n",
    "b1_12_2t, b2_12_2t, l_12_2t = readData(vid_test)\n",
    "\n",
    "b1_12_3t, b2_12_3t, l_12_3t = readData(smt_test)\n",
    "\n",
    "b1_12_4t, b2_12_4t, l_12_4t = readData(surprise_test)\n",
    "\n",
    "b1_12_5t, b2_12_5t, l_12_5t = readData(surprise2_test)\n",
    "\n",
    "lines = l_12_1t + l_12_2t + l_12_3t + l_12_4t + l_12_5t\n",
    "b1_12_test = b1_12_1t + b1_12_2t + b1_12_3t + b1_12_4t + b1_12_5t\n",
    "b2_12_test = b2_12_1t + b2_12_2t + b2_12_3t + b2_12_4t + b2_12_5t\n",
    "\n",
    "\n",
    "\n",
    "msr_test_gs = './dataset/STS2012-test/STS.gs.MSRpar.txt'\n",
    "vid_test_gs = './dataset/STS2012-test/STS.gs.MSRvid.txt'\n",
    "smt_test_gs = './dataset/STS2012-test/STS.gs.SMTeuroparl.txt'\n",
    "surprise_test_gs = './dataset/STS2012-test/STS.gs.surprise.OnWN.txt'\n",
    "surprise2_test_gs = './dataset/STS2012-test/STS.gs.surprise.SMTnews.txt'\n",
    "b_12_test = readGs(msr_test_gs)[0]\n",
    "b_12_test = b_12_test + readGs(vid_test_gs)[0]\n",
    "b_12_test = b_12_test + readGs(smt_test_gs)[0]\n",
    "b_12_test = b_12_test + readGs(surprise_test_gs)[0]\n",
    "b_12_test = b_12_test + readGs(surprise2_test_gs)[0]\n",
    "print(len(b_12_test) == len(b1_12_test) == len(b2_12_test))\n",
    "\n",
    "t14_f = './dataset/STS2014-test/STS.input.deft-forum.txt'\n",
    "t14_n = './dataset/STS2014-test/STS.input.deft-news.txt'\n",
    "t14_h = './dataset/STS2014-test/STS.input.headlines.txt'\n",
    "t14_i = './dataset/STS2014-test/STS.input.images.txt'\n",
    "t14_o = './dataset/STS2014-test/STS.input.OnWN.txt'\n",
    "t14_t = './dataset/STS2014-test/STS.input.tweet-news.txt'\n",
    "b1_14_1t, b2_14_1t, l_14_1t = readData(t14_f)\n",
    "\n",
    "b1_14_2t, b2_14_2t, l_14_2t = readData(t14_n)\n",
    "\n",
    "b1_14_3t, b2_14_3t, l_14_3t = readData(t14_h)\n",
    "\n",
    "b1_14_4t, b2_14_4t, l_14_4t = readData(t14_i)\n",
    "\n",
    "b1_14_5t, b2_14_5t, l_14_5t = readData(t14_o)\n",
    "\n",
    "b1_14_6t, b2_14_6t, l_14_6t = readData(t14_t)\n",
    "\n",
    "b1_14_test = b1_14_1t + b1_14_2t + b1_14_3t + b1_14_4t + b1_14_5t + b1_14_6t\n",
    "b2_14_test = b2_14_1t + b2_14_2t + b2_14_3t + b2_14_4t + b2_14_5t + b2_14_6t\n",
    "lines = l_14_1t + l_14_2t + l_14_3t + l_14_4t + l_14_5t + l_14_6t\n",
    "\n",
    "\n",
    "t14_f_gs = './dataset/STS2014-test/STS.gs.deft-forum.txt'\n",
    "t14_n_gs = './dataset/STS2014-test/STS.gs.deft-news.txt'\n",
    "t14_h_gs = './dataset/STS2014-test/STS.gs.headlines.txt'\n",
    "t14_i_gs = './dataset/STS2014-test/STS.gs.images.txt'\n",
    "t14_o_gs = './dataset/STS2014-test/STS.gs.OnWN.txt'\n",
    "t14_t_gs = './dataset/STS2014-test/STS.gs.tweet-news.txt'\n",
    "b_14_test = readGs(t14_f_gs)[0]\n",
    "b_14_test = b_14_test + readGs(t14_n_gs)[0]\n",
    "b_14_test = b_14_test + readGs(t14_h_gs)[0]\n",
    "b_14_test = b_14_test + readGs(t14_i_gs)[0]\n",
    "b_14_test = b_14_test + readGs(t14_o_gs)[0]\n",
    "b_14_test = b_14_test + readGs(t14_t_gs)[0]\n",
    "print(len(b_14_test) == len(b1_14_test) == len(b2_14_test))\n",
    "\n",
    "b1 = b1_12_train + b1_12_test + b1_14_test\n",
    "b2 = b2_12_train + b2_12_test + b2_14_test\n",
    "y_train = b_12_train + b_12_test + b_14_test\n",
    "print(len(b1) == len(b2) == len(y_train))\n",
    "len(b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14478"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "vectors = vectorizer.fit_transform(b1 + b2)\n",
    "vectors.shape\n",
    "vocab = vectorizer.get_feature_names()\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAXLEN = 20\n",
    "vocab.append(' ') #add empty word for padding\n",
    "\n",
    "b1 = [x.lower() for x in b1]\n",
    "b2 = [x.lower() for x in b2]\n",
    "tokenize = build_tokenizer()\n",
    "b1_tokens = [tokenize(x)[:MAXLEN] for x in b1]\n",
    "b2_tokens = [tokenize(x)[:MAXLEN] for x in b2]\n",
    "#padding\n",
    "b1_tokens = [s + [' '] * (MAXLEN - len(s)) for s in b1_tokens]\n",
    "b2_tokens = [s + [' '] * (MAXLEN - len(s)) for s in b2_tokens]\n",
    "#Reverse\n",
    "# b1_tokens = [s[::-1] for s in b1_tokens]\n",
    "# b2_tokens = [s[::-1] for s in b2_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ctable = CharacterTable(vocab, MAXLEN, wv)\n",
    "X = np.zeros((len(b1), MAXLEN, 300), dtype=np.float)\n",
    "y = np.zeros((len(b1), MAXLEN, len(vocab)), dtype=np.float)\n",
    "for i, sentence in enumerate(b1_tokens):\n",
    "    X[i] = ctable.encode(sentence, maxlen=MAXLEN)\n",
    "\n",
    "for i, sentence in enumerate(b2_tokens):\n",
    "    y[i] = ctable.encode_onehot(sentence, maxlen=MAXLEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 256\n",
    "BATCH_SIZE = 128\n",
    "LAYERS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "# \"Encode\" the input sequence using an RNN, producing an output of HIDDEN_SIZE\n",
    "# note: in a situation where your input sequences have a variable length,\n",
    "# use input_shape=(None, nb_feature).\n",
    "model.add(LSTM(HIDDEN_SIZE, dropout_W=0.5, dropout_U=0.1, input_shape=(MAXLEN, 300)))\n",
    "# For the decoder's input, we repeat the encoded input for each time step\n",
    "# We use repeatvector here because we only need the last state, not the whole sequence\n",
    "model.add(RepeatVector(MAXLEN))\n",
    "# The decoder RNN could be multiple layers stacked or a single layer\n",
    "for _ in range(LAYERS):\n",
    "    model.add(LSTM(HIDDEN_SIZE, dropout_W=0.5, dropout_U=0.1, return_sequences=True))\n",
    "\n",
    "# For each of step of the output sequence, decide which character should be chosen\n",
    "model.add(TimeDistributedDense(300))\n",
    "model.add(Activation('tanh'))\n",
    "model.add(TimeDistributedDense(len(vocab)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adagrad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8182 samples, validate on 910 samples\n",
      "Epoch 1/5\n",
      "8182/8182 [==============================] - 91s - loss: 3.4763 - acc: 0.6902 - val_loss: 2.9436 - val_acc: 0.7407\n",
      "Epoch 2/5\n",
      "8182/8182 [==============================] - 91s - loss: 3.0551 - acc: 0.7058 - val_loss: 2.8746 - val_acc: 0.7407\n",
      "Epoch 3/5\n",
      "8182/8182 [==============================] - 92s - loss: 2.9941 - acc: 0.7058 - val_loss: 2.8841 - val_acc: 0.7407\n",
      "Epoch 4/5\n",
      "8182/8182 [==============================] - 94s - loss: 2.9643 - acc: 0.7058 - val_loss: 2.8853 - val_acc: 0.7407\n",
      "Epoch 5/5\n",
      "8182/8182 [==============================] - 97s - loss: 2.9425 - acc: 0.7058 - val_loss: 2.8876 - val_acc: 0.7407\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4ad7957c10>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, batch_size=BATCH_SIZE, nb_epoch=5,\n",
    "          show_accuracy=True,validation_split = 0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' '],\n",
       " [' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' '],\n",
       " [' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' '],\n",
       " [' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' '],\n",
       " [' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' '],\n",
       " [' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' '],\n",
       " [' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' '],\n",
       " [' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' '],\n",
       " [' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' '],\n",
       " [' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = model.predict_classes(X[:10])\n",
    "res_sentences = []\n",
    "for r in res:\n",
    "    sent = []\n",
    "    for i in range(MAXLEN):\n",
    "        sent.append(ctable.indices_char[r[i]])\n",
    "    res_sentences.append(sent)\n",
    "\n",
    "res_sentences"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
