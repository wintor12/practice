{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM, GRU\n",
    "from keras.preprocessing import text\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import EarlyStopping\n",
    "from __future__ import print_function\n",
    "from keras.layers.core import Activation, TimeDistributedDense, RepeatVector\n",
    "from keras.layers import recurrent\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CharacterTable(object):\n",
    "    def __init__(self, vocab, maxlen):\n",
    "        self.vocab = vocab\n",
    "        self.maxlen = maxlen\n",
    "    \n",
    "    def encode(self, C, maxlen=None):\n",
    "        maxlen = maxlen if maxlen else self.maxlen\n",
    "        X = np.zeros((maxlen, len(self.vocab)))\n",
    "        for i, c in enumerate(C):\n",
    "            X[i, c] = 1\n",
    "        return X\n",
    "    \n",
    "    def decode(self, X, calc_argmax=True):\n",
    "        if calc_argmax:\n",
    "            X = X.argmax(axis=-1)\n",
    "        return ','.join(x for x in X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generateRandSeq(min, max, len):\n",
    "    return [np.random.randint(min, max) for _ in range(len)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAINING_SIZE = 150000\n",
    "TEST_SIZE = 10000\n",
    "DIGITS = 25\n",
    "MAXLEN = DIGITS\n",
    "voc = list(xrange(1000))\n",
    "ctable = CharacterTable(voc, MAXLEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating data...\n",
      "[506, 709, 915, 329, 844, 952, 812, 363, 653, 547, 151, 341, 730, 772, 778, 668, 684, 124, 743, 751, 125, 23, 664, 483, 935]\n",
      "[106, 109, 115, 129, 44, 152, 12, 163, 53, 147, 151, 141, 130, 172, 178, 68, 84, 124, 143, 151, 125, 23, 64, 83, 135]\n",
      "[477, 456, 648, 456, 52, 204, 311, 100, 552, 604, 273, 796, 407, 728, 391, 645, 812, 784, 804, 512, 101, 196, 684, 794, 300]\n",
      "[77, 56, 48, 56, 52, 4, 111, 100, 152, 4, 73, 196, 7, 128, 191, 45, 12, 184, 4, 112, 101, 196, 84, 194, 100]\n"
     ]
    }
   ],
   "source": [
    "inputs = []\n",
    "outputs = []\n",
    "inputs_t = []\n",
    "outputs_t = []\n",
    "print('Generating data...')\n",
    "while len(inputs) < TRAINING_SIZE:\n",
    "    s = generateRandSeq(0, len(voc), DIGITS)\n",
    "    inputs.append(s)\n",
    "    outputs.append([i%200 for i in s])\n",
    "\n",
    "while len(inputs_t) < TEST_SIZE:\n",
    "    s = generateRandSeq(0, len(voc), DIGITS)\n",
    "    inputs_t.append(s)\n",
    "    outputs_t.append([i%200 for i in s])\n",
    "print(inputs[12])\n",
    "print(outputs[12])\n",
    "print(inputs_t[15])\n",
    "print(outputs_t[15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization...\n",
      "(150000, 25)\n",
      "(150000, 25, 1000)\n",
      "(10000, 25)\n",
      "(10000, 25, 1000)\n"
     ]
    }
   ],
   "source": [
    "print('Vectorization...')\n",
    "X = np.zeros((len(inputs), MAXLEN), dtype=np.int32)\n",
    "# y = np.zeros((len(outputs), MAXLEN), dtype=np.int32)\n",
    "y = np.zeros((len(outputs), MAXLEN, len(voc)), dtype=np.bool)\n",
    "for i, sentence in enumerate(inputs):\n",
    "    X[i] = inputs[i]\n",
    "\n",
    "# for i, sentence in enumerate(outputs):\n",
    "#     y[i] = outputs[i]\n",
    "for i, sentence in enumerate(outputs):\n",
    "    y[i] = ctable.encode(sentence, maxlen=MAXLEN)\n",
    "\n",
    "X_test = np.zeros((len(inputs_t), MAXLEN), dtype=np.int32)\n",
    "# y_test = np.zeros((len(outputs_t), MAXLEN), dtype=np.int32)\n",
    "y_test = np.zeros((len(outputs_t), MAXLEN, len(voc)), dtype=np.bool)\n",
    "for i, sentence in enumerate(inputs_t):\n",
    "    X_test[i] = inputs_t[i]\n",
    "\n",
    "# for i, sentence in enumerate(outputs_t):\n",
    "#     y_test[i] = outputs_t[i]\n",
    "for i, sentence in enumerate(outputs_t):\n",
    "    y_test[i] = ctable.encode(sentence, maxlen=MAXLEN)\n",
    "    \n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([28, 27, 81, 75, 78, 11, 27, 43, 13,  2, 96, 43, 20, 62, 63,  4, 33,\n",
       "       73, 35, 70, 62, 37, 14, 96, 69], dtype=int32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "HIDDEN_SIZE = 256\n",
    "BATCH_SIZE = 200\n",
    "LAYERS = 2\n",
    "'''\n",
    "Hey guys, I also met this problem and I found this thread. Basically, \n",
    "the error info can happen when the dimension of the input data (X_train or Y_train) doesn't match with the \n",
    "model's input shape.\n",
    "\n",
    "In my case (and @LeavesBreathe 's case I guess), the problem is that \n",
    "the model is expecting the Y_train to be a 3d tensor. Because of the embedding layer, \n",
    "the 2d tensor X_train of size (n_batch, sequence_length) will be eventually converted to a 3d tensor of size \n",
    "(n_batch, sequence_length, embedding_size) and will be processed by the succeeding LSTM layer. However, \n",
    "the 2d tensor Y_train of size (n_sample, sequence_length) is not converted to 3d, \n",
    "which is needed by the decoder LSTM.\n",
    "\n",
    "To fix this problem, what I did is to convert Y_train into a 3d binary tensor (binary one-hot coding) and it worked.\n",
    "'''\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(voc), 300, input_length = MAXLEN))\n",
    "model.add(LSTM(HIDDEN_SIZE, return_sequences=True))\n",
    "for _ in range(LAYERS - 2):\n",
    "    model.add(LSTM(HIDDEN_SIZE, return_sequences=True))\n",
    "\n",
    "model.add(LSTM(HIDDEN_SIZE))\n",
    "model.add(RepeatVector(MAXLEN))\n",
    "for _ in range(LAYERS):\n",
    "    model.add(LSTM(HIDDEN_SIZE, return_sequences=True))\n",
    "\n",
    "model.add(TimeDistributedDense(input_dim=HIDDEN_SIZE, output_dim=300))\n",
    "model.add(Activation('tanh'))\n",
    "model.add(TimeDistributedDense(input_dim=300, output_dim=len(voc)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(optimizer='RMSprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 135000 samples, validate on 15000 samples\n",
      "Epoch 1/150\n",
      "135000/135000 [==============================] - 374s - loss: 5.3083 - acc: 0.0056 - val_loss: 5.2434 - val_acc: 0.0073\n",
      "Epoch 2/150\n",
      "135000/135000 [==============================] - 374s - loss: 5.1445 - acc: 0.0081 - val_loss: 5.0673 - val_acc: 0.0089\n",
      "Epoch 3/150\n",
      "135000/135000 [==============================] - 376s - loss: 4.9740 - acc: 0.0116 - val_loss: 4.8680 - val_acc: 0.0137\n",
      "Epoch 4/150\n",
      "135000/135000 [==============================] - 374s - loss: 4.7922 - acc: 0.0175 - val_loss: 4.6744 - val_acc: 0.0215\n",
      "Epoch 5/150\n",
      "135000/135000 [==============================] - 374s - loss: 4.6167 - acc: 0.0254 - val_loss: 4.4681 - val_acc: 0.0308\n",
      "Epoch 6/150\n",
      "135000/135000 [==============================] - 374s - loss: 4.4597 - acc: 0.0333 - val_loss: 4.3366 - val_acc: 0.0310\n",
      "Epoch 7/150\n",
      "135000/135000 [==============================] - 374s - loss: 4.3305 - acc: 0.0403 - val_loss: 4.4098 - val_acc: 0.0437\n",
      "Epoch 8/150\n",
      "135000/135000 [==============================] - 374s - loss: 4.2219 - acc: 0.0471 - val_loss: 4.0913 - val_acc: 0.0497\n",
      "Epoch 9/150\n",
      "135000/135000 [==============================] - 374s - loss: 4.1151 - acc: 0.0539 - val_loss: 4.3559 - val_acc: 0.0483\n",
      "Epoch 10/150\n",
      "135000/135000 [==============================] - 374s - loss: 3.9545 - acc: 0.0589 - val_loss: 3.7755 - val_acc: 0.0609\n",
      "Epoch 11/150\n",
      "135000/135000 [==============================] - 374s - loss: 3.7870 - acc: 0.0650 - val_loss: 4.2425 - val_acc: 0.0555\n",
      "Epoch 12/150\n",
      "135000/135000 [==============================] - 375s - loss: 3.6294 - acc: 0.0727 - val_loss: 3.5175 - val_acc: 0.0615\n",
      "Epoch 13/150\n",
      "135000/135000 [==============================] - 374s - loss: 3.4757 - acc: 0.0805 - val_loss: 3.4049 - val_acc: 0.0717\n",
      "Epoch 14/150\n",
      "135000/135000 [==============================] - 375s - loss: 3.3375 - acc: 0.0895 - val_loss: 3.2861 - val_acc: 0.0883\n",
      "Epoch 15/150\n",
      "135000/135000 [==============================] - 374s - loss: 3.2106 - acc: 0.0989 - val_loss: 3.1607 - val_acc: 0.0838\n",
      "Epoch 16/150\n",
      "135000/135000 [==============================] - 375s - loss: 3.0960 - acc: 0.1098 - val_loss: 3.2261 - val_acc: 0.1102\n",
      "Epoch 17/150\n",
      "135000/135000 [==============================] - 374s - loss: 2.9597 - acc: 0.1256 - val_loss: 2.9177 - val_acc: 0.1096\n",
      "Epoch 18/150\n",
      "135000/135000 [==============================] - 375s - loss: 2.8108 - acc: 0.1462 - val_loss: 2.6946 - val_acc: 0.1521\n",
      "Epoch 19/150\n",
      "135000/135000 [==============================] - 374s - loss: 2.6527 - acc: 0.1720 - val_loss: 2.7759 - val_acc: 0.1649\n",
      "Epoch 20/150\n",
      "135000/135000 [==============================] - 375s - loss: 2.4764 - acc: 0.2064 - val_loss: 2.5409 - val_acc: 0.2090\n",
      "Epoch 21/150\n",
      "135000/135000 [==============================] - 374s - loss: 2.2748 - acc: 0.2511 - val_loss: 2.1600 - val_acc: 0.2775\n",
      "Epoch 22/150\n",
      "135000/135000 [==============================] - 375s - loss: 2.0766 - acc: 0.3021 - val_loss: 1.9886 - val_acc: 0.3243\n",
      "Epoch 23/150\n",
      "135000/135000 [==============================] - 374s - loss: 1.8980 - acc: 0.3512 - val_loss: 1.8649 - val_acc: 0.3537\n",
      "Epoch 24/150\n",
      "135000/135000 [==============================] - 308s - loss: 1.7308 - acc: 0.4013 - val_loss: 1.5432 - val_acc: 0.4655\n",
      "Epoch 25/150\n",
      "135000/135000 [==============================] - 282s - loss: 1.5613 - acc: 0.4557 - val_loss: 1.4898 - val_acc: 0.4795\n",
      "Epoch 26/150\n",
      "135000/135000 [==============================] - 282s - loss: 1.3977 - acc: 0.5131 - val_loss: 1.3276 - val_acc: 0.5333\n",
      "Epoch 27/150\n",
      "135000/135000 [==============================] - 282s - loss: 1.2682 - acc: 0.5559 - val_loss: 1.2922 - val_acc: 0.5523\n",
      "Epoch 28/150\n",
      "135000/135000 [==============================] - 282s - loss: 1.1577 - acc: 0.5930 - val_loss: 1.0992 - val_acc: 0.6167\n",
      "Epoch 29/150\n",
      "135000/135000 [==============================] - 282s - loss: 1.0603 - acc: 0.6262 - val_loss: 1.1208 - val_acc: 0.6015\n",
      "Epoch 30/150\n",
      "135000/135000 [==============================] - 282s - loss: 0.9779 - acc: 0.6555 - val_loss: 0.9797 - val_acc: 0.6558\n",
      "Epoch 31/150\n",
      "135000/135000 [==============================] - 282s - loss: 0.9018 - acc: 0.6826 - val_loss: 0.9491 - val_acc: 0.6713\n",
      "Epoch 32/150\n",
      "135000/135000 [==============================] - 282s - loss: 0.8350 - acc: 0.7064 - val_loss: 0.8212 - val_acc: 0.7113\n",
      "Epoch 33/150\n",
      "135000/135000 [==============================] - 282s - loss: 0.7722 - acc: 0.7299 - val_loss: 0.8077 - val_acc: 0.7140\n",
      "Epoch 34/150\n",
      "135000/135000 [==============================] - 282s - loss: 0.7112 - acc: 0.7528 - val_loss: 0.7554 - val_acc: 0.7307\n",
      "Epoch 35/150\n",
      "135000/135000 [==============================] - 282s - loss: 0.6552 - acc: 0.7737 - val_loss: 0.7537 - val_acc: 0.7293\n",
      "Epoch 36/150\n",
      "135000/135000 [==============================] - 282s - loss: 0.6005 - acc: 0.7952 - val_loss: 0.6320 - val_acc: 0.7898\n",
      "Epoch 37/150\n",
      "135000/135000 [==============================] - 282s - loss: 0.5497 - acc: 0.8158 - val_loss: 0.5498 - val_acc: 0.8089\n",
      "Epoch 38/150\n",
      "135000/135000 [==============================] - 282s - loss: 0.4965 - acc: 0.8366 - val_loss: 0.4345 - val_acc: 0.8609\n",
      "Epoch 39/150\n",
      "135000/135000 [==============================] - 282s - loss: 0.4469 - acc: 0.8568 - val_loss: 0.6555 - val_acc: 0.7868\n",
      "Epoch 40/150\n",
      "135000/135000 [==============================] - 282s - loss: 0.3944 - acc: 0.8799 - val_loss: 0.3802 - val_acc: 0.8766\n",
      "Epoch 41/150\n",
      "135000/135000 [==============================] - 282s - loss: 0.3500 - acc: 0.8974 - val_loss: 0.1345 - val_acc: 0.9641\n",
      "Epoch 42/150\n",
      "135000/135000 [==============================] - 281s - loss: 0.3077 - acc: 0.9143 - val_loss: 0.3771 - val_acc: 0.8869\n",
      "Epoch 43/150\n",
      "135000/135000 [==============================] - 282s - loss: 0.2736 - acc: 0.9270 - val_loss: 0.0909 - val_acc: 0.9764\n",
      "Epoch 44/150\n",
      "135000/135000 [==============================] - 282s - loss: 0.2423 - acc: 0.9368 - val_loss: 0.2535 - val_acc: 0.9331\n",
      "Epoch 45/150\n",
      "135000/135000 [==============================] - 282s - loss: 0.2219 - acc: 0.9442 - val_loss: 0.1226 - val_acc: 0.9615\n",
      "Epoch 46/150\n",
      "135000/135000 [==============================] - 282s - loss: 0.1938 - acc: 0.9526 - val_loss: 0.0596 - val_acc: 0.9840\n",
      "Epoch 47/150\n",
      "135000/135000 [==============================] - 282s - loss: 0.1797 - acc: 0.9574 - val_loss: 0.7681 - val_acc: 0.8324\n",
      "Epoch 48/150\n",
      "135000/135000 [==============================] - 282s - loss: 0.1663 - acc: 0.9608 - val_loss: 0.0457 - val_acc: 0.9881\n",
      "Epoch 49/150\n",
      "135000/135000 [==============================] - 282s - loss: 0.1508 - acc: 0.9654 - val_loss: 0.0407 - val_acc: 0.9895\n",
      "Epoch 50/150\n",
      "135000/135000 [==============================] - 282s - loss: 0.1380 - acc: 0.9689 - val_loss: 0.1576 - val_acc: 0.9637\n",
      "Epoch 51/150\n",
      "135000/135000 [==============================] - 282s - loss: 0.1332 - acc: 0.9704 - val_loss: 0.0565 - val_acc: 0.9860\n",
      "Epoch 52/150\n",
      "135000/135000 [==============================] - 282s - loss: 0.1247 - acc: 0.9728 - val_loss: 0.0272 - val_acc: 0.9934\n",
      "Epoch 53/150\n",
      "135000/135000 [==============================] - 282s - loss: 0.1181 - acc: 0.9745 - val_loss: 0.0278 - val_acc: 0.9930\n",
      "Epoch 54/150\n",
      "135000/135000 [==============================] - 282s - loss: 0.1098 - acc: 0.9757 - val_loss: 0.0747 - val_acc: 0.9804\n",
      "Epoch 55/150\n",
      "135000/135000 [==============================] - 282s - loss: 0.1094 - acc: 0.9775 - val_loss: 0.0211 - val_acc: 0.9949\n",
      "Epoch 56/150\n",
      "135000/135000 [==============================] - 282s - loss: 0.1036 - acc: 0.9780 - val_loss: 0.0292 - val_acc: 0.9917\n",
      "Epoch 57/150\n",
      "135000/135000 [==============================] - 282s - loss: 0.0986 - acc: 0.9789 - val_loss: 0.0261 - val_acc: 0.9931\n",
      "Epoch 58/150\n",
      "135000/135000 [==============================] - 282s - loss: 0.0955 - acc: 0.9801 - val_loss: 0.0227 - val_acc: 0.9944\n",
      "Epoch 59/150\n",
      "135000/135000 [==============================] - 282s - loss: 0.0906 - acc: 0.9809 - val_loss: 0.0224 - val_acc: 0.9940\n",
      "Epoch 60/150\n",
      "135000/135000 [==============================] - 282s - loss: 0.0860 - acc: 0.9821 - val_loss: 0.0224 - val_acc: 0.9936\n",
      "Epoch 61/150\n",
      "135000/135000 [==============================] - 282s - loss: 0.0863 - acc: 0.9822 - val_loss: 0.0141 - val_acc: 0.9966\n",
      "Epoch 62/150\n",
      "135000/135000 [==============================] - 282s - loss: 0.0831 - acc: 0.9828 - val_loss: 0.0187 - val_acc: 0.9952\n",
      "Epoch 63/150\n",
      "135000/135000 [==============================] - 282s - loss: 0.0804 - acc: 0.9833 - val_loss: 0.0190 - val_acc: 0.9951\n",
      "Epoch 64/150\n",
      "135000/135000 [==============================] - 282s - loss: 0.0777 - acc: 0.9844 - val_loss: 0.0133 - val_acc: 0.9966\n",
      "Epoch 65/150\n",
      "135000/135000 [==============================] - 282s - loss: 0.0773 - acc: 0.9844 - val_loss: 0.7257 - val_acc: 0.8857\n",
      "Epoch 66/150\n",
      "135000/135000 [==============================] - 282s - loss: 0.0764 - acc: 0.9847 - val_loss: 0.0416 - val_acc: 0.9870\n",
      "Epoch 67/150\n",
      "135000/135000 [==============================] - 282s - loss: 0.0708 - acc: 0.9855 - val_loss: 0.0107 - val_acc: 0.9974\n",
      "Epoch 68/150\n",
      "135000/135000 [==============================] - 282s - loss: 0.0705 - acc: 0.9857 - val_loss: 0.0088 - val_acc: 0.9981\n",
      "Epoch 69/150\n",
      "135000/135000 [==============================] - 282s - loss: 0.0700 - acc: 0.9857 - val_loss: 0.0204 - val_acc: 0.9942\n",
      "Epoch 70/150\n",
      "135000/135000 [==============================] - 282s - loss: 0.0695 - acc: 0.9859 - val_loss: 0.0151 - val_acc: 0.9959\n",
      "Epoch 71/150\n",
      "135000/135000 [==============================] - 282s - loss: 0.0655 - acc: 0.9862 - val_loss: 0.0094 - val_acc: 0.9978\n",
      "Epoch 72/150\n",
      "135000/135000 [==============================] - 282s - loss: 0.0665 - acc: 0.9863 - val_loss: 0.0737 - val_acc: 0.9757\n",
      "Epoch 73/150\n",
      "135000/135000 [==============================] - 282s - loss: 0.0642 - acc: 0.9869 - val_loss: 0.0422 - val_acc: 0.9866\n",
      "Epoch 74/150\n",
      "135000/135000 [==============================] - 282s - loss: 0.0621 - acc: 0.9871 - val_loss: 0.0225 - val_acc: 0.9934\n",
      "Epoch 75/150\n",
      "135000/135000 [==============================] - 281s - loss: 0.0675 - acc: 0.9868 - val_loss: 0.0362 - val_acc: 0.9884\n",
      "10000/10000 [==============================] - 9s     \n",
      "Test score: 0.0360735890269\n",
      "Test accuracy: 0.988488013744\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=6)\n",
    "hist = model.fit(X, y, batch_size=BATCH_SIZE, nb_epoch=150, \n",
    "                 callbacks=[early_stopping],\n",
    "          validation_split = 0.1, shuffle=True)\n",
    "\n",
    "score, acc = model.evaluate(X_test, y_test,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            show_accuracy=True)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 8s     \n",
      "Test score: 0.00171989639057\n",
      "Test accuracy: 0.999540005922\n"
     ]
    }
   ],
   "source": [
    "score, acc = model.evaluate(X_test, y_test,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            show_accuracy=True)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
