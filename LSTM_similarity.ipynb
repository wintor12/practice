{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM,GRU\n",
    "from keras.preprocessing import text\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "token_pattern=r\"(?u)\\b\\w\\w+\\b\"\n",
    "def build_tokenizer():\n",
    "    \"\"\"Return a function that splits a string into a sequence of tokens\"\"\"\n",
    "    pattern = re.compile(token_pattern)\n",
    "    return lambda doc: pattern.findall(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readData(src):\n",
    "    b1 = []\n",
    "    b2 = []\n",
    "    with open(src) as p:\n",
    "        for i, line in enumerate(p):\n",
    "            s = line.split('\\t')\n",
    "            if len(s) == 2:\n",
    "                b1.append(s[0])\n",
    "                b2.append(s[1][:-1]) #remove \\n\n",
    "                lines = i + 1\n",
    "    return b1, b2, lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readGs(src):\n",
    "    b = []\n",
    "    with open(src) as p:\n",
    "        for i, line in enumerate(p):\n",
    "            b.append(round(float(line),0))\n",
    "            lines = i + 1\n",
    "    return b, lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read 2012 trainning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750\n",
      "750\n",
      "734\n",
      "2234\n"
     ]
    }
   ],
   "source": [
    "msr = './dataset/STS2012-train/STS.input.MSRpar.txt'\n",
    "msrvid = './dataset/STS2012-train/STS.input.MSRvid.txt'\n",
    "smt = './dataset/STS2012-train/STS.input.SMTeuroparl.txt'\n",
    "b1_12_1, b2_12_1, l_12_1 = readData(msr)\n",
    "print l_12_1\n",
    "b1_12_2, b2_12_2, l_12_2 = readData(msrvid)\n",
    "print l_12_2\n",
    "b1_12_3, b2_12_3, l_12_3 = readData(smt)\n",
    "print l_12_3\n",
    "lines_12 = l_12_1 + l_12_2 + l_12_3\n",
    "b1_12_train = b1_12_1 + b1_12_2 + b1_12_3\n",
    "b2_12_train = b2_12_1 + b2_12_2 + b2_12_3\n",
    "print lines_12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "msr_gs = './dataset/STS2012-train/STS.gs.MSRpar.txt'\n",
    "msr_gs_vid = './dataset/STS2012-train/STS.gs.MSRvid.txt'\n",
    "smt_gs = './dataset/STS2012-train/STS.gs.SMTeuroparl.txt'\n",
    "b_12_train = readGs(msr_gs)[0]\n",
    "b_12_train = b_12_train + readGs(msr_gs_vid)[0]\n",
    "b_12_train = b_12_train + readGs(smt_gs)[0]\n",
    "print len(b_12_train) == len(b1_12_train) == len(b2_12_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read 2012 test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750\n",
      "750\n",
      "459\n",
      "750\n",
      "399\n",
      "3108\n"
     ]
    }
   ],
   "source": [
    "msr_test = './dataset/STS2012-test/STS.input.MSRpar.txt'\n",
    "vid_test = './dataset/STS2012-test/STS.input.MSRvid.txt'\n",
    "smt_test = './dataset/STS2012-test/STS.input.SMTeuroparl.txt'\n",
    "surprise_test = './dataset/STS2012-test/STS.input.surprise.OnWN.txt'\n",
    "surprise2_test = './dataset/STS2012-test/STS.input.surprise.SMTnews.txt'\n",
    "b1_12_1t, b2_12_1t, l_12_1t = readData(msr_test)\n",
    "print l_12_1t\n",
    "b1_12_2t, b2_12_2t, l_12_2t = readData(vid_test)\n",
    "print l_12_2t\n",
    "b1_12_3t, b2_12_3t, l_12_3t = readData(smt_test)\n",
    "print l_12_3t\n",
    "b1_12_4t, b2_12_4t, l_12_4t = readData(surprise_test)\n",
    "print l_12_4t\n",
    "b1_12_5t, b2_12_5t, l_12_5t = readData(surprise2_test)\n",
    "print l_12_5t\n",
    "lines = l_12_1t + l_12_2t + l_12_3t + l_12_4t + l_12_5t\n",
    "b1_12_test = b1_12_1t + b1_12_2t + b1_12_3t + b1_12_4t + b1_12_5t\n",
    "b2_12_test = b2_12_1t + b2_12_2t + b2_12_3t + b2_12_4t + b2_12_5t\n",
    "print lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "msr_test_gs = './dataset/STS2012-test/STS.gs.MSRpar.txt'\n",
    "vid_test_gs = './dataset/STS2012-test/STS.gs.MSRvid.txt'\n",
    "smt_test_gs = './dataset/STS2012-test/STS.gs.SMTeuroparl.txt'\n",
    "surprise_test_gs = './dataset/STS2012-test/STS.gs.surprise.OnWN.txt'\n",
    "surprise2_test_gs = './dataset/STS2012-test/STS.gs.surprise.SMTnews.txt'\n",
    "b_12_test = readGs(msr_test_gs)[0]\n",
    "b_12_test = b_12_test + readGs(vid_test_gs)[0]\n",
    "b_12_test = b_12_test + readGs(smt_test_gs)[0]\n",
    "b_12_test = b_12_test + readGs(surprise_test_gs)[0]\n",
    "b_12_test = b_12_test + readGs(surprise2_test_gs)[0]\n",
    "print len(b_12_test) == len(b1_12_test) == len(b2_12_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read 2014 test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450\n",
      "300\n",
      "750\n",
      "750\n",
      "750\n",
      "750\n",
      "3750\n"
     ]
    }
   ],
   "source": [
    "t14_f = './dataset/STS2014-test/STS.input.deft-forum.txt'\n",
    "t14_n = './dataset/STS2014-test/STS.input.deft-news.txt'\n",
    "t14_h = './dataset/STS2014-test/STS.input.headlines.txt'\n",
    "t14_i = './dataset/STS2014-test/STS.input.images.txt'\n",
    "t14_o = './dataset/STS2014-test/STS.input.OnWN.txt'\n",
    "t14_t = './dataset/STS2014-test/STS.input.tweet-news.txt'\n",
    "b1_14_1t, b2_14_1t, l_14_1t = readData(t14_f)\n",
    "print l_14_1t\n",
    "b1_14_2t, b2_14_2t, l_14_2t = readData(t14_n)\n",
    "print l_14_2t\n",
    "b1_14_3t, b2_14_3t, l_14_3t = readData(t14_h)\n",
    "print l_14_3t\n",
    "b1_14_4t, b2_14_4t, l_14_4t = readData(t14_i)\n",
    "print l_14_4t\n",
    "b1_14_5t, b2_14_5t, l_14_5t = readData(t14_o)\n",
    "print l_14_5t\n",
    "b1_14_6t, b2_14_6t, l_14_6t = readData(t14_t)\n",
    "print l_14_6t\n",
    "b1_14_test = b1_14_1t + b1_14_2t + b1_14_3t + b1_14_4t + b1_14_5t + b1_14_6t\n",
    "b2_14_test = b2_14_1t + b2_14_2t + b2_14_3t + b2_14_4t + b2_14_5t + b2_14_6t\n",
    "lines = l_14_1t + l_14_2t + l_14_3t + l_14_4t + l_14_5t + l_14_6t\n",
    "print lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "t14_f_gs = './dataset/STS2014-test/STS.gs.deft-forum.txt'\n",
    "t14_n_gs = './dataset/STS2014-test/STS.gs.deft-news.txt'\n",
    "t14_h_gs = './dataset/STS2014-test/STS.gs.headlines.txt'\n",
    "t14_i_gs = './dataset/STS2014-test/STS.gs.images.txt'\n",
    "t14_o_gs = './dataset/STS2014-test/STS.gs.OnWN.txt'\n",
    "t14_t_gs = './dataset/STS2014-test/STS.gs.tweet-news.txt'\n",
    "b_14_test = readGs(t14_f_gs)[0]\n",
    "b_14_test = b_14_test + readGs(t14_n_gs)[0]\n",
    "b_14_test = b_14_test + readGs(t14_h_gs)[0]\n",
    "b_14_test = b_14_test + readGs(t14_i_gs)[0]\n",
    "b_14_test = b_14_test + readGs(t14_o_gs)[0]\n",
    "b_14_test = b_14_test + readGs(t14_t_gs)[0]\n",
    "print len(b_14_test) == len(b1_14_test) == len(b2_14_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add all years train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# b1 = b1_12_train + b1_12_test + b1_14_test\n",
    "# b2 = b2_12_train + b2_12_test + b2_14_test\n",
    "# y_train = b_12_train + b_12_test + b_14_test\n",
    "# print len(b1) == len(b2) == len(y_train)\n",
    "## Double input size\n",
    "b1 = b1_12_train + b1_12_test + b1_14_test + b2_12_train + b2_12_test + b2_14_test\n",
    "b2 = b2_12_train + b2_12_test + b2_14_test + b1_12_train + b1_12_test + b1_14_test\n",
    "y_train = b_12_train + b_12_test + b_14_test + b_12_train + b_12_test + b_14_test\n",
    "print len(b1) == len(b2) == len(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read 2015 train data and 2013 test data as validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1570\n"
     ]
    }
   ],
   "source": [
    "val_f = './dataset/STS2015-train/STS.input.answers-forum.txt'\n",
    "val_s = './dataset/STS2015-train/STS.input.answers-students.txt'\n",
    "val_b = './dataset/STS2015-train/STS.input.belief.txt'\n",
    "val_h = './dataset/STS2015-train/STS.input.headlines.txt'\n",
    "val_i = './dataset/STS2015-train/STS.input.images.txt'\n",
    "val_fn = './dataset/STS2013-test/STS.input.FNWN.txt'\n",
    "val_he = './dataset/STS2013-test/STS.input.headlines.txt'\n",
    "val_on = './dataset/STS2013-test/STS.input.OnWN.txt'\n",
    "v1_15_1, v2_15_1, l_15_1 = readData(val_f)\n",
    "v1_15_2, v2_15_2, l_15_2 = readData(val_s)\n",
    "v1_15_3, v2_15_3, l_15_3 = readData(val_b)\n",
    "v1_15_4, v2_15_4, l_15_4 = readData(val_h)\n",
    "v1_15_5, v2_15_5, l_15_5 = readData(val_i)\n",
    "v1_13_1, v2_13_1, l_13_1 = readData(val_fn)\n",
    "v1_13_2, v2_13_2, l_13_2 = readData(val_he)\n",
    "v1_13_3, v2_13_3, l_13_3 = readData(val_on)\n",
    "lines = l_15_1 + l_15_2 + l_15_3 + l_15_4 + l_15_5 + l_13_1 + l_13_2 + l_13_3\n",
    "v1 = v1_15_1 + v1_15_2 + v1_15_3 + v1_15_4 + v1_15_5 + v1_13_1 + v1_13_2 + v1_13_3\n",
    "v2 = v2_15_1 + v2_15_2 + v2_15_3 + v2_15_4 + v2_15_5 + v2_13_1 + v2_13_2 + v2_13_3\n",
    "print lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1570\n"
     ]
    }
   ],
   "source": [
    "val_gs_f = './dataset/STS2015-train/STS.gs.answers-forum.txt'\n",
    "val_gs_s = './dataset/STS2015-train/STS.gs.answers-students.txt'\n",
    "val_gs_b = './dataset/STS2015-train/STS.gs.belief.txt'\n",
    "val_gs_h = './dataset/STS2015-train/STS.gs.headlines.txt'\n",
    "val_gs_i = './dataset/STS2015-train/STS.gs.images.txt'\n",
    "val_gs_fn = './dataset/STS2013-test/STS.gs.FNWN.txt'\n",
    "val_gs_he = './dataset/STS2013-test/STS.gs.headlines.txt'\n",
    "val_gs_on = './dataset/STS2013-test/STS.gs.OnWN.txt'\n",
    "y_val = readGs(val_gs_f)[0]\n",
    "y_val = y_val + readGs(val_gs_s)[0]\n",
    "y_val = y_val + readGs(val_gs_b)[0]\n",
    "y_val = y_val + readGs(val_gs_h)[0]\n",
    "y_val = y_val + readGs(val_gs_i)[0]\n",
    "y_val = y_val + readGs(val_gs_fn)[0]\n",
    "y_val = y_val + readGs(val_gs_he)[0]\n",
    "y_val = y_val + readGs(val_gs_on)[0]\n",
    "print len(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read 2015 test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "1500\n",
      "2000\n",
      "1500\n",
      "1500\n"
     ]
    }
   ],
   "source": [
    "test_f = './dataset/STS2015-test/STS.input.answers-forums.txt'\n",
    "b1_test_f, b2_test_f, lines_f = readData(test_f)\n",
    "test_s = './dataset/STS2015-test/STS.input.answers-students.txt'\n",
    "b1_test_s, b2_test_s, lines_s = readData(test_s)\n",
    "test_b = './dataset/STS2015-test/STS.input.belief.txt'\n",
    "b1_test_b, b2_test_b, lines_b = readData(test_b)\n",
    "test_h = './dataset/STS2015-test/STS.input.headlines.txt'\n",
    "b1_test_h, b2_test_h, lines_h = readData(test_h)\n",
    "test_i = './dataset/STS2015-test/STS.input.images.txt'\n",
    "b1_test_i, b2_test_i, lines_i = readData(test_i)\n",
    "print lines_f\n",
    "print lines_s\n",
    "print lines_b\n",
    "print lines_h\n",
    "print lines_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14758\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectors = vectorizer.fit_transform(b1 + b2)\n",
    "# vectors_test = vectorizer.transform(b1_test + b2_test)\n",
    "vectors.shape\n",
    "vocab = vectorizer.get_feature_names()\n",
    "print len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:7: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:13: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    }
   ],
   "source": [
    "tokenize = build_tokenizer()\n",
    "X_train1 = []\n",
    "X_train2 = []\n",
    "for seq in b1:\n",
    "    s = []\n",
    "    for word in tokenize(seq):\n",
    "        if word in vocab:\n",
    "            s.append(vectorizer.vocabulary_[word] + 1)\n",
    "    X_train1.append(s)\n",
    "for seq in b2:\n",
    "    s = []\n",
    "    for word in tokenize(seq):\n",
    "        if word in vocab:\n",
    "            s.append(vectorizer.vocabulary_[word] + 1)\n",
    "    X_train2.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18184\n",
      "18184\n"
     ]
    }
   ],
   "source": [
    "print len(X_train1)\n",
    "print len(X_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:6: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:12: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:34: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:40: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:48: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:54: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    }
   ],
   "source": [
    "X_test1_f = []\n",
    "X_test2_f = []\n",
    "for seq in b1_test_f:\n",
    "    s = []\n",
    "    for word in tokenize(seq):\n",
    "        if word in vocab:\n",
    "            s.append(vectorizer.vocabulary_[word] + 1)\n",
    "    X_test1_f.append(s)\n",
    "for seq in b2_test_f:\n",
    "    s = []\n",
    "    for word in tokenize(seq):\n",
    "        if word in vocab:\n",
    "            s.append(vectorizer.vocabulary_[word] + 1)\n",
    "    X_test2_f.append(s)\n",
    "X_test1_s = []\n",
    "X_test2_s = []\n",
    "for seq in b1_test_s:\n",
    "    s = []\n",
    "    for word in tokenize(seq):\n",
    "        if word in vocab:\n",
    "            s.append(vectorizer.vocabulary_[word] + 1)\n",
    "    X_test1_s.append(s)\n",
    "for seq in b2_test_s:\n",
    "    s = []\n",
    "    for word in tokenize(seq):\n",
    "        if word in vocab:\n",
    "            s.append(vectorizer.vocabulary_[word] + 1)\n",
    "    X_test2_s.append(s)\n",
    "X_test1_b = []\n",
    "X_test2_b = []\n",
    "for seq in b1_test_b:\n",
    "    s = []\n",
    "    for word in tokenize(seq):\n",
    "        if word in vocab:\n",
    "            s.append(vectorizer.vocabulary_[word] + 1)\n",
    "    X_test1_b.append(s)\n",
    "for seq in b2_test_b:\n",
    "    s = []\n",
    "    for word in tokenize(seq):\n",
    "        if word in vocab:\n",
    "            s.append(vectorizer.vocabulary_[word] + 1)\n",
    "    X_test2_b.append(s)\n",
    "X_test1_h = []\n",
    "X_test2_h = []\n",
    "for seq in b1_test_h:\n",
    "    s = []\n",
    "    for word in tokenize(seq):\n",
    "        if word in vocab:\n",
    "            s.append(vectorizer.vocabulary_[word] + 1)\n",
    "    X_test1_h.append(s)\n",
    "for seq in b2_test_h:\n",
    "    s = []\n",
    "    for word in tokenize(seq):\n",
    "        if word in vocab:\n",
    "            s.append(vectorizer.vocabulary_[word] + 1)\n",
    "    X_test2_h.append(s)\n",
    "X_test1_i = []\n",
    "X_test2_i = []\n",
    "for seq in b1_test_i:\n",
    "    s = []\n",
    "    for word in tokenize(seq):\n",
    "        if word in vocab:\n",
    "            s.append(vectorizer.vocabulary_[word] + 1)\n",
    "    X_test1_i.append(s)\n",
    "for seq in b2_test_i:\n",
    "    s = []\n",
    "    for word in tokenize(seq):\n",
    "        if word in vocab:\n",
    "            s.append(vectorizer.vocabulary_[word] + 1)\n",
    "    X_test2_i.append(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "1500\n",
      "2000\n",
      "1500\n",
      "1500\n"
     ]
    }
   ],
   "source": [
    "print len(X_test1_f)\n",
    "print len(X_test2_s)\n",
    "print len(X_test1_b)\n",
    "print len(X_test2_h)\n",
    "print len(X_test1_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1570\n",
      "1570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:6: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:12: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    }
   ],
   "source": [
    "X_val1 = []\n",
    "X_val2 = []\n",
    "for seq in v1:\n",
    "    s = []\n",
    "    for word in tokenize(seq):\n",
    "        if word in vocab:\n",
    "            s.append(vectorizer.vocabulary_[word] + 1)\n",
    "    X_val1.append(s)\n",
    "for seq in v2:\n",
    "    s = []\n",
    "    for word in tokenize(seq):\n",
    "        if word in vocab:\n",
    "            s.append(vectorizer.vocabulary_[word] + 1)\n",
    "    X_val2.append(s)\n",
    "print len(X_val1)\n",
    "print len(X_val2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('X_train shape:', (18184, 25))\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 25\n",
    "X_train1 = sequence.pad_sequences(X_train1, maxlen=MAX_LEN)\n",
    "X_train2 = sequence.pad_sequences(X_train2, maxlen=MAX_LEN)\n",
    "\n",
    "print('X_train shape:', X_train1.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('X_val shape:', (1570, 25))\n"
     ]
    }
   ],
   "source": [
    "X_val1 = sequence.pad_sequences(X_val1, maxlen=MAX_LEN)\n",
    "X_val2 = sequence.pad_sequences(X_val2, maxlen=MAX_LEN)\n",
    "\n",
    "print('X_val shape:', X_val1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('X_test1_f shape:', (2000, 25))\n",
      "('X_test2_f shape:', (2000, 25))\n"
     ]
    }
   ],
   "source": [
    "X_test1_f = sequence.pad_sequences(X_test1_f,  maxlen=MAX_LEN)\n",
    "X_test2_f = sequence.pad_sequences(X_test2_f,  maxlen=MAX_LEN)\n",
    "print('X_test1_f shape:', X_test1_f.shape)\n",
    "print('X_test2_f shape:', X_test2_f.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_train, y_val = [np_utils.to_categorical(x) for x in (y_train, y_val)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use pre_trained word2vec embedding for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done loading\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "wv = Word2Vec.load_word2vec_format(\"/home/tong/Documents/python/GoogleNews-vectors-negative300.bin.gz\", binary = True)\n",
    "print \"done\" + \" loading\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2103\n"
     ]
    }
   ],
   "source": [
    "vocab_dim = 300 # dimensionality of your word vectors\n",
    "n_symbols = len(vocab) + 1 # adding 1 to account for 0th index (for masking)\n",
    "embedding_weights = np.random.rand(n_symbols,vocab_dim)\n",
    "count = 0 ## count of words not in word2vec\n",
    "for word in vocab:\n",
    "    if word in wv:\n",
    "        embedding_weights[vectorizer.vocabulary_[word] + 1,:] = wv[word]\n",
    "    else:\n",
    "#         print word\n",
    "        count += 1\n",
    "print count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14570\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print vectorizer.vocabulary_[\"woman\"] + 1\n",
    "print np.array_equal(wv['woman'], embedding_weights[vectorizer.vocabulary_[\"woman\"] + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Build complete\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Merge\n",
    "print('Build model...')\n",
    "encoder_a = Sequential()\n",
    "encoder_a.add(Embedding(n_symbols, vocab_dim, input_length=MAX_LEN, weights=[embedding_weights]))\n",
    "encoder_a.add(GRU(vocab_dim, dropout_W=0.5, dropout_U=0.1))  # try using a GRU instead, for fun\n",
    "encoder_a.add(Dropout(0.5))\n",
    "\n",
    "encoder_b = Sequential()\n",
    "encoder_b.add(Embedding(n_symbols, vocab_dim, input_length=MAX_LEN, weights=[embedding_weights]))\n",
    "encoder_b.add(GRU(vocab_dim, dropout_W=0.5, dropout_U=0.1)) \n",
    "encoder_b.add(Dropout(0.5))\n",
    "\n",
    "decoder = Sequential()\n",
    "decoder.add(Merge([encoder_a, encoder_b], mode='concat'))\n",
    "decoder.add(Dense(6, activation='softmax'))\n",
    "decoder.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "print('Build complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Layer is not connected. Did you forget to set \"input_shape\"?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-9b8d84c8d145>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mforwards_a\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout_W\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout_U\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mbackwards_a\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout_W\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout_U\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgo_backwards\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mmerged_a\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mforwards_a\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackwards_a\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'concat'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconcat_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mencoder_a\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmerged_a\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mencoder_a\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/keras/layers/core.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, layers, mode, concat_axis, dot_axes)\u001b[0m\n\u001b[0;32m    488\u001b[0m             \u001b[0minput_shapes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    489\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 490\u001b[1;33m                 \u001b[0moshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    491\u001b[0m                 \u001b[0moshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconcat_axis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m                 \u001b[0moshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/keras/layers/recurrent.pyc\u001b[0m in \u001b[0;36moutput_shape\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0moutput_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m         \u001b[0minput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreturn_sequences\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/keras/layers/core.pyc\u001b[0m in \u001b[0;36minput_shape\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    208\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_input_shape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 210\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Layer is not connected. Did you forget to set \"input_shape\"?'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mset_input_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: Layer is not connected. Did you forget to set \"input_shape\"?"
     ]
    }
   ],
   "source": [
    "from keras.layers import Merge\n",
    "print('Build model...')\n",
    "encoder_a = Sequential()\n",
    "encoder_a.add(Embedding(n_symbols, vocab_dim, input_length=MAX_LEN, weights=[embedding_weights]))\n",
    "forwards_a = LSTM(vocab_dim, dropout_W=0.5, dropout_U=0.1)\n",
    "backwards_a = LSTM(vocab_dim, dropout_W=0.5, dropout_U=0.1, go_backwards=True)\n",
    "merged_a = Merge([forwards_a, backwards_a], mode='concat', concat_axis=-1)\n",
    "encoder_a.add(merged_a)\n",
    "encoder_a.add(Dropout(0.5))\n",
    "\n",
    "encoder_b = Sequential()\n",
    "encoder_b.add(Embedding(n_symbols, vocab_dim, input_length=MAX_LEN, weights=[embedding_weights]))\n",
    "forwards_b = LSTM(vocab_dim, dropout_W=0.5, dropout_U=0.1)\n",
    "backwards_b = LSTM(vocab_dim, dropout_W=0.5, dropout_U=0.1, go_backwards=True)\n",
    "encoder_b.add(Merge([forwards_b, backwards_b], mode='concat', concat_axis=-1))\n",
    "encoder_b.add(Dropout(0.5))\n",
    "\n",
    "decoder = Sequential()\n",
    "decoder.add(Merge([encoder_a, encoder_b], mode='concat'))\n",
    "decoder.add(Dense(6, activation='softmax'))\n",
    "decoder.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "print('Build complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1570\n"
     ]
    }
   ],
   "source": [
    "print len(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 18184 samples, validate on 1570 samples\n",
      "Epoch 1/100\n",
      "18184/18184 [==============================] - 28s - loss: 1.6424 - acc: 0.3043 - val_loss: 2.0317 - val_acc: 0.2707\n",
      "Epoch 2/100\n",
      "18184/18184 [==============================] - 28s - loss: 1.5490 - acc: 0.3441 - val_loss: 1.9093 - val_acc: 0.2752\n",
      "Epoch 3/100\n",
      "18184/18184 [==============================] - 28s - loss: 1.5021 - acc: 0.3740 - val_loss: 1.9118 - val_acc: 0.2739\n",
      "Epoch 4/100\n",
      "18184/18184 [==============================] - 28s - loss: 1.4672 - acc: 0.3892 - val_loss: 1.8596 - val_acc: 0.2739\n",
      "Epoch 5/100\n",
      "18184/18184 [==============================] - 28s - loss: 1.4298 - acc: 0.4091 - val_loss: 1.9940 - val_acc: 0.2777\n",
      "Epoch 6/100\n",
      "18184/18184 [==============================] - 28s - loss: 1.3984 - acc: 0.4238 - val_loss: 1.8867 - val_acc: 0.2911\n",
      "Epoch 7/100\n",
      "18184/18184 [==============================] - 28s - loss: 1.3778 - acc: 0.4351 - val_loss: 1.8601 - val_acc: 0.2860\n",
      "Epoch 8/100\n",
      "18184/18184 [==============================] - 28s - loss: 1.3410 - acc: 0.4528 - val_loss: 1.9041 - val_acc: 0.2624\n",
      "Epoch 9/100\n",
      "18184/18184 [==============================] - 28s - loss: 1.3167 - acc: 0.4722 - val_loss: 1.9462 - val_acc: 0.2713\n",
      "Epoch 10/100\n",
      "18184/18184 [==============================] - 28s - loss: 1.2829 - acc: 0.4843 - val_loss: 1.9076 - val_acc: 0.2758\n",
      "Epoch 11/100\n",
      "18184/18184 [==============================] - 28s - loss: 1.2549 - acc: 0.4984 - val_loss: 1.9990 - val_acc: 0.2682\n",
      "Epoch 12/100\n",
      "18184/18184 [==============================] - 28s - loss: 1.2216 - acc: 0.5104 - val_loss: 1.9854 - val_acc: 0.2783\n",
      "Epoch 13/100\n",
      "18184/18184 [==============================] - 28s - loss: 1.1808 - acc: 0.5354 - val_loss: 2.1248 - val_acc: 0.2529\n",
      "Epoch 14/100\n",
      "18184/18184 [==============================] - 28s - loss: 1.1610 - acc: 0.5429 - val_loss: 2.0481 - val_acc: 0.2713\n",
      "Epoch 15/100\n",
      "18184/18184 [==============================] - 28s - loss: 1.1241 - acc: 0.5651 - val_loss: 2.0923 - val_acc: 0.2529\n",
      "Epoch 16/100\n",
      "18184/18184 [==============================] - 28s - loss: 1.0872 - acc: 0.5752 - val_loss: 2.2057 - val_acc: 0.2471\n",
      "Epoch 17/100\n",
      "18184/18184 [==============================] - 28s - loss: 1.0496 - acc: 0.5872 - val_loss: 2.1422 - val_acc: 0.2484\n",
      "Epoch 18/100\n",
      "18184/18184 [==============================] - 28s - loss: 1.0170 - acc: 0.6095 - val_loss: 2.1242 - val_acc: 0.2599\n",
      "Epoch 19/100\n",
      "18184/18184 [==============================] - 28s - loss: 0.9874 - acc: 0.6201 - val_loss: 2.2592 - val_acc: 0.2465\n",
      "Epoch 20/100\n",
      "18184/18184 [==============================] - 28s - loss: 0.9568 - acc: 0.6309 - val_loss: 2.2895 - val_acc: 0.2580\n",
      "Epoch 21/100\n",
      "18184/18184 [==============================] - 28s - loss: 0.9173 - acc: 0.6479 - val_loss: 2.3805 - val_acc: 0.2459\n",
      "Epoch 22/100\n",
      "18184/18184 [==============================] - 28s - loss: 0.8819 - acc: 0.6648 - val_loss: 2.4183 - val_acc: 0.2420\n",
      "Epoch 23/100\n",
      "18184/18184 [==============================] - 28s - loss: 0.8551 - acc: 0.6726 - val_loss: 2.4863 - val_acc: 0.2331\n",
      "Epoch 24/100\n",
      "18184/18184 [==============================] - 28s - loss: 0.8192 - acc: 0.6880 - val_loss: 2.5091 - val_acc: 0.2452\n",
      "Epoch 25/100\n",
      "18184/18184 [==============================] - 28s - loss: 0.8026 - acc: 0.6956 - val_loss: 2.5440 - val_acc: 0.2599\n",
      "Epoch 26/100\n",
      "18184/18184 [==============================] - 29s - loss: 0.7695 - acc: 0.7108 - val_loss: 2.6759 - val_acc: 0.2408\n",
      "Epoch 27/100\n",
      "18184/18184 [==============================] - 28s - loss: 0.7396 - acc: 0.7216 - val_loss: 2.7017 - val_acc: 0.2459\n",
      "Epoch 28/100\n",
      "18184/18184 [==============================] - 29s - loss: 0.7061 - acc: 0.7340 - val_loss: 2.8106 - val_acc: 0.2376\n",
      "Epoch 29/100\n",
      "18184/18184 [==============================] - 28s - loss: 0.6868 - acc: 0.7436 - val_loss: 2.9013 - val_acc: 0.2338\n",
      "Epoch 30/100\n",
      "18184/18184 [==============================] - 29s - loss: 0.6632 - acc: 0.7508 - val_loss: 2.8842 - val_acc: 0.2420\n",
      "Epoch 31/100\n",
      "18184/18184 [==============================] - 29s - loss: 0.6356 - acc: 0.7616 - val_loss: 3.0496 - val_acc: 0.2427\n",
      "Epoch 32/100\n",
      "18184/18184 [==============================] - 28s - loss: 0.6015 - acc: 0.7748 - val_loss: 3.1815 - val_acc: 0.2287\n",
      "Epoch 33/100\n",
      "18184/18184 [==============================] - 28s - loss: 0.5918 - acc: 0.7785 - val_loss: 3.1766 - val_acc: 0.2401\n",
      "Epoch 34/100\n",
      "18184/18184 [==============================] - 28s - loss: 0.5791 - acc: 0.7828 - val_loss: 3.3219 - val_acc: 0.2268\n",
      "Epoch 35/100\n",
      "18184/18184 [==============================] - 28s - loss: 0.5503 - acc: 0.7945 - val_loss: 3.3787 - val_acc: 0.2376\n",
      "Epoch 36/100\n",
      "18184/18184 [==============================] - 28s - loss: 0.5450 - acc: 0.7965 - val_loss: 3.2814 - val_acc: 0.2427\n",
      "Epoch 37/100\n",
      "18184/18184 [==============================] - 28s - loss: 0.5148 - acc: 0.8076 - val_loss: 3.4772 - val_acc: 0.2318\n",
      "Epoch 38/100\n",
      "18184/18184 [==============================] - 28s - loss: 0.5107 - acc: 0.8120 - val_loss: 3.4684 - val_acc: 0.2255\n",
      "Epoch 39/100\n",
      "18184/18184 [==============================] - 28s - loss: 0.4924 - acc: 0.8191 - val_loss: 3.4831 - val_acc: 0.2306\n",
      "Epoch 40/100\n",
      "18184/18184 [==============================] - 28s - loss: 0.4708 - acc: 0.8220 - val_loss: 3.6312 - val_acc: 0.2350\n",
      "Epoch 41/100\n",
      "18184/18184 [==============================] - 28s - loss: 0.4540 - acc: 0.8335 - val_loss: 3.7232 - val_acc: 0.2357\n",
      "Epoch 42/100\n",
      "18184/18184 [==============================] - 28s - loss: 0.4457 - acc: 0.8359 - val_loss: 3.7071 - val_acc: 0.2325\n",
      "Epoch 43/100\n",
      "18184/18184 [==============================] - 28s - loss: 0.4335 - acc: 0.8393 - val_loss: 3.7936 - val_acc: 0.2312\n",
      "Epoch 44/100\n",
      "18184/18184 [==============================] - 29s - loss: 0.4058 - acc: 0.8499 - val_loss: 3.9510 - val_acc: 0.2255\n",
      "Epoch 45/100\n",
      "18184/18184 [==============================] - 28s - loss: 0.4085 - acc: 0.8494 - val_loss: 3.9822 - val_acc: 0.2299\n",
      "Epoch 46/100\n",
      "18184/18184 [==============================] - 28s - loss: 0.4001 - acc: 0.8532 - val_loss: 4.0029 - val_acc: 0.2229\n",
      "Epoch 47/100\n",
      "18184/18184 [==============================] - 29s - loss: 0.3857 - acc: 0.8589 - val_loss: 4.1303 - val_acc: 0.2229\n",
      "Epoch 48/100\n",
      "18184/18184 [==============================] - 29s - loss: 0.3823 - acc: 0.8600 - val_loss: 4.1900 - val_acc: 0.2229\n",
      "Epoch 49/100\n",
      "18184/18184 [==============================] - 28s - loss: 0.3665 - acc: 0.8633 - val_loss: 4.3608 - val_acc: 0.2185\n",
      "Epoch 50/100\n",
      "18184/18184 [==============================] - 28s - loss: 0.3625 - acc: 0.8677 - val_loss: 4.2083 - val_acc: 0.2338\n",
      "Epoch 51/100\n",
      "18184/18184 [==============================] - 28s - loss: 0.3543 - acc: 0.8683 - val_loss: 4.4052 - val_acc: 0.2255\n",
      "Epoch 52/100\n",
      "18184/18184 [==============================] - 28s - loss: 0.3486 - acc: 0.8726 - val_loss: 4.3182 - val_acc: 0.2217\n",
      "Epoch 53/100\n",
      "18184/18184 [==============================] - 28s - loss: 0.3333 - acc: 0.8793 - val_loss: 4.4807 - val_acc: 0.2255\n",
      "Epoch 54/100\n",
      "18184/18184 [==============================] - 28s - loss: 0.3311 - acc: 0.8806 - val_loss: 4.4443 - val_acc: 0.2318\n",
      "Epoch 55/100\n",
      "18184/18184 [==============================] - 28s - loss: 0.3286 - acc: 0.8786 - val_loss: 4.4904 - val_acc: 0.2248\n",
      "Epoch 56/100\n",
      "18184/18184 [==============================] - 28s - loss: 0.3100 - acc: 0.8850 - val_loss: 4.5702 - val_acc: 0.2204\n",
      "Epoch 57/100\n",
      "18184/18184 [==============================] - 28s - loss: 0.3056 - acc: 0.8886 - val_loss: 4.6626 - val_acc: 0.2191\n",
      "Epoch 58/100\n",
      "18184/18184 [==============================] - 28s - loss: 0.3007 - acc: 0.8905 - val_loss: 4.7409 - val_acc: 0.2242\n",
      "Epoch 59/100\n",
      "18184/18184 [==============================] - 28s - loss: 0.2878 - acc: 0.8941 - val_loss: 4.7248 - val_acc: 0.2121\n",
      "Epoch 60/100\n",
      "18184/18184 [==============================] - 28s - loss: 0.2918 - acc: 0.8939 - val_loss: 4.7559 - val_acc: 0.2185\n",
      "Epoch 61/100\n",
      "18184/18184 [==============================] - 28s - loss: 0.2800 - acc: 0.8960 - val_loss: 4.7904 - val_acc: 0.2229\n",
      "Epoch 62/100\n",
      "18184/18184 [==============================] - 28s - loss: 0.2804 - acc: 0.8967 - val_loss: 4.9089 - val_acc: 0.2229\n",
      "Epoch 63/100\n",
      "18184/18184 [==============================] - 28s - loss: 0.2737 - acc: 0.9008 - val_loss: 4.8622 - val_acc: 0.2331\n",
      "Epoch 64/100\n",
      "18184/18184 [==============================] - 29s - loss: 0.2667 - acc: 0.9024 - val_loss: 4.8796 - val_acc: 0.2350\n",
      "Epoch 65/100\n",
      "18184/18184 [==============================] - 28s - loss: 0.2596 - acc: 0.9039 - val_loss: 4.9909 - val_acc: 0.2280\n",
      "Epoch 66/100\n",
      "18184/18184 [==============================] - 29s - loss: 0.2617 - acc: 0.9050 - val_loss: 5.1167 - val_acc: 0.2197\n",
      "Epoch 67/100\n",
      "18184/18184 [==============================] - 28s - loss: 0.2514 - acc: 0.9090 - val_loss: 5.1123 - val_acc: 0.2280\n",
      "Epoch 68/100\n",
      "18184/18184 [==============================] - 28s - loss: 0.2556 - acc: 0.9067 - val_loss: 5.2016 - val_acc: 0.2178\n",
      "Epoch 69/100\n",
      "18184/18184 [==============================] - 28s - loss: 0.2455 - acc: 0.9096 - val_loss: 5.2053 - val_acc: 0.2242\n",
      "Epoch 70/100\n",
      "18184/18184 [==============================] - 28s - loss: 0.2407 - acc: 0.9107 - val_loss: 5.3148 - val_acc: 0.2217\n",
      "Epoch 71/100\n",
      "18184/18184 [==============================] - 28s - loss: 0.2397 - acc: 0.9122 - val_loss: 5.1658 - val_acc: 0.2248\n",
      "Epoch 72/100\n",
      "18184/18184 [==============================] - 28s - loss: 0.2300 - acc: 0.9159 - val_loss: 5.2680 - val_acc: 0.2223\n",
      "Epoch 73/100\n",
      "18184/18184 [==============================] - 28s - loss: 0.2280 - acc: 0.9179 - val_loss: 5.4534 - val_acc: 0.2210\n",
      "Epoch 74/100\n",
      "18184/18184 [==============================] - 28s - loss: 0.2242 - acc: 0.9189 - val_loss: 5.3751 - val_acc: 0.2229\n",
      "Epoch 75/100\n",
      "18184/18184 [==============================] - 29s - loss: 0.2223 - acc: 0.9178 - val_loss: 5.2899 - val_acc: 0.2236\n",
      "Epoch 76/100\n",
      "18184/18184 [==============================] - 29s - loss: 0.2170 - acc: 0.9208 - val_loss: 5.4998 - val_acc: 0.2197\n",
      "Epoch 77/100\n",
      "18184/18184 [==============================] - 29s - loss: 0.2140 - acc: 0.9217 - val_loss: 5.4640 - val_acc: 0.2204\n",
      "Epoch 78/100\n",
      "18184/18184 [==============================] - 30s - loss: 0.2084 - acc: 0.9238 - val_loss: 5.5564 - val_acc: 0.2191\n",
      "Epoch 79/100\n",
      "18184/18184 [==============================] - 29s - loss: 0.2111 - acc: 0.9229 - val_loss: 5.4858 - val_acc: 0.2236\n",
      "Epoch 80/100\n",
      "18184/18184 [==============================] - 28s - loss: 0.2041 - acc: 0.9261 - val_loss: 5.5702 - val_acc: 0.2210\n",
      "Epoch 81/100\n",
      "18184/18184 [==============================] - 29s - loss: 0.2017 - acc: 0.9271 - val_loss: 5.4805 - val_acc: 0.2242\n",
      "Epoch 82/100\n",
      "18184/18184 [==============================] - 28s - loss: 0.1991 - acc: 0.9279 - val_loss: 5.6424 - val_acc: 0.2229\n",
      "Epoch 83/100\n",
      "18184/18184 [==============================] - 28s - loss: 0.1973 - acc: 0.9276 - val_loss: 5.5679 - val_acc: 0.2242\n",
      "Epoch 84/100\n",
      "18184/18184 [==============================] - 28s - loss: 0.1975 - acc: 0.9281 - val_loss: 5.6795 - val_acc: 0.2178\n",
      "Epoch 85/100\n",
      "18184/18184 [==============================] - 28s - loss: 0.1877 - acc: 0.9314 - val_loss: 5.7029 - val_acc: 0.2261\n",
      "Epoch 86/100\n",
      "18184/18184 [==============================] - 28s - loss: 0.1839 - acc: 0.9337 - val_loss: 5.6998 - val_acc: 0.2242\n",
      "Epoch 87/100\n",
      "18184/18184 [==============================] - 28s - loss: 0.1877 - acc: 0.9340 - val_loss: 5.5675 - val_acc: 0.2293\n",
      "Epoch 88/100\n",
      "18184/18184 [==============================] - 28s - loss: 0.1869 - acc: 0.9316 - val_loss: 5.7673 - val_acc: 0.2293\n",
      "Epoch 89/100\n",
      "18184/18184 [==============================] - 28s - loss: 0.1877 - acc: 0.9331 - val_loss: 5.7795 - val_acc: 0.2255\n",
      "Epoch 90/100\n",
      "18184/18184 [==============================] - 28s - loss: 0.1814 - acc: 0.9335 - val_loss: 5.8049 - val_acc: 0.2274\n",
      "Epoch 91/100\n",
      "18184/18184 [==============================] - 28s - loss: 0.1775 - acc: 0.9349 - val_loss: 5.8394 - val_acc: 0.2159\n",
      "Epoch 92/100\n",
      "18184/18184 [==============================] - 28s - loss: 0.1698 - acc: 0.9376 - val_loss: 5.9559 - val_acc: 0.2242\n",
      "Epoch 93/100\n",
      "18184/18184 [==============================] - 28s - loss: 0.1727 - acc: 0.9381 - val_loss: 5.9443 - val_acc: 0.2293\n",
      "Epoch 94/100\n",
      "18184/18184 [==============================] - 28s - loss: 0.1725 - acc: 0.9378 - val_loss: 5.8617 - val_acc: 0.2293\n",
      "Epoch 95/100\n",
      "18184/18184 [==============================] - 28s - loss: 0.1682 - acc: 0.9401 - val_loss: 6.0497 - val_acc: 0.2197\n",
      "Epoch 96/100\n",
      "18184/18184 [==============================] - 28s - loss: 0.1651 - acc: 0.9406 - val_loss: 6.0206 - val_acc: 0.2229\n",
      "Epoch 97/100\n",
      "18184/18184 [==============================] - 28s - loss: 0.1695 - acc: 0.9382 - val_loss: 5.9708 - val_acc: 0.2223\n",
      "Epoch 98/100\n",
      "18184/18184 [==============================] - 28s - loss: 0.1627 - acc: 0.9424 - val_loss: 6.0368 - val_acc: 0.2236\n",
      "Epoch 99/100\n",
      "18184/18184 [==============================] - 28s - loss: 0.1533 - acc: 0.9438 - val_loss: 6.1638 - val_acc: 0.2242\n",
      "Epoch 100/100\n",
      "18184/18184 [==============================] - 28s - loss: 0.1560 - acc: 0.9443 - val_loss: 6.1146 - val_acc: 0.2217\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "\n",
    "print('Train...')\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=4)\n",
    "hist = decoder.fit([X_train1, X_train2], y_train, batch_size=batch_size, nb_epoch=100, show_accuracy=True, \n",
    "            validation_data=([X_val1, X_val2], y_val), \n",
    "#                    callbacks=[early_stopping]\n",
    "                  )\n",
    "# print(hist.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1413 samples, validate on 157 samples\n",
      "Epoch 1/100\n",
      "1413/1413 [==============================] - 5s - loss: 1.8650 - acc: 0.2130 - val_loss: 1.7859 - val_acc: 0.1720\n",
      "Epoch 2/100\n",
      "1413/1413 [==============================] - 4s - loss: 1.7492 - acc: 0.2604 - val_loss: 1.8563 - val_acc: 0.2038\n",
      "Epoch 3/100\n",
      "1413/1413 [==============================] - 4s - loss: 1.7125 - acc: 0.2626 - val_loss: 1.7463 - val_acc: 0.2102\n",
      "Epoch 4/100\n",
      "1413/1413 [==============================] - 4s - loss: 1.6761 - acc: 0.2831 - val_loss: 1.6164 - val_acc: 0.4395\n",
      "Epoch 5/100\n",
      "1413/1413 [==============================] - 4s - loss: 1.6379 - acc: 0.3128 - val_loss: 1.6528 - val_acc: 0.2293\n",
      "Epoch 6/100\n",
      "1413/1413 [==============================] - 4s - loss: 1.6165 - acc: 0.3383 - val_loss: 1.6205 - val_acc: 0.2930\n",
      "Epoch 7/100\n",
      "1413/1413 [==============================] - 4s - loss: 1.5898 - acc: 0.3510 - val_loss: 1.5786 - val_acc: 0.4140\n",
      "Epoch 8/100\n",
      "1413/1413 [==============================] - 4s - loss: 1.5707 - acc: 0.3595 - val_loss: 1.6784 - val_acc: 0.3376\n",
      "Epoch 9/100\n",
      "1413/1413 [==============================] - 4s - loss: 1.5485 - acc: 0.3737 - val_loss: 1.4964 - val_acc: 0.4204\n",
      "Epoch 10/100\n",
      "1413/1413 [==============================] - 4s - loss: 1.5308 - acc: 0.3687 - val_loss: 1.5467 - val_acc: 0.3949\n",
      "Epoch 11/100\n",
      "1413/1413 [==============================] - 4s - loss: 1.5007 - acc: 0.3942 - val_loss: 1.5459 - val_acc: 0.4013\n",
      "Epoch 12/100\n",
      "1413/1413 [==============================] - 5s - loss: 1.4803 - acc: 0.4062 - val_loss: 1.5443 - val_acc: 0.3949\n",
      "Epoch 13/100\n",
      "1413/1413 [==============================] - 4s - loss: 1.4515 - acc: 0.4168 - val_loss: 1.5406 - val_acc: 0.3885\n",
      "Epoch 14/100\n",
      "1413/1413 [==============================] - 4s - loss: 1.4089 - acc: 0.4508 - val_loss: 1.5963 - val_acc: 0.3885\n",
      "Epoch 15/100\n",
      "1413/1413 [==============================] - 4s - loss: 1.3733 - acc: 0.4558 - val_loss: 1.6341 - val_acc: 0.3758\n",
      "Epoch 16/100\n",
      "1413/1413 [==============================] - 4s - loss: 1.3181 - acc: 0.4968 - val_loss: 1.7041 - val_acc: 0.3376\n",
      "Epoch 17/100\n",
      "1413/1413 [==============================] - 4s - loss: 1.2917 - acc: 0.5074 - val_loss: 1.6022 - val_acc: 0.4331\n",
      "Epoch 18/100\n",
      "1413/1413 [==============================] - 4s - loss: 1.2374 - acc: 0.5131 - val_loss: 1.6140 - val_acc: 0.3439\n",
      "Epoch 19/100\n",
      "1413/1413 [==============================] - 4s - loss: 1.1975 - acc: 0.5456 - val_loss: 1.7165 - val_acc: 0.3121\n",
      "Epoch 20/100\n",
      "1413/1413 [==============================] - 4s - loss: 1.1649 - acc: 0.5541 - val_loss: 1.6309 - val_acc: 0.3758\n",
      "Epoch 21/100\n",
      "1413/1413 [==============================] - 5s - loss: 1.0870 - acc: 0.5895 - val_loss: 1.6602 - val_acc: 0.2930\n",
      "Epoch 22/100\n",
      "1413/1413 [==============================] - 5s - loss: 1.0281 - acc: 0.6270 - val_loss: 1.7677 - val_acc: 0.2803\n",
      "Epoch 23/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.9878 - acc: 0.6447 - val_loss: 1.6845 - val_acc: 0.3822\n",
      "Epoch 24/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.9070 - acc: 0.6794 - val_loss: 1.7967 - val_acc: 0.3121\n",
      "Epoch 25/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.8482 - acc: 0.6985 - val_loss: 1.9505 - val_acc: 0.3439\n",
      "Epoch 26/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.8075 - acc: 0.7254 - val_loss: 1.9031 - val_acc: 0.2803\n",
      "Epoch 27/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.7609 - acc: 0.7233 - val_loss: 2.0296 - val_acc: 0.2739\n",
      "Epoch 28/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.7213 - acc: 0.7523 - val_loss: 2.1121 - val_acc: 0.3376\n",
      "Epoch 29/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.6609 - acc: 0.7665 - val_loss: 1.9909 - val_acc: 0.3376\n",
      "Epoch 30/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.5986 - acc: 0.7976 - val_loss: 2.0612 - val_acc: 0.3312\n",
      "Epoch 31/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.5689 - acc: 0.8160 - val_loss: 2.2446 - val_acc: 0.3439\n",
      "Epoch 32/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.5058 - acc: 0.8337 - val_loss: 2.3571 - val_acc: 0.2866\n",
      "Epoch 33/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.5100 - acc: 0.8259 - val_loss: 2.4788 - val_acc: 0.2866\n",
      "Epoch 34/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.4566 - acc: 0.8436 - val_loss: 2.7126 - val_acc: 0.2739\n",
      "Epoch 35/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.4500 - acc: 0.8471 - val_loss: 2.4363 - val_acc: 0.3121\n",
      "Epoch 36/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.4193 - acc: 0.8627 - val_loss: 2.4571 - val_acc: 0.3567\n",
      "Epoch 37/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.3669 - acc: 0.8790 - val_loss: 2.5672 - val_acc: 0.3439\n",
      "Epoch 38/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.3512 - acc: 0.8825 - val_loss: 2.7846 - val_acc: 0.3885\n",
      "Epoch 39/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.3357 - acc: 0.8960 - val_loss: 2.8504 - val_acc: 0.3121\n",
      "Epoch 40/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.3263 - acc: 0.8953 - val_loss: 2.7285 - val_acc: 0.3312\n",
      "Epoch 41/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.2757 - acc: 0.9087 - val_loss: 2.8559 - val_acc: 0.3312\n",
      "Epoch 42/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.2794 - acc: 0.9094 - val_loss: 3.0356 - val_acc: 0.3312\n",
      "Epoch 43/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.2729 - acc: 0.9144 - val_loss: 2.9578 - val_acc: 0.3121\n",
      "Epoch 44/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.2608 - acc: 0.9179 - val_loss: 3.3252 - val_acc: 0.2803\n",
      "Epoch 45/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.2536 - acc: 0.9130 - val_loss: 3.1893 - val_acc: 0.2866\n",
      "Epoch 46/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.2183 - acc: 0.9335 - val_loss: 3.0771 - val_acc: 0.3312\n",
      "Epoch 47/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.1992 - acc: 0.9398 - val_loss: 3.3509 - val_acc: 0.3248\n",
      "Epoch 48/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.2053 - acc: 0.9349 - val_loss: 3.2183 - val_acc: 0.2994\n",
      "Epoch 49/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.2066 - acc: 0.9356 - val_loss: 3.1724 - val_acc: 0.3312\n",
      "Epoch 50/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.2088 - acc: 0.9278 - val_loss: 3.2967 - val_acc: 0.3121\n",
      "Epoch 51/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.1969 - acc: 0.9377 - val_loss: 3.3220 - val_acc: 0.2994\n",
      "Epoch 52/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.1768 - acc: 0.9441 - val_loss: 3.4794 - val_acc: 0.3185\n",
      "Epoch 53/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.1711 - acc: 0.9427 - val_loss: 3.4100 - val_acc: 0.3439\n",
      "Epoch 54/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.1662 - acc: 0.9420 - val_loss: 3.5260 - val_acc: 0.3057\n",
      "Epoch 55/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.1675 - acc: 0.9441 - val_loss: 3.5902 - val_acc: 0.3376\n",
      "Epoch 56/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.1429 - acc: 0.9483 - val_loss: 3.5998 - val_acc: 0.3567\n",
      "Epoch 57/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.1536 - acc: 0.9519 - val_loss: 3.5960 - val_acc: 0.3567\n",
      "Epoch 58/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.1446 - acc: 0.9455 - val_loss: 3.6684 - val_acc: 0.2803\n",
      "Epoch 59/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.1160 - acc: 0.9689 - val_loss: 3.8264 - val_acc: 0.3057\n",
      "Epoch 60/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.1308 - acc: 0.9590 - val_loss: 3.8092 - val_acc: 0.3439\n",
      "Epoch 61/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.1282 - acc: 0.9618 - val_loss: 3.7316 - val_acc: 0.3248\n",
      "Epoch 62/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.1129 - acc: 0.9611 - val_loss: 3.7225 - val_acc: 0.3248\n",
      "Epoch 63/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.1173 - acc: 0.9618 - val_loss: 4.0660 - val_acc: 0.2866\n",
      "Epoch 64/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.1074 - acc: 0.9660 - val_loss: 3.7886 - val_acc: 0.2994\n",
      "Epoch 65/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.1038 - acc: 0.9660 - val_loss: 3.9751 - val_acc: 0.3057\n",
      "Epoch 66/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.1099 - acc: 0.9611 - val_loss: 4.0022 - val_acc: 0.3057\n",
      "Epoch 67/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.1087 - acc: 0.9667 - val_loss: 4.0484 - val_acc: 0.3121\n",
      "Epoch 68/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.0927 - acc: 0.9696 - val_loss: 4.0106 - val_acc: 0.3185\n",
      "Epoch 69/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.1035 - acc: 0.9660 - val_loss: 4.0493 - val_acc: 0.3248\n",
      "Epoch 70/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.1023 - acc: 0.9660 - val_loss: 4.1186 - val_acc: 0.2866\n",
      "Epoch 71/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.0893 - acc: 0.9710 - val_loss: 4.0839 - val_acc: 0.3503\n",
      "Epoch 72/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.0984 - acc: 0.9667 - val_loss: 4.1363 - val_acc: 0.3503\n",
      "Epoch 73/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.0832 - acc: 0.9745 - val_loss: 4.1467 - val_acc: 0.3185\n",
      "Epoch 74/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.0976 - acc: 0.9696 - val_loss: 4.3497 - val_acc: 0.2866\n",
      "Epoch 75/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.0874 - acc: 0.9696 - val_loss: 4.2429 - val_acc: 0.2994\n",
      "Epoch 76/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.0789 - acc: 0.9724 - val_loss: 4.6248 - val_acc: 0.2803\n",
      "Epoch 77/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.0830 - acc: 0.9696 - val_loss: 4.1941 - val_acc: 0.3376\n",
      "Epoch 78/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.0768 - acc: 0.9724 - val_loss: 4.6176 - val_acc: 0.2994\n",
      "Epoch 79/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.0830 - acc: 0.9710 - val_loss: 4.7412 - val_acc: 0.2930\n",
      "Epoch 80/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.0710 - acc: 0.9795 - val_loss: 4.3868 - val_acc: 0.3376\n",
      "Epoch 81/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.0753 - acc: 0.9781 - val_loss: 4.5251 - val_acc: 0.3439\n",
      "Epoch 82/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.0601 - acc: 0.9844 - val_loss: 4.5361 - val_acc: 0.3185\n",
      "Epoch 83/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.0632 - acc: 0.9795 - val_loss: 4.6108 - val_acc: 0.3248\n",
      "Epoch 84/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.0913 - acc: 0.9696 - val_loss: 4.5547 - val_acc: 0.2866\n",
      "Epoch 85/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.0714 - acc: 0.9809 - val_loss: 4.5034 - val_acc: 0.3185\n",
      "Epoch 86/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.0703 - acc: 0.9731 - val_loss: 4.5861 - val_acc: 0.3248\n",
      "Epoch 87/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.0654 - acc: 0.9788 - val_loss: 4.5523 - val_acc: 0.3503\n",
      "Epoch 88/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.0618 - acc: 0.9830 - val_loss: 4.5395 - val_acc: 0.3567\n",
      "Epoch 89/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.0611 - acc: 0.9851 - val_loss: 4.6799 - val_acc: 0.3376\n",
      "Epoch 90/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.0659 - acc: 0.9795 - val_loss: 4.6002 - val_acc: 0.3248\n",
      "Epoch 91/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.0641 - acc: 0.9788 - val_loss: 4.8258 - val_acc: 0.3248\n",
      "Epoch 92/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.0593 - acc: 0.9830 - val_loss: 4.6893 - val_acc: 0.3376\n",
      "Epoch 93/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.0680 - acc: 0.9731 - val_loss: 4.9048 - val_acc: 0.3185\n",
      "Epoch 94/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.0581 - acc: 0.9837 - val_loss: 4.6501 - val_acc: 0.3503\n",
      "Epoch 95/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.0620 - acc: 0.9816 - val_loss: 4.6882 - val_acc: 0.3439\n",
      "Epoch 96/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.0591 - acc: 0.9823 - val_loss: 4.7803 - val_acc: 0.3631\n",
      "Epoch 97/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.0577 - acc: 0.9795 - val_loss: 4.8381 - val_acc: 0.3185\n",
      "Epoch 98/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.0531 - acc: 0.9823 - val_loss: 4.8868 - val_acc: 0.2994\n",
      "Epoch 99/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.0634 - acc: 0.9774 - val_loss: 4.7720 - val_acc: 0.3567\n",
      "Epoch 100/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.0548 - acc: 0.9802 - val_loss: 4.8976 - val_acc: 0.3312\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fceea5ffd50>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 50\n",
    "\n",
    "decoder.fit([X_val1, X_val2], y_val, batch_size=batch_size, nb_epoch=100, show_accuracy=True, \n",
    "            validation_split = 0.1, shuffle=True, \n",
    "#                    callbacks=[early_stopping]\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predict score of 2015 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "All input arrays and the target array must have the same number of samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-114-f056a2cd9e2f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m score, acc = decoder.evaluate([X_test1_f, X_test2_f], y_test,\n\u001b[0;32m      2\u001b[0m                             \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m                             show_accuracy=True)\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Test score:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Test accuracy:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/keras/models.pyc\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, X, y, batch_size, show_accuracy, verbose, sample_weight)\u001b[0m\n\u001b[0;32m    765\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 767\u001b[1;33m                 raise Exception('All input arrays and the target array must '\n\u001b[0m\u001b[0;32m    768\u001b[0m                                 'have the same number of samples.')\n\u001b[0;32m    769\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: All input arrays and the target array must have the same number of samples."
     ]
    }
   ],
   "source": [
    "score, acc = decoder.evaluate([X_test1_f, X_test2_f], y_test,\n",
    "                            batch_size=batch_size,\n",
    "                            show_accuracy=True)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def writeRes(dst, res):\n",
    "    with open(dst, 'w') as thefile:\n",
    "        thefile.write(\"\\n\".join(str(i) for i in res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 0s     \n"
     ]
    }
   ],
   "source": [
    "res = decoder.predict_classes([X_test1_f, X_test2_f]) \n",
    "# res = [r[0] for r in res]\n",
    "# writeRes('./dataset/STS2015-test/sys.forum', res)\n",
    "np.savetxt('./dataset/STS2015-test/sys.forum', res, newline='\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
