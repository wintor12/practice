{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GeForce GTX 980 Ti (CNMeM is disabled, CuDNN 4007)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.preprocessing import text\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "token_pattern=r\"(?u)\\b\\w\\w+\\b\"\n",
    "def build_tokenizer():\n",
    "    \"\"\"Return a function that splits a string into a sequence of tokens\"\"\"\n",
    "    pattern = re.compile(token_pattern)\n",
    "    return lambda doc: pattern.findall(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readData(src):\n",
    "    b1 = []\n",
    "    b2 = []\n",
    "    with open(src) as p:\n",
    "        for i, line in enumerate(p):\n",
    "            s = line.split('\\t')\n",
    "            if len(s) == 2:\n",
    "                b1.append(s[0])\n",
    "                b2.append(s[1][:-1]) #remove \\n\n",
    "                lines = i + 1\n",
    "    return b1, b2, lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readGs(src):\n",
    "    b = []\n",
    "    with open(src) as p:\n",
    "        for i, line in enumerate(p):\n",
    "            b.append(round(float(line),0))\n",
    "            lines = i + 1\n",
    "    return b, lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read 2012 trainning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750\n",
      "750\n",
      "734\n",
      "2234\n"
     ]
    }
   ],
   "source": [
    "msr = './dataset/STS2012-train/STS.input.MSRpar.txt'\n",
    "msrvid = './dataset/STS2012-train/STS.input.MSRvid.txt'\n",
    "smt = './dataset/STS2012-train/STS.input.SMTeuroparl.txt'\n",
    "b1_12_1, b2_12_1, l_12_1 = readData(msr)\n",
    "print l_12_1\n",
    "b1_12_2, b2_12_2, l_12_2 = readData(msrvid)\n",
    "print l_12_2\n",
    "b1_12_3, b2_12_3, l_12_3 = readData(smt)\n",
    "print l_12_3\n",
    "lines_12 = l_12_1 + l_12_2 + l_12_3\n",
    "b1_12_train = b1_12_1 + b1_12_2 + b1_12_3\n",
    "b2_12_train = b2_12_1 + b2_12_2 + b2_12_3\n",
    "print lines_12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "msr_gs = './dataset/STS2012-train/STS.gs.MSRpar.txt'\n",
    "msr_gs_vid = './dataset/STS2012-train/STS.gs.MSRvid.txt'\n",
    "smt_gs = './dataset/STS2012-train/STS.gs.SMTeuroparl.txt'\n",
    "b_12_train = readGs(msr_gs)[0]\n",
    "b_12_train = b_12_train + readGs(msr_gs_vid)[0]\n",
    "b_12_train = b_12_train + readGs(smt_gs)[0]\n",
    "print len(b_12_train) == len(b1_12_train) == len(b2_12_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read 2012 test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750\n",
      "750\n",
      "459\n",
      "750\n",
      "399\n",
      "3108\n"
     ]
    }
   ],
   "source": [
    "msr_test = './dataset/STS2012-test/STS.input.MSRpar.txt'\n",
    "vid_test = './dataset/STS2012-test/STS.input.MSRvid.txt'\n",
    "smt_test = './dataset/STS2012-test/STS.input.SMTeuroparl.txt'\n",
    "surprise_test = './dataset/STS2012-test/STS.input.surprise.OnWN.txt'\n",
    "surprise2_test = './dataset/STS2012-test/STS.input.surprise.SMTnews.txt'\n",
    "b1_12_1t, b2_12_1t, l_12_1t = readData(msr_test)\n",
    "print l_12_1t\n",
    "b1_12_2t, b2_12_2t, l_12_2t = readData(vid_test)\n",
    "print l_12_2t\n",
    "b1_12_3t, b2_12_3t, l_12_3t = readData(smt_test)\n",
    "print l_12_3t\n",
    "b1_12_4t, b2_12_4t, l_12_4t = readData(surprise_test)\n",
    "print l_12_4t\n",
    "b1_12_5t, b2_12_5t, l_12_5t = readData(surprise2_test)\n",
    "print l_12_5t\n",
    "lines = l_12_1t + l_12_2t + l_12_3t + l_12_4t + l_12_5t\n",
    "b1_12_test = b1_12_1t + b1_12_2t + b1_12_3t + b1_12_4t + b1_12_5t\n",
    "b2_12_test = b2_12_1t + b2_12_2t + b2_12_3t + b2_12_4t + b2_12_5t\n",
    "print lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "msr_test_gs = './dataset/STS2012-test/STS.gs.MSRpar.txt'\n",
    "vid_test_gs = './dataset/STS2012-test/STS.gs.MSRvid.txt'\n",
    "smt_test_gs = './dataset/STS2012-test/STS.gs.SMTeuroparl.txt'\n",
    "surprise_test_gs = './dataset/STS2012-test/STS.gs.surprise.OnWN.txt'\n",
    "surprise2_test_gs = './dataset/STS2012-test/STS.gs.surprise.SMTnews.txt'\n",
    "b_12_test = readGs(msr_test_gs)[0]\n",
    "b_12_test = b_12_test + readGs(vid_test_gs)[0]\n",
    "b_12_test = b_12_test + readGs(smt_test_gs)[0]\n",
    "b_12_test = b_12_test + readGs(surprise_test_gs)[0]\n",
    "b_12_test = b_12_test + readGs(surprise2_test_gs)[0]\n",
    "print len(b_12_test) == len(b1_12_test) == len(b2_12_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read 2014 test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450\n",
      "300\n",
      "750\n",
      "750\n",
      "750\n",
      "750\n",
      "3750\n"
     ]
    }
   ],
   "source": [
    "t14_f = './dataset/STS2014-test/STS.input.deft-forum.txt'\n",
    "t14_n = './dataset/STS2014-test/STS.input.deft-news.txt'\n",
    "t14_h = './dataset/STS2014-test/STS.input.headlines.txt'\n",
    "t14_i = './dataset/STS2014-test/STS.input.images.txt'\n",
    "t14_o = './dataset/STS2014-test/STS.input.OnWN.txt'\n",
    "t14_t = './dataset/STS2014-test/STS.input.tweet-news.txt'\n",
    "b1_14_1t, b2_14_1t, l_14_1t = readData(t14_f)\n",
    "print l_14_1t\n",
    "b1_14_2t, b2_14_2t, l_14_2t = readData(t14_n)\n",
    "print l_14_2t\n",
    "b1_14_3t, b2_14_3t, l_14_3t = readData(t14_h)\n",
    "print l_14_3t\n",
    "b1_14_4t, b2_14_4t, l_14_4t = readData(t14_i)\n",
    "print l_14_4t\n",
    "b1_14_5t, b2_14_5t, l_14_5t = readData(t14_o)\n",
    "print l_14_5t\n",
    "b1_14_6t, b2_14_6t, l_14_6t = readData(t14_t)\n",
    "print l_14_6t\n",
    "b1_14_test = b1_14_1t + b1_14_2t + b1_14_3t + b1_14_4t + b1_14_5t + b1_14_6t\n",
    "b2_14_test = b2_14_1t + b2_14_2t + b2_14_3t + b2_14_4t + b2_14_5t + b2_14_6t\n",
    "lines = l_14_1t + l_14_2t + l_14_3t + l_14_4t + l_14_5t + l_14_6t\n",
    "print lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "t14_f_gs = './dataset/STS2014-test/STS.gs.deft-forum.txt'\n",
    "t14_n_gs = './dataset/STS2014-test/STS.gs.deft-news.txt'\n",
    "t14_h_gs = './dataset/STS2014-test/STS.gs.headlines.txt'\n",
    "t14_i_gs = './dataset/STS2014-test/STS.gs.images.txt'\n",
    "t14_o_gs = './dataset/STS2014-test/STS.gs.OnWN.txt'\n",
    "t14_t_gs = './dataset/STS2014-test/STS.gs.tweet-news.txt'\n",
    "b_14_test = readGs(t14_f_gs)[0]\n",
    "b_14_test = b_14_test + readGs(t14_n_gs)[0]\n",
    "b_14_test = b_14_test + readGs(t14_h_gs)[0]\n",
    "b_14_test = b_14_test + readGs(t14_i_gs)[0]\n",
    "b_14_test = b_14_test + readGs(t14_o_gs)[0]\n",
    "b_14_test = b_14_test + readGs(t14_t_gs)[0]\n",
    "print len(b_14_test) == len(b1_14_test) == len(b2_14_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add all years train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# b1 = b1_12_train + b1_12_test + b1_14_test\n",
    "# b2 = b2_12_train + b2_12_test + b2_14_test\n",
    "# y_train = b_12_train + b_12_test + b_14_test\n",
    "# print len(b1) == len(b2) == len(y_train)\n",
    "## Double input size\n",
    "b1 = b1_12_train + b1_12_test + b1_14_test + b2_12_train + b2_12_test + b2_14_test\n",
    "b2 = b2_12_train + b2_12_test + b2_14_test + b1_12_train + b1_12_test + b1_14_test\n",
    "y_train = b_12_train + b_12_test + b_14_test + b_12_train + b_12_test + b_14_test\n",
    "print len(b1) == len(b2) == len(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read 2015 train data and 2013 test data as validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1570\n"
     ]
    }
   ],
   "source": [
    "val_f = './dataset/STS2015-train/STS.input.answers-forum.txt'\n",
    "val_s = './dataset/STS2015-train/STS.input.answers-students.txt'\n",
    "val_b = './dataset/STS2015-train/STS.input.belief.txt'\n",
    "val_h = './dataset/STS2015-train/STS.input.headlines.txt'\n",
    "val_i = './dataset/STS2015-train/STS.input.images.txt'\n",
    "val_fn = './dataset/STS2013-test/STS.input.FNWN.txt'\n",
    "val_he = './dataset/STS2013-test/STS.input.headlines.txt'\n",
    "val_on = './dataset/STS2013-test/STS.input.OnWN.txt'\n",
    "v1_15_1, v2_15_1, l_15_1 = readData(val_f)\n",
    "v1_15_2, v2_15_2, l_15_2 = readData(val_s)\n",
    "v1_15_3, v2_15_3, l_15_3 = readData(val_b)\n",
    "v1_15_4, v2_15_4, l_15_4 = readData(val_h)\n",
    "v1_15_5, v2_15_5, l_15_5 = readData(val_i)\n",
    "v1_13_1, v2_13_1, l_13_1 = readData(val_fn)\n",
    "v1_13_2, v2_13_2, l_13_2 = readData(val_he)\n",
    "v1_13_3, v2_13_3, l_13_3 = readData(val_on)\n",
    "lines = l_15_1 + l_15_2 + l_15_3 + l_15_4 + l_15_5 + l_13_1 + l_13_2 + l_13_3\n",
    "v1 = v1_15_1 + v1_15_2 + v1_15_3 + v1_15_4 + v1_15_5 + v1_13_1 + v1_13_2 + v1_13_3\n",
    "v2 = v2_15_1 + v2_15_2 + v2_15_3 + v2_15_4 + v2_15_5 + v2_13_1 + v2_13_2 + v2_13_3\n",
    "print lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1570\n"
     ]
    }
   ],
   "source": [
    "val_gs_f = './dataset/STS2015-train/STS.gs.answers-forum.txt'\n",
    "val_gs_s = './dataset/STS2015-train/STS.gs.answers-students.txt'\n",
    "val_gs_b = './dataset/STS2015-train/STS.gs.belief.txt'\n",
    "val_gs_h = './dataset/STS2015-train/STS.gs.headlines.txt'\n",
    "val_gs_i = './dataset/STS2015-train/STS.gs.images.txt'\n",
    "val_gs_fn = './dataset/STS2013-test/STS.gs.FNWN.txt'\n",
    "val_gs_he = './dataset/STS2013-test/STS.gs.headlines.txt'\n",
    "val_gs_on = './dataset/STS2013-test/STS.gs.OnWN.txt'\n",
    "y_val = readGs(val_gs_f)[0]\n",
    "y_val = y_val + readGs(val_gs_s)[0]\n",
    "y_val = y_val + readGs(val_gs_b)[0]\n",
    "y_val = y_val + readGs(val_gs_h)[0]\n",
    "y_val = y_val + readGs(val_gs_i)[0]\n",
    "y_val = y_val + readGs(val_gs_fn)[0]\n",
    "y_val = y_val + readGs(val_gs_he)[0]\n",
    "y_val = y_val + readGs(val_gs_on)[0]\n",
    "print len(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read 2015 test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "1500\n",
      "2000\n",
      "1500\n",
      "1500\n"
     ]
    }
   ],
   "source": [
    "test_f = './dataset/STS2015-test/STS.input.answers-forums.txt'\n",
    "b1_test_f, b2_test_f, lines_f = readData(test_f)\n",
    "test_s = './dataset/STS2015-test/STS.input.answers-students.txt'\n",
    "b1_test_s, b2_test_s, lines_s = readData(test_s)\n",
    "test_b = './dataset/STS2015-test/STS.input.belief.txt'\n",
    "b1_test_b, b2_test_b, lines_b = readData(test_b)\n",
    "test_h = './dataset/STS2015-test/STS.input.headlines.txt'\n",
    "b1_test_h, b2_test_h, lines_h = readData(test_h)\n",
    "test_i = './dataset/STS2015-test/STS.input.images.txt'\n",
    "b1_test_i, b2_test_i, lines_i = readData(test_i)\n",
    "print lines_f\n",
    "print lines_s\n",
    "print lines_b\n",
    "print lines_h\n",
    "print lines_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14758\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectors = vectorizer.fit_transform(b1 + b2)\n",
    "# vectors_test = vectorizer.transform(b1_test + b2_test)\n",
    "vectors.shape\n",
    "vocab = vectorizer.get_feature_names()\n",
    "print len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:7: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:13: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    }
   ],
   "source": [
    "tokenize = build_tokenizer()\n",
    "X_train1 = []\n",
    "X_train2 = []\n",
    "for seq in b1:\n",
    "    s = []\n",
    "    for word in tokenize(seq):\n",
    "        if word in vocab:\n",
    "            s.append(vectorizer.vocabulary_[word] + 1)\n",
    "    X_train1.append(s)\n",
    "for seq in b2:\n",
    "    s = []\n",
    "    for word in tokenize(seq):\n",
    "        if word in vocab:\n",
    "            s.append(vectorizer.vocabulary_[word] + 1)\n",
    "    X_train2.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18184\n",
      "18184\n"
     ]
    }
   ],
   "source": [
    "print len(X_train1)\n",
    "print len(X_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:6: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:12: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:34: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:40: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:48: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:54: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    }
   ],
   "source": [
    "X_test1_f = []\n",
    "X_test2_f = []\n",
    "for seq in b1_test_f:\n",
    "    s = []\n",
    "    for word in tokenize(seq):\n",
    "        if word in vocab:\n",
    "            s.append(vectorizer.vocabulary_[word] + 1)\n",
    "    X_test1_f.append(s)\n",
    "for seq in b2_test_f:\n",
    "    s = []\n",
    "    for word in tokenize(seq):\n",
    "        if word in vocab:\n",
    "            s.append(vectorizer.vocabulary_[word] + 1)\n",
    "    X_test2_f.append(s)\n",
    "X_test1_s = []\n",
    "X_test2_s = []\n",
    "for seq in b1_test_s:\n",
    "    s = []\n",
    "    for word in tokenize(seq):\n",
    "        if word in vocab:\n",
    "            s.append(vectorizer.vocabulary_[word] + 1)\n",
    "    X_test1_s.append(s)\n",
    "for seq in b2_test_s:\n",
    "    s = []\n",
    "    for word in tokenize(seq):\n",
    "        if word in vocab:\n",
    "            s.append(vectorizer.vocabulary_[word] + 1)\n",
    "    X_test2_s.append(s)\n",
    "X_test1_b = []\n",
    "X_test2_b = []\n",
    "for seq in b1_test_b:\n",
    "    s = []\n",
    "    for word in tokenize(seq):\n",
    "        if word in vocab:\n",
    "            s.append(vectorizer.vocabulary_[word] + 1)\n",
    "    X_test1_b.append(s)\n",
    "for seq in b2_test_b:\n",
    "    s = []\n",
    "    for word in tokenize(seq):\n",
    "        if word in vocab:\n",
    "            s.append(vectorizer.vocabulary_[word] + 1)\n",
    "    X_test2_b.append(s)\n",
    "X_test1_h = []\n",
    "X_test2_h = []\n",
    "for seq in b1_test_h:\n",
    "    s = []\n",
    "    for word in tokenize(seq):\n",
    "        if word in vocab:\n",
    "            s.append(vectorizer.vocabulary_[word] + 1)\n",
    "    X_test1_h.append(s)\n",
    "for seq in b2_test_h:\n",
    "    s = []\n",
    "    for word in tokenize(seq):\n",
    "        if word in vocab:\n",
    "            s.append(vectorizer.vocabulary_[word] + 1)\n",
    "    X_test2_h.append(s)\n",
    "X_test1_i = []\n",
    "X_test2_i = []\n",
    "for seq in b1_test_i:\n",
    "    s = []\n",
    "    for word in tokenize(seq):\n",
    "        if word in vocab:\n",
    "            s.append(vectorizer.vocabulary_[word] + 1)\n",
    "    X_test1_i.append(s)\n",
    "for seq in b2_test_i:\n",
    "    s = []\n",
    "    for word in tokenize(seq):\n",
    "        if word in vocab:\n",
    "            s.append(vectorizer.vocabulary_[word] + 1)\n",
    "    X_test2_i.append(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "1500\n",
      "2000\n",
      "1500\n",
      "1500\n"
     ]
    }
   ],
   "source": [
    "print len(X_test1_f)\n",
    "print len(X_test2_s)\n",
    "print len(X_test1_b)\n",
    "print len(X_test2_h)\n",
    "print len(X_test1_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1570\n",
      "1570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:6: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:12: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    }
   ],
   "source": [
    "X_val1 = []\n",
    "X_val2 = []\n",
    "for seq in v1:\n",
    "    s = []\n",
    "    for word in tokenize(seq):\n",
    "        if word in vocab:\n",
    "            s.append(vectorizer.vocabulary_[word] + 1)\n",
    "    X_val1.append(s)\n",
    "for seq in v2:\n",
    "    s = []\n",
    "    for word in tokenize(seq):\n",
    "        if word in vocab:\n",
    "            s.append(vectorizer.vocabulary_[word] + 1)\n",
    "    X_val2.append(s)\n",
    "print len(X_val1)\n",
    "print len(X_val2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('X_train shape:', (18184, 25))\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 25\n",
    "X_train1 = sequence.pad_sequences(X_train1, maxlen=MAX_LEN)\n",
    "X_train2 = sequence.pad_sequences(X_train2, maxlen=MAX_LEN)\n",
    "\n",
    "print('X_train shape:', X_train1.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('X_val shape:', (1570, 25))\n"
     ]
    }
   ],
   "source": [
    "X_val1 = sequence.pad_sequences(X_val1, maxlen=MAX_LEN)\n",
    "X_val2 = sequence.pad_sequences(X_val2, maxlen=MAX_LEN)\n",
    "\n",
    "print('X_val shape:', X_val1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('X_test1_f shape:', (2000, 25))\n",
      "('X_test2_f shape:', (2000, 25))\n"
     ]
    }
   ],
   "source": [
    "X_test1_f = sequence.pad_sequences(X_test1_f,  maxlen=MAX_LEN)\n",
    "X_test2_f = sequence.pad_sequences(X_test2_f,  maxlen=MAX_LEN)\n",
    "print('X_test1_f shape:', X_test1_f.shape)\n",
    "print('X_test2_f shape:', X_test2_f.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_train, y_val = [np_utils.to_categorical(x) for x in (y_train, y_val)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use pre_trained word2vec embedding for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done loading\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "wv = Word2Vec.load_word2vec_format(\"/home/tong/Documents/python/GoogleNews-vectors-negative300.bin.gz\", binary = True)\n",
    "print \"done\" + \" loading\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab_dim = 300 # dimensionality of your word vectors\n",
    "n_symbols = len(vocab) + 1 # adding 1 to account for 0th index (for masking)\n",
    "embedding_weights = np.random.rand(n_symbols,vocab_dim)\n",
    "for word in vocab:\n",
    "    if word in wv:\n",
    "        embedding_weights[vectorizer.vocabulary_[word] + 1,:] = wv[word]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14570\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print vectorizer.vocabulary_[\"woman\"] + 1\n",
    "print np.array_equal(wv['woman'], embedding_weights[vectorizer.vocabulary_[\"woman\"] + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Build complete\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Merge\n",
    "print('Build model...')\n",
    "encoder_a = Sequential()\n",
    "encoder_a.add(Embedding(n_symbols, vocab_dim, input_length=MAX_LEN, weights=[embedding_weights]))\n",
    "encoder_a.add(LSTM(vocab_dim, dropout_W=0.5, dropout_U=0.1))  # try using a GRU instead, for fun\n",
    "encoder_a.add(Dropout(0.5))\n",
    "\n",
    "encoder_b = Sequential()\n",
    "encoder_b.add(Embedding(n_symbols, vocab_dim, input_length=MAX_LEN, weights=[embedding_weights]))\n",
    "encoder_b.add(LSTM(vocab_dim, dropout_W=0.5, dropout_U=0.1)) \n",
    "encoder_b.add(Dropout(0.5))\n",
    "\n",
    "decoder = Sequential()\n",
    "decoder.add(Merge([encoder_a, encoder_b], mode='concat'))\n",
    "decoder.add(Dense(6, activation='softmax'))\n",
    "decoder.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "print('Build complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Layer is not connected. Did you forget to set \"input_shape\"?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-9b8d84c8d145>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mforwards_a\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout_W\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout_U\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mbackwards_a\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout_W\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout_U\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgo_backwards\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mmerged_a\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mforwards_a\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackwards_a\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'concat'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconcat_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mencoder_a\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmerged_a\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mencoder_a\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/keras/layers/core.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, layers, mode, concat_axis, dot_axes)\u001b[0m\n\u001b[0;32m    488\u001b[0m             \u001b[0minput_shapes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    489\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 490\u001b[1;33m                 \u001b[0moshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    491\u001b[0m                 \u001b[0moshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconcat_axis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m                 \u001b[0moshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/keras/layers/recurrent.pyc\u001b[0m in \u001b[0;36moutput_shape\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0moutput_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m         \u001b[0minput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreturn_sequences\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/keras/layers/core.pyc\u001b[0m in \u001b[0;36minput_shape\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    208\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_input_shape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 210\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Layer is not connected. Did you forget to set \"input_shape\"?'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mset_input_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: Layer is not connected. Did you forget to set \"input_shape\"?"
     ]
    }
   ],
   "source": [
    "from keras.layers import Merge\n",
    "print('Build model...')\n",
    "encoder_a = Sequential()\n",
    "encoder_a.add(Embedding(n_symbols, vocab_dim, input_length=MAX_LEN, weights=[embedding_weights]))\n",
    "forwards_a = LSTM(vocab_dim, dropout_W=0.5, dropout_U=0.1)\n",
    "backwards_a = LSTM(vocab_dim, dropout_W=0.5, dropout_U=0.1, go_backwards=True)\n",
    "merged_a = Merge([forwards_a, backwards_a], mode='concat', concat_axis=-1)\n",
    "encoder_a.add(merged_a)\n",
    "encoder_a.add(Dropout(0.5))\n",
    "\n",
    "encoder_b = Sequential()\n",
    "encoder_b.add(Embedding(n_symbols, vocab_dim, input_length=MAX_LEN, weights=[embedding_weights]))\n",
    "forwards_b = LSTM(vocab_dim, dropout_W=0.5, dropout_U=0.1)\n",
    "backwards_b = LSTM(vocab_dim, dropout_W=0.5, dropout_U=0.1, go_backwards=True)\n",
    "encoder_b.add(Merge([forwards_b, backwards_b], mode='concat', concat_axis=-1))\n",
    "encoder_b.add(Dropout(0.5))\n",
    "\n",
    "decoder = Sequential()\n",
    "decoder.add(Merge([encoder_a, encoder_b], mode='concat'))\n",
    "decoder.add(Dense(6, activation='softmax'))\n",
    "decoder.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "print('Build complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1570\n"
     ]
    }
   ],
   "source": [
    "print len(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 18184 samples, validate on 1570 samples\n",
      "Epoch 1/100\n",
      "18184/18184 [==============================] - 33s - loss: 1.6595 - acc: 0.2771 - val_loss: 1.8551 - val_acc: 0.2675\n",
      "Epoch 2/100\n",
      "18184/18184 [==============================] - 33s - loss: 1.5694 - acc: 0.3253 - val_loss: 1.8709 - val_acc: 0.2809\n",
      "Epoch 3/100\n",
      "18184/18184 [==============================] - 33s - loss: 1.5208 - acc: 0.3569 - val_loss: 1.9320 - val_acc: 0.2420\n",
      "Epoch 4/100\n",
      "18184/18184 [==============================] - 33s - loss: 1.4941 - acc: 0.3708 - val_loss: 1.9844 - val_acc: 0.2904\n",
      "Epoch 5/100\n",
      "18184/18184 [==============================] - 33s - loss: 1.4630 - acc: 0.3863 - val_loss: 1.9813 - val_acc: 0.2796\n",
      "Epoch 6/100\n",
      "18184/18184 [==============================] - 33s - loss: 1.4371 - acc: 0.4034 - val_loss: 1.8801 - val_acc: 0.2777\n",
      "Epoch 7/100\n",
      "18184/18184 [==============================] - 33s - loss: 1.4130 - acc: 0.4170 - val_loss: 1.9059 - val_acc: 0.2631\n",
      "Epoch 8/100\n",
      "18184/18184 [==============================] - 33s - loss: 1.3844 - acc: 0.4326 - val_loss: 1.8599 - val_acc: 0.2707\n",
      "Epoch 9/100\n",
      "18184/18184 [==============================] - 34s - loss: 1.3596 - acc: 0.4409 - val_loss: 1.9442 - val_acc: 0.2541\n",
      "Epoch 10/100\n",
      "18184/18184 [==============================] - 33s - loss: 1.3281 - acc: 0.4621 - val_loss: 1.8917 - val_acc: 0.2662\n",
      "Epoch 11/100\n",
      "18184/18184 [==============================] - 33s - loss: 1.3003 - acc: 0.4756 - val_loss: 2.0357 - val_acc: 0.2535\n",
      "Epoch 12/100\n",
      "18184/18184 [==============================] - 33s - loss: 1.2762 - acc: 0.4869 - val_loss: 2.0542 - val_acc: 0.2548\n",
      "Epoch 13/100\n",
      "18184/18184 [==============================] - 33s - loss: 1.2462 - acc: 0.4999 - val_loss: 1.9516 - val_acc: 0.2592\n",
      "Epoch 14/100\n",
      "18184/18184 [==============================] - 33s - loss: 1.2148 - acc: 0.5119 - val_loss: 2.0488 - val_acc: 0.2554\n",
      "Epoch 15/100\n",
      "18184/18184 [==============================] - 33s - loss: 1.1903 - acc: 0.5230 - val_loss: 2.0130 - val_acc: 0.2694\n",
      "Epoch 16/100\n",
      "18184/18184 [==============================] - 33s - loss: 1.1545 - acc: 0.5434 - val_loss: 2.1416 - val_acc: 0.2599\n",
      "Epoch 17/100\n",
      "18184/18184 [==============================] - 33s - loss: 1.1321 - acc: 0.5545 - val_loss: 2.0623 - val_acc: 0.2701\n",
      "Epoch 18/100\n",
      "18184/18184 [==============================] - 33s - loss: 1.0998 - acc: 0.5692 - val_loss: 2.1226 - val_acc: 0.2510\n",
      "Epoch 19/100\n",
      "18184/18184 [==============================] - 34s - loss: 1.0654 - acc: 0.5838 - val_loss: 2.2590 - val_acc: 0.2529\n",
      "Epoch 20/100\n",
      "18184/18184 [==============================] - 33s - loss: 1.0351 - acc: 0.5926 - val_loss: 2.2352 - val_acc: 0.2592\n",
      "Epoch 21/100\n",
      "18184/18184 [==============================] - 33s - loss: 1.0077 - acc: 0.6084 - val_loss: 2.2198 - val_acc: 0.2401\n",
      "Epoch 22/100\n",
      "18184/18184 [==============================] - 33s - loss: 0.9691 - acc: 0.6234 - val_loss: 2.2415 - val_acc: 0.2554\n",
      "Epoch 23/100\n",
      "18184/18184 [==============================] - 34s - loss: 0.9383 - acc: 0.6414 - val_loss: 2.2746 - val_acc: 0.2535\n",
      "Epoch 24/100\n",
      "18184/18184 [==============================] - 34s - loss: 0.9126 - acc: 0.6479 - val_loss: 2.4082 - val_acc: 0.2484\n",
      "Epoch 25/100\n",
      "18184/18184 [==============================] - 33s - loss: 0.8927 - acc: 0.6582 - val_loss: 2.4241 - val_acc: 0.2382\n",
      "Epoch 26/100\n",
      "18184/18184 [==============================] - 33s - loss: 0.8578 - acc: 0.6687 - val_loss: 2.3831 - val_acc: 0.2573\n",
      "Epoch 27/100\n",
      "18184/18184 [==============================] - 33s - loss: 0.8272 - acc: 0.6838 - val_loss: 2.5930 - val_acc: 0.2465\n",
      "Epoch 28/100\n",
      "18184/18184 [==============================] - 33s - loss: 0.8016 - acc: 0.6939 - val_loss: 2.6206 - val_acc: 0.2389\n",
      "Epoch 29/100\n",
      "18184/18184 [==============================] - 33s - loss: 0.7852 - acc: 0.7013 - val_loss: 2.6457 - val_acc: 0.2408\n",
      "Epoch 30/100\n",
      "18184/18184 [==============================] - 33s - loss: 0.7566 - acc: 0.7136 - val_loss: 2.6429 - val_acc: 0.2439\n",
      "Epoch 31/100\n",
      "18184/18184 [==============================] - 33s - loss: 0.7335 - acc: 0.7204 - val_loss: 2.8024 - val_acc: 0.2389\n",
      "Epoch 32/100\n",
      "18184/18184 [==============================] - 33s - loss: 0.7148 - acc: 0.7306 - val_loss: 2.8691 - val_acc: 0.2382\n",
      "Epoch 33/100\n",
      "18184/18184 [==============================] - 33s - loss: 0.6991 - acc: 0.7361 - val_loss: 2.8625 - val_acc: 0.2420\n",
      "Epoch 34/100\n",
      "18184/18184 [==============================] - 33s - loss: 0.6599 - acc: 0.7492 - val_loss: 2.8958 - val_acc: 0.2414\n",
      "Epoch 35/100\n",
      "18184/18184 [==============================] - 33s - loss: 0.6502 - acc: 0.7529 - val_loss: 2.9430 - val_acc: 0.2325\n",
      "Epoch 36/100\n",
      "18184/18184 [==============================] - 33s - loss: 0.6265 - acc: 0.7636 - val_loss: 2.9966 - val_acc: 0.2331\n",
      "Epoch 37/100\n",
      "18184/18184 [==============================] - 33s - loss: 0.6048 - acc: 0.7734 - val_loss: 3.1376 - val_acc: 0.2331\n",
      "Epoch 38/100\n",
      "18184/18184 [==============================] - 33s - loss: 0.5927 - acc: 0.7794 - val_loss: 3.1982 - val_acc: 0.2274\n",
      "Epoch 39/100\n",
      "18184/18184 [==============================] - 33s - loss: 0.5713 - acc: 0.7857 - val_loss: 3.2942 - val_acc: 0.2318\n",
      "Epoch 40/100\n",
      "18184/18184 [==============================] - 33s - loss: 0.5493 - acc: 0.7959 - val_loss: 3.3257 - val_acc: 0.2318\n",
      "Epoch 41/100\n",
      "18184/18184 [==============================] - 33s - loss: 0.5411 - acc: 0.7998 - val_loss: 3.2936 - val_acc: 0.2306\n",
      "Epoch 42/100\n",
      "18184/18184 [==============================] - 33s - loss: 0.5260 - acc: 0.8034 - val_loss: 3.3669 - val_acc: 0.2401\n",
      "Epoch 43/100\n",
      "18184/18184 [==============================] - 33s - loss: 0.5059 - acc: 0.8097 - val_loss: 3.4822 - val_acc: 0.2389\n",
      "Epoch 44/100\n",
      "18184/18184 [==============================] - 33s - loss: 0.5048 - acc: 0.8106 - val_loss: 3.4693 - val_acc: 0.2382\n",
      "Epoch 45/100\n",
      "18184/18184 [==============================] - 33s - loss: 0.4820 - acc: 0.8189 - val_loss: 3.6252 - val_acc: 0.2427\n",
      "Epoch 46/100\n",
      "18184/18184 [==============================] - 33s - loss: 0.4775 - acc: 0.8229 - val_loss: 3.6854 - val_acc: 0.2331\n",
      "Epoch 47/100\n",
      "18184/18184 [==============================] - 33s - loss: 0.4629 - acc: 0.8251 - val_loss: 3.7408 - val_acc: 0.2299\n",
      "Epoch 48/100\n",
      "18184/18184 [==============================] - 33s - loss: 0.4551 - acc: 0.8313 - val_loss: 3.8047 - val_acc: 0.2268\n",
      "Epoch 49/100\n",
      "18184/18184 [==============================] - 33s - loss: 0.4421 - acc: 0.8364 - val_loss: 3.8158 - val_acc: 0.2318\n",
      "Epoch 50/100\n",
      "18184/18184 [==============================] - 33s - loss: 0.4289 - acc: 0.8380 - val_loss: 3.7967 - val_acc: 0.2268\n",
      "Epoch 51/100\n",
      "18184/18184 [==============================] - 33s - loss: 0.4149 - acc: 0.8445 - val_loss: 3.9690 - val_acc: 0.2255\n",
      "Epoch 52/100\n",
      "18184/18184 [==============================] - 33s - loss: 0.4109 - acc: 0.8469 - val_loss: 3.9623 - val_acc: 0.2318\n",
      "Epoch 53/100\n",
      "18184/18184 [==============================] - 33s - loss: 0.3938 - acc: 0.8485 - val_loss: 4.0436 - val_acc: 0.2350\n",
      "Epoch 54/100\n",
      "18184/18184 [==============================] - 33s - loss: 0.3900 - acc: 0.8531 - val_loss: 4.0392 - val_acc: 0.2261\n",
      "Epoch 55/100\n",
      "18184/18184 [==============================] - 33s - loss: 0.3824 - acc: 0.8569 - val_loss: 4.1298 - val_acc: 0.2236\n",
      "Epoch 56/100\n",
      "18184/18184 [==============================] - 33s - loss: 0.3743 - acc: 0.8606 - val_loss: 4.2201 - val_acc: 0.2229\n",
      "Epoch 57/100\n",
      "18184/18184 [==============================] - 33s - loss: 0.3643 - acc: 0.8649 - val_loss: 4.3192 - val_acc: 0.2242\n",
      "Epoch 58/100\n",
      "18184/18184 [==============================] - 33s - loss: 0.3566 - acc: 0.8666 - val_loss: 4.3433 - val_acc: 0.2268\n",
      "Epoch 59/100\n",
      "18184/18184 [==============================] - 33s - loss: 0.3497 - acc: 0.8724 - val_loss: 4.4047 - val_acc: 0.2312\n",
      "Epoch 60/100\n",
      "18184/18184 [==============================] - 33s - loss: 0.3430 - acc: 0.8702 - val_loss: 4.4284 - val_acc: 0.2287\n",
      "Epoch 61/100\n",
      "18184/18184 [==============================] - 33s - loss: 0.3419 - acc: 0.8727 - val_loss: 4.4806 - val_acc: 0.2242\n",
      "Epoch 62/100\n",
      "18184/18184 [==============================] - 33s - loss: 0.3311 - acc: 0.8803 - val_loss: 4.5398 - val_acc: 0.2306\n",
      "Epoch 63/100\n",
      "18184/18184 [==============================] - 33s - loss: 0.3201 - acc: 0.8781 - val_loss: 4.5723 - val_acc: 0.2255\n",
      "Epoch 64/100\n",
      "18184/18184 [==============================] - 33s - loss: 0.3131 - acc: 0.8856 - val_loss: 4.6270 - val_acc: 0.2223\n",
      "Epoch 65/100\n",
      "18184/18184 [==============================] - 33s - loss: 0.3119 - acc: 0.8851 - val_loss: 4.5379 - val_acc: 0.2274\n",
      "Epoch 66/100\n",
      "18184/18184 [==============================] - 33s - loss: 0.3043 - acc: 0.8849 - val_loss: 4.6675 - val_acc: 0.2325\n",
      "Epoch 67/100\n",
      "18184/18184 [==============================] - 33s - loss: 0.3017 - acc: 0.8868 - val_loss: 4.7779 - val_acc: 0.2185\n",
      "Epoch 68/100\n",
      "18184/18184 [==============================] - 33s - loss: 0.2902 - acc: 0.8915 - val_loss: 4.8574 - val_acc: 0.2210\n",
      "Epoch 69/100\n",
      "18184/18184 [==============================] - 33s - loss: 0.2965 - acc: 0.8886 - val_loss: 4.8518 - val_acc: 0.2236\n",
      "Epoch 70/100\n",
      "18184/18184 [==============================] - 32s - loss: 0.2830 - acc: 0.8928 - val_loss: 4.8847 - val_acc: 0.2274\n",
      "Epoch 71/100\n",
      "18184/18184 [==============================] - 32s - loss: 0.2851 - acc: 0.8937 - val_loss: 4.8863 - val_acc: 0.2255\n",
      "Epoch 72/100\n",
      "18184/18184 [==============================] - 32s - loss: 0.2819 - acc: 0.8935 - val_loss: 5.0074 - val_acc: 0.2293\n",
      "Epoch 73/100\n",
      "18184/18184 [==============================] - 32s - loss: 0.2641 - acc: 0.9013 - val_loss: 5.2336 - val_acc: 0.2248\n",
      "Epoch 74/100\n",
      "18184/18184 [==============================] - 33s - loss: 0.2673 - acc: 0.9005 - val_loss: 5.0635 - val_acc: 0.2248\n",
      "Epoch 75/100\n",
      "18184/18184 [==============================] - 33s - loss: 0.2649 - acc: 0.9013 - val_loss: 5.0636 - val_acc: 0.2268\n",
      "Epoch 76/100\n",
      "18184/18184 [==============================] - 32s - loss: 0.2598 - acc: 0.9034 - val_loss: 5.1456 - val_acc: 0.2210\n",
      "Epoch 77/100\n",
      "18184/18184 [==============================] - 33s - loss: 0.2559 - acc: 0.9037 - val_loss: 5.1661 - val_acc: 0.2248\n",
      "Epoch 78/100\n",
      "18184/18184 [==============================] - 33s - loss: 0.2542 - acc: 0.9046 - val_loss: 5.1590 - val_acc: 0.2248\n",
      "Epoch 79/100\n",
      "18184/18184 [==============================] - 33s - loss: 0.2471 - acc: 0.9074 - val_loss: 5.2617 - val_acc: 0.2197\n",
      "Epoch 80/100\n",
      "18184/18184 [==============================] - 33s - loss: 0.2454 - acc: 0.9080 - val_loss: 5.3739 - val_acc: 0.2236\n",
      "Epoch 81/100\n",
      "18184/18184 [==============================] - 33s - loss: 0.2435 - acc: 0.9115 - val_loss: 5.2700 - val_acc: 0.2210\n",
      "Epoch 82/100\n",
      "18184/18184 [==============================] - 33s - loss: 0.2340 - acc: 0.9133 - val_loss: 5.4107 - val_acc: 0.2197\n",
      "Epoch 83/100\n",
      "18184/18184 [==============================] - 33s - loss: 0.2310 - acc: 0.9150 - val_loss: 5.4359 - val_acc: 0.2236\n",
      "Epoch 84/100\n",
      "18184/18184 [==============================] - 33s - loss: 0.2331 - acc: 0.9137 - val_loss: 5.4651 - val_acc: 0.2242\n",
      "Epoch 85/100\n",
      "18184/18184 [==============================] - 33s - loss: 0.2268 - acc: 0.9157 - val_loss: 5.4364 - val_acc: 0.2229\n",
      "Epoch 86/100\n",
      "18184/18184 [==============================] - 33s - loss: 0.2281 - acc: 0.9143 - val_loss: 5.4998 - val_acc: 0.2217\n",
      "Epoch 87/100\n",
      "18184/18184 [==============================] - 33s - loss: 0.2146 - acc: 0.9191 - val_loss: 5.6358 - val_acc: 0.2255\n",
      "Epoch 88/100\n",
      "18184/18184 [==============================] - 32s - loss: 0.2130 - acc: 0.9199 - val_loss: 5.5840 - val_acc: 0.2159\n",
      "Epoch 89/100\n",
      "18184/18184 [==============================] - 33s - loss: 0.2081 - acc: 0.9227 - val_loss: 5.7029 - val_acc: 0.2166\n",
      "Epoch 90/100\n",
      "18184/18184 [==============================] - 33s - loss: 0.2091 - acc: 0.9220 - val_loss: 5.6556 - val_acc: 0.2210\n",
      "Epoch 91/100\n",
      "18184/18184 [==============================] - 33s - loss: 0.2072 - acc: 0.9230 - val_loss: 5.7081 - val_acc: 0.2191\n",
      "Epoch 92/100\n",
      "18184/18184 [==============================] - 33s - loss: 0.2047 - acc: 0.9225 - val_loss: 5.7339 - val_acc: 0.2236\n",
      "Epoch 93/100\n",
      "18184/18184 [==============================] - 33s - loss: 0.2022 - acc: 0.9252 - val_loss: 5.9437 - val_acc: 0.2178\n",
      "Epoch 94/100\n",
      "18184/18184 [==============================] - 32s - loss: 0.1985 - acc: 0.9271 - val_loss: 5.8755 - val_acc: 0.2166\n",
      "Epoch 95/100\n",
      "18184/18184 [==============================] - 32s - loss: 0.1943 - acc: 0.9261 - val_loss: 5.9734 - val_acc: 0.2217\n",
      "Epoch 96/100\n",
      "18184/18184 [==============================] - 32s - loss: 0.1962 - acc: 0.9275 - val_loss: 5.9682 - val_acc: 0.2185\n",
      "Epoch 97/100\n",
      "18184/18184 [==============================] - 33s - loss: 0.1950 - acc: 0.9272 - val_loss: 6.0080 - val_acc: 0.2172\n",
      "Epoch 98/100\n",
      "18184/18184 [==============================] - 32s - loss: 0.1914 - acc: 0.9287 - val_loss: 6.1649 - val_acc: 0.2159\n",
      "Epoch 99/100\n",
      "18184/18184 [==============================] - 32s - loss: 0.1918 - acc: 0.9293 - val_loss: 6.0354 - val_acc: 0.2172\n",
      "Epoch 100/100\n",
      "18184/18184 [==============================] - 33s - loss: 0.1962 - acc: 0.9296 - val_loss: 5.9599 - val_acc: 0.2191\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "\n",
    "print('Train...')\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=4)\n",
    "hist = decoder.fit([X_train1, X_train2], y_train, batch_size=batch_size, nb_epoch=100, show_accuracy=True, \n",
    "            validation_data=([X_val1, X_val2], y_val), \n",
    "#                    callbacks=[early_stopping]\n",
    "                  )\n",
    "# print(hist.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1413 samples, validate on 157 samples\n",
      "Epoch 1/100\n",
      "1413/1413 [==============================] - 5s - loss: 1.8650 - acc: 0.2130 - val_loss: 1.7859 - val_acc: 0.1720\n",
      "Epoch 2/100\n",
      "1413/1413 [==============================] - 4s - loss: 1.7492 - acc: 0.2604 - val_loss: 1.8563 - val_acc: 0.2038\n",
      "Epoch 3/100\n",
      "1413/1413 [==============================] - 4s - loss: 1.7125 - acc: 0.2626 - val_loss: 1.7463 - val_acc: 0.2102\n",
      "Epoch 4/100\n",
      "1413/1413 [==============================] - 4s - loss: 1.6761 - acc: 0.2831 - val_loss: 1.6164 - val_acc: 0.4395\n",
      "Epoch 5/100\n",
      "1413/1413 [==============================] - 4s - loss: 1.6379 - acc: 0.3128 - val_loss: 1.6528 - val_acc: 0.2293\n",
      "Epoch 6/100\n",
      "1413/1413 [==============================] - 4s - loss: 1.6165 - acc: 0.3383 - val_loss: 1.6205 - val_acc: 0.2930\n",
      "Epoch 7/100\n",
      "1413/1413 [==============================] - 4s - loss: 1.5898 - acc: 0.3510 - val_loss: 1.5786 - val_acc: 0.4140\n",
      "Epoch 8/100\n",
      "1413/1413 [==============================] - 4s - loss: 1.5707 - acc: 0.3595 - val_loss: 1.6784 - val_acc: 0.3376\n",
      "Epoch 9/100\n",
      "1413/1413 [==============================] - 4s - loss: 1.5485 - acc: 0.3737 - val_loss: 1.4964 - val_acc: 0.4204\n",
      "Epoch 10/100\n",
      "1413/1413 [==============================] - 4s - loss: 1.5308 - acc: 0.3687 - val_loss: 1.5467 - val_acc: 0.3949\n",
      "Epoch 11/100\n",
      "1413/1413 [==============================] - 4s - loss: 1.5007 - acc: 0.3942 - val_loss: 1.5459 - val_acc: 0.4013\n",
      "Epoch 12/100\n",
      "1413/1413 [==============================] - 5s - loss: 1.4803 - acc: 0.4062 - val_loss: 1.5443 - val_acc: 0.3949\n",
      "Epoch 13/100\n",
      "1413/1413 [==============================] - 4s - loss: 1.4515 - acc: 0.4168 - val_loss: 1.5406 - val_acc: 0.3885\n",
      "Epoch 14/100\n",
      "1413/1413 [==============================] - 4s - loss: 1.4089 - acc: 0.4508 - val_loss: 1.5963 - val_acc: 0.3885\n",
      "Epoch 15/100\n",
      "1413/1413 [==============================] - 4s - loss: 1.3733 - acc: 0.4558 - val_loss: 1.6341 - val_acc: 0.3758\n",
      "Epoch 16/100\n",
      "1413/1413 [==============================] - 4s - loss: 1.3181 - acc: 0.4968 - val_loss: 1.7041 - val_acc: 0.3376\n",
      "Epoch 17/100\n",
      "1413/1413 [==============================] - 4s - loss: 1.2917 - acc: 0.5074 - val_loss: 1.6022 - val_acc: 0.4331\n",
      "Epoch 18/100\n",
      "1413/1413 [==============================] - 4s - loss: 1.2374 - acc: 0.5131 - val_loss: 1.6140 - val_acc: 0.3439\n",
      "Epoch 19/100\n",
      "1413/1413 [==============================] - 4s - loss: 1.1975 - acc: 0.5456 - val_loss: 1.7165 - val_acc: 0.3121\n",
      "Epoch 20/100\n",
      "1413/1413 [==============================] - 4s - loss: 1.1649 - acc: 0.5541 - val_loss: 1.6309 - val_acc: 0.3758\n",
      "Epoch 21/100\n",
      "1413/1413 [==============================] - 5s - loss: 1.0870 - acc: 0.5895 - val_loss: 1.6602 - val_acc: 0.2930\n",
      "Epoch 22/100\n",
      "1413/1413 [==============================] - 5s - loss: 1.0281 - acc: 0.6270 - val_loss: 1.7677 - val_acc: 0.2803\n",
      "Epoch 23/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.9878 - acc: 0.6447 - val_loss: 1.6845 - val_acc: 0.3822\n",
      "Epoch 24/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.9070 - acc: 0.6794 - val_loss: 1.7967 - val_acc: 0.3121\n",
      "Epoch 25/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.8482 - acc: 0.6985 - val_loss: 1.9505 - val_acc: 0.3439\n",
      "Epoch 26/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.8075 - acc: 0.7254 - val_loss: 1.9031 - val_acc: 0.2803\n",
      "Epoch 27/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.7609 - acc: 0.7233 - val_loss: 2.0296 - val_acc: 0.2739\n",
      "Epoch 28/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.7213 - acc: 0.7523 - val_loss: 2.1121 - val_acc: 0.3376\n",
      "Epoch 29/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.6609 - acc: 0.7665 - val_loss: 1.9909 - val_acc: 0.3376\n",
      "Epoch 30/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.5986 - acc: 0.7976 - val_loss: 2.0612 - val_acc: 0.3312\n",
      "Epoch 31/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.5689 - acc: 0.8160 - val_loss: 2.2446 - val_acc: 0.3439\n",
      "Epoch 32/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.5058 - acc: 0.8337 - val_loss: 2.3571 - val_acc: 0.2866\n",
      "Epoch 33/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.5100 - acc: 0.8259 - val_loss: 2.4788 - val_acc: 0.2866\n",
      "Epoch 34/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.4566 - acc: 0.8436 - val_loss: 2.7126 - val_acc: 0.2739\n",
      "Epoch 35/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.4500 - acc: 0.8471 - val_loss: 2.4363 - val_acc: 0.3121\n",
      "Epoch 36/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.4193 - acc: 0.8627 - val_loss: 2.4571 - val_acc: 0.3567\n",
      "Epoch 37/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.3669 - acc: 0.8790 - val_loss: 2.5672 - val_acc: 0.3439\n",
      "Epoch 38/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.3512 - acc: 0.8825 - val_loss: 2.7846 - val_acc: 0.3885\n",
      "Epoch 39/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.3357 - acc: 0.8960 - val_loss: 2.8504 - val_acc: 0.3121\n",
      "Epoch 40/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.3263 - acc: 0.8953 - val_loss: 2.7285 - val_acc: 0.3312\n",
      "Epoch 41/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.2757 - acc: 0.9087 - val_loss: 2.8559 - val_acc: 0.3312\n",
      "Epoch 42/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.2794 - acc: 0.9094 - val_loss: 3.0356 - val_acc: 0.3312\n",
      "Epoch 43/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.2729 - acc: 0.9144 - val_loss: 2.9578 - val_acc: 0.3121\n",
      "Epoch 44/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.2608 - acc: 0.9179 - val_loss: 3.3252 - val_acc: 0.2803\n",
      "Epoch 45/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.2536 - acc: 0.9130 - val_loss: 3.1893 - val_acc: 0.2866\n",
      "Epoch 46/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.2183 - acc: 0.9335 - val_loss: 3.0771 - val_acc: 0.3312\n",
      "Epoch 47/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.1992 - acc: 0.9398 - val_loss: 3.3509 - val_acc: 0.3248\n",
      "Epoch 48/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.2053 - acc: 0.9349 - val_loss: 3.2183 - val_acc: 0.2994\n",
      "Epoch 49/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.2066 - acc: 0.9356 - val_loss: 3.1724 - val_acc: 0.3312\n",
      "Epoch 50/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.2088 - acc: 0.9278 - val_loss: 3.2967 - val_acc: 0.3121\n",
      "Epoch 51/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.1969 - acc: 0.9377 - val_loss: 3.3220 - val_acc: 0.2994\n",
      "Epoch 52/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.1768 - acc: 0.9441 - val_loss: 3.4794 - val_acc: 0.3185\n",
      "Epoch 53/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.1711 - acc: 0.9427 - val_loss: 3.4100 - val_acc: 0.3439\n",
      "Epoch 54/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.1662 - acc: 0.9420 - val_loss: 3.5260 - val_acc: 0.3057\n",
      "Epoch 55/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.1675 - acc: 0.9441 - val_loss: 3.5902 - val_acc: 0.3376\n",
      "Epoch 56/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.1429 - acc: 0.9483 - val_loss: 3.5998 - val_acc: 0.3567\n",
      "Epoch 57/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.1536 - acc: 0.9519 - val_loss: 3.5960 - val_acc: 0.3567\n",
      "Epoch 58/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.1446 - acc: 0.9455 - val_loss: 3.6684 - val_acc: 0.2803\n",
      "Epoch 59/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.1160 - acc: 0.9689 - val_loss: 3.8264 - val_acc: 0.3057\n",
      "Epoch 60/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.1308 - acc: 0.9590 - val_loss: 3.8092 - val_acc: 0.3439\n",
      "Epoch 61/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.1282 - acc: 0.9618 - val_loss: 3.7316 - val_acc: 0.3248\n",
      "Epoch 62/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.1129 - acc: 0.9611 - val_loss: 3.7225 - val_acc: 0.3248\n",
      "Epoch 63/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.1173 - acc: 0.9618 - val_loss: 4.0660 - val_acc: 0.2866\n",
      "Epoch 64/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.1074 - acc: 0.9660 - val_loss: 3.7886 - val_acc: 0.2994\n",
      "Epoch 65/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.1038 - acc: 0.9660 - val_loss: 3.9751 - val_acc: 0.3057\n",
      "Epoch 66/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.1099 - acc: 0.9611 - val_loss: 4.0022 - val_acc: 0.3057\n",
      "Epoch 67/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.1087 - acc: 0.9667 - val_loss: 4.0484 - val_acc: 0.3121\n",
      "Epoch 68/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.0927 - acc: 0.9696 - val_loss: 4.0106 - val_acc: 0.3185\n",
      "Epoch 69/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.1035 - acc: 0.9660 - val_loss: 4.0493 - val_acc: 0.3248\n",
      "Epoch 70/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.1023 - acc: 0.9660 - val_loss: 4.1186 - val_acc: 0.2866\n",
      "Epoch 71/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.0893 - acc: 0.9710 - val_loss: 4.0839 - val_acc: 0.3503\n",
      "Epoch 72/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.0984 - acc: 0.9667 - val_loss: 4.1363 - val_acc: 0.3503\n",
      "Epoch 73/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.0832 - acc: 0.9745 - val_loss: 4.1467 - val_acc: 0.3185\n",
      "Epoch 74/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.0976 - acc: 0.9696 - val_loss: 4.3497 - val_acc: 0.2866\n",
      "Epoch 75/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.0874 - acc: 0.9696 - val_loss: 4.2429 - val_acc: 0.2994\n",
      "Epoch 76/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.0789 - acc: 0.9724 - val_loss: 4.6248 - val_acc: 0.2803\n",
      "Epoch 77/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.0830 - acc: 0.9696 - val_loss: 4.1941 - val_acc: 0.3376\n",
      "Epoch 78/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.0768 - acc: 0.9724 - val_loss: 4.6176 - val_acc: 0.2994\n",
      "Epoch 79/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.0830 - acc: 0.9710 - val_loss: 4.7412 - val_acc: 0.2930\n",
      "Epoch 80/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.0710 - acc: 0.9795 - val_loss: 4.3868 - val_acc: 0.3376\n",
      "Epoch 81/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.0753 - acc: 0.9781 - val_loss: 4.5251 - val_acc: 0.3439\n",
      "Epoch 82/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.0601 - acc: 0.9844 - val_loss: 4.5361 - val_acc: 0.3185\n",
      "Epoch 83/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.0632 - acc: 0.9795 - val_loss: 4.6108 - val_acc: 0.3248\n",
      "Epoch 84/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.0913 - acc: 0.9696 - val_loss: 4.5547 - val_acc: 0.2866\n",
      "Epoch 85/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.0714 - acc: 0.9809 - val_loss: 4.5034 - val_acc: 0.3185\n",
      "Epoch 86/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.0703 - acc: 0.9731 - val_loss: 4.5861 - val_acc: 0.3248\n",
      "Epoch 87/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.0654 - acc: 0.9788 - val_loss: 4.5523 - val_acc: 0.3503\n",
      "Epoch 88/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.0618 - acc: 0.9830 - val_loss: 4.5395 - val_acc: 0.3567\n",
      "Epoch 89/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.0611 - acc: 0.9851 - val_loss: 4.6799 - val_acc: 0.3376\n",
      "Epoch 90/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.0659 - acc: 0.9795 - val_loss: 4.6002 - val_acc: 0.3248\n",
      "Epoch 91/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.0641 - acc: 0.9788 - val_loss: 4.8258 - val_acc: 0.3248\n",
      "Epoch 92/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.0593 - acc: 0.9830 - val_loss: 4.6893 - val_acc: 0.3376\n",
      "Epoch 93/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.0680 - acc: 0.9731 - val_loss: 4.9048 - val_acc: 0.3185\n",
      "Epoch 94/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.0581 - acc: 0.9837 - val_loss: 4.6501 - val_acc: 0.3503\n",
      "Epoch 95/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.0620 - acc: 0.9816 - val_loss: 4.6882 - val_acc: 0.3439\n",
      "Epoch 96/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.0591 - acc: 0.9823 - val_loss: 4.7803 - val_acc: 0.3631\n",
      "Epoch 97/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.0577 - acc: 0.9795 - val_loss: 4.8381 - val_acc: 0.3185\n",
      "Epoch 98/100\n",
      "1413/1413 [==============================] - 5s - loss: 0.0531 - acc: 0.9823 - val_loss: 4.8868 - val_acc: 0.2994\n",
      "Epoch 99/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.0634 - acc: 0.9774 - val_loss: 4.7720 - val_acc: 0.3567\n",
      "Epoch 100/100\n",
      "1413/1413 [==============================] - 4s - loss: 0.0548 - acc: 0.9802 - val_loss: 4.8976 - val_acc: 0.3312\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fceea5ffd50>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 50\n",
    "\n",
    "decoder.fit([X_val1, X_val2], y_val, batch_size=batch_size, nb_epoch=100, show_accuracy=True, \n",
    "            validation_split = 0.1, shuffle=True, \n",
    "#                    callbacks=[early_stopping]\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predict score of 2015 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "All input arrays and the target array must have the same number of samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-114-f056a2cd9e2f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m score, acc = decoder.evaluate([X_test1_f, X_test2_f], y_test,\n\u001b[0;32m      2\u001b[0m                             \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m                             show_accuracy=True)\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Test score:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Test accuracy:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/keras/models.pyc\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, X, y, batch_size, show_accuracy, verbose, sample_weight)\u001b[0m\n\u001b[0;32m    765\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 767\u001b[1;33m                 raise Exception('All input arrays and the target array must '\n\u001b[0m\u001b[0;32m    768\u001b[0m                                 'have the same number of samples.')\n\u001b[0;32m    769\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: All input arrays and the target array must have the same number of samples."
     ]
    }
   ],
   "source": [
    "score, acc = decoder.evaluate([X_test1_f, X_test2_f], y_test,\n",
    "                            batch_size=batch_size,\n",
    "                            show_accuracy=True)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def writeRes(dst, res):\n",
    "    with open(dst, 'w') as thefile:\n",
    "        thefile.write(\"\\n\".join(str(i) for i in res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 0s     \n"
     ]
    }
   ],
   "source": [
    "res = decoder.predict_classes([X_test1_f, X_test2_f]) \n",
    "# res = [r[0] for r in res]\n",
    "# writeRes('./dataset/STS2015-test/sys.forum', res)\n",
    "np.savetxt('./dataset/STS2015-test/sys.forum', res, newline='\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
