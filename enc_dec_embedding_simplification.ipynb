{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GeForce GTX 980 Ti (CNMeM is disabled, CuDNN 4007)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM, GRU\n",
    "from keras.preprocessing import text\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import EarlyStopping\n",
    "from __future__ import print_function\n",
    "from keras.layers.core import Activation, TimeDistributedDense, RepeatVector\n",
    "from keras.layers import recurrent\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done loading\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "wv = Word2Vec.load_word2vec_format(\"/home/tong/Documents/python/GoogleNews-vectors-negative300.bin.gz\", binary = True)\n",
    "print(\"done\" + \" loading\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CharacterTable(object):\n",
    "    '''\n",
    "        Given a set of characters:\n",
    "        + Encode them to a one hot integer representation\n",
    "        + Decode the one hot integer representation to their character output\n",
    "        + Decode a vector of probabilties to their character output\n",
    "        '''\n",
    "    def __init__(self, vocab, maxlen, wv):\n",
    "        self.vocab = vocab\n",
    "        self.char_indices = dict((c, i) for i, c in enumerate(self.vocab))\n",
    "        self.indices_char = dict((i, c) for i, c in enumerate(self.vocab))\n",
    "        self.maxlen = maxlen\n",
    "        self.wv = wv\n",
    "        self.embedding = {}\n",
    "        for i, c in enumerate(self.vocab):\n",
    "            if c in wv:\n",
    "                self.embedding[c] = wv[c]\n",
    "            else:\n",
    "                self.embedding[c] = np.random.rand(300)\n",
    "    \n",
    "    def encode_onehot(self, C, maxlen=None):\n",
    "        maxlen = maxlen if maxlen else self.maxlen\n",
    "        X = np.zeros((maxlen, len(self.vocab)))\n",
    "        for i, c in enumerate(C):\n",
    "            try:\n",
    "                X[i, self.char_indices[c]] = 1\n",
    "            except KeyError:\n",
    "                X[i, self.char_indices[' ']] = 1\n",
    "        return X\n",
    "    \n",
    "    def encode(self, C, maxlen=None):\n",
    "        maxlen = maxlen if maxlen else self.maxlen\n",
    "        X = np.zeros((maxlen, 300))\n",
    "        for i, c in enumerate(C):\n",
    "            try:\n",
    "                X[i] = self.embedding[c]\n",
    "            except KeyError:\n",
    "                X[i] = np.random.rand(300)\n",
    "        return X\n",
    "    \n",
    "    def decode(self, X, calc_argmax=True):\n",
    "        if calc_argmax:\n",
    "            X = X.argmax(axis=-1)\n",
    "        return ' '.join(self.indices_char[x] for x in X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "token_pattern=r\"(?u)\\b\\w\\w+\\b\"\n",
    "def build_tokenizer():\n",
    "    \"\"\"Return a function that splits a string into a sequence of tokens\"\"\"\n",
    "    pattern = re.compile(token_pattern)\n",
    "    return lambda doc: pattern.findall(doc)\n",
    "\n",
    "\n",
    "def readData(src):\n",
    "    b1 = []\n",
    "    with open(src) as p:\n",
    "        for i, line in enumerate(p):\n",
    "            s = line.split('\\t')\n",
    "            b1.append(s[2].strip())\n",
    "            lines = i + 1\n",
    "    return b1, lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b1, lines = readData('./dataset/normal.aligned')\n",
    "b2, lines = readData('./dataset/simple.aligned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "167689\n"
     ]
    }
   ],
   "source": [
    "print (len(b1) == len(b2))\n",
    "print (lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is the county seat of Alfalfa County .\n",
      "27849\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "27849"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize = build_tokenizer()\n",
    "indices = [index for index, s in enumerate(b1) if len(tokenize(s)) > 15 and len(tokenize(s)) < 20]\n",
    "print(b1[0])\n",
    "print(len(indices))\n",
    "b1 = [b1[i] for i in indices]\n",
    "b2 = [b2[i] for i in indices]\n",
    "len(b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55698, 20271)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english', min_df = 3)\n",
    "vectors = vectorizer.fit_transform(b1 + b2)\n",
    "print(vectors.shape)\n",
    "vocab = vectorizer.get_feature_names()\n",
    "len(vocab)\n",
    "vocab.append(' ') #add empty word for padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAXLEN = 19\n",
    "b1 = [x.lower() for x in b1]\n",
    "b2 = [x.lower() for x in b2]\n",
    "b1_tokens = [tokenize(x)[:MAXLEN] for x in b1]\n",
    "b2_tokens = [tokenize(x)[:MAXLEN] for x in b2]\n",
    "#padding\n",
    "b1_tokens = [s + [' '] * (MAXLEN - len(s)) for s in b1_tokens]\n",
    "b2_tokens = [s + [' '] * (MAXLEN - len(s)) for s in b2_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ctable = CharacterTable(vocab, MAXLEN, wv)\n",
    "X = np.zeros((len(b1), MAXLEN, 300), dtype=np.float)\n",
    "y = np.zeros((len(b1), MAXLEN, len(vocab)), dtype=np.bool)\n",
    "for i, sentence in enumerate(b1_tokens):\n",
    "    X[i] = ctable.encode(sentence, maxlen=MAXLEN)\n",
    "\n",
    "for i, sentence in enumerate(b2_tokens):\n",
    "    y[i] = ctable.encode_onehot(sentence, maxlen=MAXLEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 256\n",
    "BATCH_SIZE = 100\n",
    "LAYERS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "# \"Encode\" the input sequence using an RNN, producing an output of HIDDEN_SIZE\n",
    "# note: in a situation where your input sequences have a variable length,\n",
    "# use input_shape=(None, nb_feature).\n",
    "model.add(LSTM(HIDDEN_SIZE, dropout_W=0.1, dropout_U=0.1, input_shape=(MAXLEN, 300), return_sequences=True))\n",
    "for _ in range(LAYERS - 2):\n",
    "    model.add(LSTM(HIDDEN_SIZE, dropout_W=0.1, dropout_U=0.1, return_sequences=True))\n",
    "model.add(LSTM(HIDDEN_SIZE, dropout_W=0.1, dropout_U=0.1))\n",
    "# For the decoder's input, we repeat the encoded input for each time step\n",
    "# We use repeatvector here because we only need the last state, not the whole sequence\n",
    "model.add(RepeatVector(MAXLEN))\n",
    "# The decoder RNN could be multiple layers stacked or a single layer\n",
    "for _ in range(LAYERS):\n",
    "    model.add(LSTM(HIDDEN_SIZE, dropout_W=0.1, dropout_U=0.1, return_sequences=True))\n",
    "\n",
    "# For each of step of the output sequence, decide which character should be chosen\n",
    "model.add(TimeDistributedDense(300))\n",
    "model.add(Activation('tanh'))\n",
    "model.add(TimeDistributedDense(len(vocab)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adagrad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25064 samples, validate on 2785 samples\n",
      "Epoch 1/50\n",
      "25064/25064 [==============================] - 453s - loss: 4.7042 - acc: 0.5461 - val_loss: 4.6500 - val_acc: 0.5448\n",
      "Epoch 2/50\n",
      "25064/25064 [==============================] - 438s - loss: 4.5547 - acc: 0.5495 - val_loss: 4.6083 - val_acc: 0.5448\n",
      "Epoch 3/50\n",
      "25064/25064 [==============================] - 523s - loss: 4.5172 - acc: 0.5495 - val_loss: 4.6000 - val_acc: 0.5448\n",
      "Epoch 4/50\n",
      "25064/25064 [==============================] - 422s - loss: 4.4947 - acc: 0.5495 - val_loss: 4.5941 - val_acc: 0.5448\n",
      "Epoch 5/50\n",
      "25064/25064 [==============================] - 476s - loss: 4.4774 - acc: 0.5495 - val_loss: 4.5883 - val_acc: 0.5448\n",
      "Epoch 6/50\n",
      "25064/25064 [==============================] - 404s - loss: 4.4632 - acc: 0.5495 - val_loss: 4.5876 - val_acc: 0.5448\n",
      "Epoch 7/50\n",
      "25064/25064 [==============================] - 476s - loss: 4.4511 - acc: 0.5495 - val_loss: 4.5867 - val_acc: 0.5448\n",
      "Epoch 8/50\n",
      "25064/25064 [==============================] - 503s - loss: 4.4402 - acc: 0.5495 - val_loss: 4.5894 - val_acc: 0.5448\n",
      "Epoch 9/50\n",
      "25064/25064 [==============================] - 524s - loss: 4.4302 - acc: 0.5495 - val_loss: 4.5859 - val_acc: 0.5448\n",
      "Epoch 10/50\n",
      "25064/25064 [==============================] - 437s - loss: 4.4214 - acc: 0.5495 - val_loss: 4.5877 - val_acc: 0.5448\n",
      "Epoch 11/50\n",
      "25064/25064 [==============================] - 535s - loss: 4.4140 - acc: 0.5495 - val_loss: 4.5880 - val_acc: 0.5448\n",
      "Epoch 12/50\n",
      "25064/25064 [==============================] - 414s - loss: 4.4068 - acc: 0.5495 - val_loss: 4.5916 - val_acc: 0.5448\n",
      "Epoch 13/50\n",
      "25064/25064 [==============================] - 400s - loss: 4.4003 - acc: 0.5495 - val_loss: 4.5925 - val_acc: 0.5448\n",
      "Epoch 14/50\n",
      "25064/25064 [==============================] - 401s - loss: 4.3940 - acc: 0.5495 - val_loss: 4.5937 - val_acc: 0.5448\n",
      "Epoch 15/50\n",
      "25064/25064 [==============================] - 402s - loss: 4.3883 - acc: 0.5495 - val_loss: 4.5957 - val_acc: 0.5448\n",
      "Epoch 16/50\n",
      "25064/25064 [==============================] - 402s - loss: 4.3832 - acc: 0.5495 - val_loss: 4.5982 - val_acc: 0.5448\n",
      "Epoch 17/50\n",
      "25064/25064 [==============================] - 401s - loss: 4.3781 - acc: 0.5495 - val_loss: 4.6013 - val_acc: 0.5448\n",
      "Epoch 18/50\n",
      "25064/25064 [==============================] - 401s - loss: 4.3735 - acc: 0.5495 - val_loss: 4.6038 - val_acc: 0.5448\n",
      "Epoch 19/50\n",
      "25064/25064 [==============================] - 399s - loss: 4.3691 - acc: 0.5495 - val_loss: 4.6034 - val_acc: 0.5448\n",
      "Epoch 20/50\n",
      "25064/25064 [==============================] - 401s - loss: 4.3648 - acc: 0.5495 - val_loss: 4.6097 - val_acc: 0.5448\n",
      "Epoch 21/50\n",
      "25064/25064 [==============================] - 400s - loss: 4.3607 - acc: 0.5495 - val_loss: 4.6080 - val_acc: 0.5448\n",
      "Epoch 22/50\n",
      "25064/25064 [==============================] - 401s - loss: 4.3565 - acc: 0.5495 - val_loss: 4.6118 - val_acc: 0.5448\n",
      "Epoch 23/50\n",
      "25064/25064 [==============================] - 399s - loss: 4.3526 - acc: 0.5495 - val_loss: 4.6130 - val_acc: 0.5448\n",
      "Epoch 24/50\n",
      "25064/25064 [==============================] - 399s - loss: 4.3489 - acc: 0.5495 - val_loss: 4.6151 - val_acc: 0.5448\n",
      "Epoch 25/50\n",
      "25064/25064 [==============================] - 400s - loss: 4.3454 - acc: 0.5495 - val_loss: 4.6171 - val_acc: 0.5448\n",
      "Epoch 26/50\n",
      "25064/25064 [==============================] - 400s - loss: 4.3418 - acc: 0.5495 - val_loss: 4.6204 - val_acc: 0.5448\n",
      "Epoch 27/50\n",
      "25064/25064 [==============================] - 399s - loss: 4.3387 - acc: 0.5495 - val_loss: 4.6237 - val_acc: 0.5448\n",
      "Epoch 28/50\n",
      "25064/25064 [==============================] - 400s - loss: 4.3356 - acc: 0.5495 - val_loss: 4.6246 - val_acc: 0.5448\n",
      "Epoch 29/50\n",
      "25064/25064 [==============================] - 399s - loss: 4.3323 - acc: 0.5495 - val_loss: 4.6272 - val_acc: 0.5448\n",
      "Epoch 30/50\n",
      "25064/25064 [==============================] - 399s - loss: 4.3293 - acc: 0.5495 - val_loss: 4.6290 - val_acc: 0.5448\n",
      "Epoch 31/50\n",
      "25064/25064 [==============================] - 399s - loss: 4.3265 - acc: 0.5495 - val_loss: 4.6299 - val_acc: 0.5448\n",
      "Epoch 32/50\n",
      "25064/25064 [==============================] - 399s - loss: 4.3236 - acc: 0.5495 - val_loss: 4.6319 - val_acc: 0.5448\n",
      "Epoch 33/50\n",
      "25064/25064 [==============================] - 399s - loss: 4.3208 - acc: 0.5495 - val_loss: 4.6337 - val_acc: 0.5448\n",
      "Epoch 34/50\n",
      "25064/25064 [==============================] - 399s - loss: 4.3181 - acc: 0.5495 - val_loss: 4.6358 - val_acc: 0.5448\n",
      "Epoch 35/50\n",
      "25064/25064 [==============================] - 400s - loss: 4.3156 - acc: 0.5495 - val_loss: 4.6383 - val_acc: 0.5448\n",
      "Epoch 36/50\n",
      "25064/25064 [==============================] - 399s - loss: 4.3133 - acc: 0.5495 - val_loss: 4.6402 - val_acc: 0.5448\n",
      "Epoch 37/50\n",
      "25064/25064 [==============================] - 399s - loss: 4.3107 - acc: 0.5495 - val_loss: 4.6414 - val_acc: 0.5448\n",
      "Epoch 38/50\n",
      "25064/25064 [==============================] - 400s - loss: 4.3084 - acc: 0.5495 - val_loss: 4.6429 - val_acc: 0.5448\n",
      "Epoch 39/50\n",
      "25064/25064 [==============================] - 399s - loss: 4.3062 - acc: 0.5495 - val_loss: 4.6445 - val_acc: 0.5448\n",
      "Epoch 40/50\n",
      "25064/25064 [==============================] - 400s - loss: 4.3039 - acc: 0.5495 - val_loss: 4.6465 - val_acc: 0.5448\n",
      "Epoch 41/50\n",
      "25064/25064 [==============================] - 400s - loss: 4.3016 - acc: 0.5495 - val_loss: 4.6484 - val_acc: 0.5448\n",
      "Epoch 42/50\n",
      "25064/25064 [==============================] - 399s - loss: 4.2997 - acc: 0.5495 - val_loss: 4.6499 - val_acc: 0.5448\n",
      "Epoch 43/50\n",
      "25064/25064 [==============================] - 399s - loss: 4.2976 - acc: 0.5495 - val_loss: 4.6520 - val_acc: 0.5448\n",
      "Epoch 44/50\n",
      "25064/25064 [==============================] - 400s - loss: 4.2957 - acc: 0.5495 - val_loss: 4.6531 - val_acc: 0.5448\n",
      "Epoch 45/50\n",
      "25064/25064 [==============================] - 403s - loss: 4.2937 - acc: 0.5495 - val_loss: 4.6550 - val_acc: 0.5448\n",
      "Epoch 46/50\n",
      "25064/25064 [==============================] - 429s - loss: 4.2918 - acc: 0.5495 - val_loss: 4.6567 - val_acc: 0.5448\n",
      "Epoch 47/50\n",
      "25064/25064 [==============================] - 437s - loss: 4.2900 - acc: 0.5495 - val_loss: 4.6581 - val_acc: 0.5448\n",
      "Epoch 48/50\n",
      "25064/25064 [==============================] - 452s - loss: 4.2882 - acc: 0.5495 - val_loss: 4.6594 - val_acc: 0.5448\n",
      "Epoch 49/50\n",
      "25064/25064 [==============================] - 462s - loss: 4.2864 - acc: 0.5495 - val_loss: 4.6616 - val_acc: 0.5448\n",
      "Epoch 50/50\n",
      "25064/25064 [==============================] - 476s - loss: 4.2847 - acc: 0.5495 - val_loss: 4.6628 - val_acc: 0.5448\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(X, y, batch_size=BATCH_SIZE, nb_epoch=50,\n",
    "          show_accuracy=True,validation_split = 0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "axes = plt.gca()\n",
    "x_min = hist.epoch[0]\n",
    "x_max = hist.epoch[-1]+1\n",
    "axes.set_xlim([x_min,x_max])\n",
    "\n",
    "plt.scatter(hist.epoch, hist.history['loss'], color='g')\n",
    "plt.plot(hist.history['loss'], color='g', label='Training Loss')\n",
    "plt.scatter(hist.epoch, hist.history['val_loss'], color='b')\n",
    "plt.plot(hist.history['val_loss'], color='b', label='Validation Loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss & Validation Loss vs Epochs')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure(2)\n",
    "\n",
    "axes = plt.gca()\n",
    "x_min = hist.epoch[0]\n",
    "x_max = hist.epoch[-1]+1\n",
    "axes.set_xlim([x_min,x_max])\n",
    "\n",
    "plt.scatter(hist.epoch, hist.history['acc'], color='r')\n",
    "plt.plot(hist.history['acc'], color='r', label='Training Accuracy')\n",
    "plt.scatter(hist.epoch, hist.history['val_acc'], color='c')\n",
    "plt.plot(hist.history['val_acc'], color='c', label='Validation Accuracy')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Trainging Accuracy & Validation Accuracy vs Epochs')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s\n",
      "19\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' '],\n",
       " [' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' '],\n",
       " [' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' '],\n",
       " [' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' '],\n",
       " [' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' '],\n",
       " [' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' '],\n",
       " [' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' '],\n",
       " [' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' '],\n",
       " [' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' '],\n",
       " [' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ']]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = model.predict_classes(X[:10])\n",
    "res_sentences = []\n",
    "for r in res:\n",
    "    sent = []\n",
    "    for i in range(MAXLEN):\n",
    "        sent.append(ctable.indices_char[r[i]])\n",
    "    res_sentences.append(sent)\n",
    "\n",
    "print(len(res_sentences[0]))\n",
    "res_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "json_string = model.to_json()\n",
    "open('my_model_architecture.json', 'w').write(json_string)\n",
    "model.save_weights('my_model_weights.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
