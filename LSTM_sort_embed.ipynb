{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM, GRU\n",
    "from keras.preprocessing import text\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import EarlyStopping\n",
    "from __future__ import print_function\n",
    "from keras.layers.core import Activation, TimeDistributedDense, RepeatVector\n",
    "from keras.layers import recurrent\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CharacterTable(object):\n",
    "    def __init__(self, vocab, maxlen):\n",
    "        self.vocab = vocab\n",
    "        self.maxlen = maxlen\n",
    "    \n",
    "    def encode(self, C, maxlen=None):\n",
    "        maxlen = maxlen if maxlen else self.maxlen\n",
    "        X = np.zeros((maxlen, len(self.vocab)))\n",
    "        for i, c in enumerate(C):\n",
    "            X[i, c] = 1\n",
    "        return X\n",
    "    \n",
    "    def decode(self, X, calc_argmax=True):\n",
    "        if calc_argmax:\n",
    "            X = X.argmax(axis=-1)\n",
    "        return ','.join(x for x in X)\n",
    "    \n",
    "def generateRandSeq(min, max, len):\n",
    "    return [np.random.randint(min, max) for _ in range(len)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TRAINING_SIZE = 150000\n",
    "TEST_SIZE = 10000\n",
    "DIGITS = 25\n",
    "MAXLEN = DIGITS\n",
    "voc = list(xrange(1000))\n",
    "ctable = CharacterTable(voc, MAXLEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating data...\n",
      "[86, 636, 674, 750, 775, 618, 474, 596, 65, 168, 965, 975, 19, 999, 236, 57, 80, 29, 984, 468, 600, 624, 494, 538, 688]\n",
      "[19, 29, 57, 65, 80, 86, 168, 236, 468, 474, 494, 538, 596, 600, 618, 624, 636, 674, 688, 750, 775, 965, 975, 984, 999]\n"
     ]
    }
   ],
   "source": [
    "inputs = []\n",
    "outputs = []\n",
    "inputs_t = []\n",
    "outputs_t = []\n",
    "print('Generating data...')\n",
    "while len(inputs) < TRAINING_SIZE:\n",
    "    s = generateRandSeq(0, len(voc), DIGITS)\n",
    "    inputs.append(s)\n",
    "    # outputs.append(s[::-1])\n",
    "    outputs.append(sorted(s))\n",
    "\n",
    "while len(inputs_t) < TEST_SIZE:\n",
    "    s = generateRandSeq(0, len(voc), DIGITS)\n",
    "    inputs_t.append(s)\n",
    "    # outputs_t.append(s[::-1])\n",
    "    outputs_t.append(sorted(s))\n",
    "print(inputs[12])\n",
    "print(outputs[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization...\n",
      "(150000, 25)\n",
      "(150000, 25, 1000)\n",
      "(10000, 25)\n",
      "(10000, 25, 1000)\n"
     ]
    }
   ],
   "source": [
    "print('Vectorization...')\n",
    "X = np.zeros((len(inputs), MAXLEN), dtype=np.int32)\n",
    "# y = np.zeros((len(outputs), MAXLEN), dtype=np.int32)\n",
    "y = np.zeros((len(outputs), MAXLEN, len(voc)), dtype=np.bool)\n",
    "for i, sentence in enumerate(inputs):\n",
    "    X[i] = inputs[i]\n",
    "\n",
    "# for i, sentence in enumerate(outputs):\n",
    "#     y[i] = outputs[i]\n",
    "for i, sentence in enumerate(outputs):\n",
    "    y[i] = ctable.encode(sentence, maxlen=MAXLEN)\n",
    "\n",
    "X_test = np.zeros((len(inputs_t), MAXLEN), dtype=np.int32)\n",
    "# y_test = np.zeros((len(outputs_t), MAXLEN), dtype=np.int32)\n",
    "y_test = np.zeros((len(outputs_t), MAXLEN, len(voc)), dtype=np.bool)\n",
    "for i, sentence in enumerate(inputs_t):\n",
    "    X_test[i] = inputs_t[i]\n",
    "\n",
    "# for i, sentence in enumerate(outputs_t):\n",
    "#     y_test[i] = outputs_t[i]\n",
    "for i, sentence in enumerate(outputs_t):\n",
    "    y_test[i] = ctable.encode(sentence, maxlen=MAXLEN)\n",
    "    \n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "HIDDEN_SIZE = 256\n",
    "BATCH_SIZE = 200\n",
    "LAYERS = 2\n",
    "'''\n",
    "Hey guys, I also met this problem and I found this thread. Basically, \n",
    "the error info can happen when the dimension of the input data (X_train or Y_train) doesn't match with the \n",
    "model's input shape.\n",
    "\n",
    "In my case (and @LeavesBreathe 's case I guess), the problem is that \n",
    "the model is expecting the Y_train to be a 3d tensor. Because of the embedding layer, \n",
    "the 2d tensor X_train of size (n_batch, sequence_length) will be eventually converted to a 3d tensor of size \n",
    "(n_batch, sequence_length, embedding_size) and will be processed by the succeeding LSTM layer. However, \n",
    "the 2d tensor Y_train of size (n_sample, sequence_length) is not converted to 3d, \n",
    "which is needed by the decoder LSTM.\n",
    "\n",
    "To fix this problem, what I did is to convert Y_train into a 3d binary tensor (binary one-hot coding) and it worked.\n",
    "'''\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(voc), 300, input_length = MAXLEN))\n",
    "model.add(LSTM(HIDDEN_SIZE, return_sequences=True))\n",
    "for _ in range(LAYERS - 2):\n",
    "    model.add(LSTM(HIDDEN_SIZE, return_sequences=True))\n",
    "\n",
    "model.add(LSTM(HIDDEN_SIZE))\n",
    "model.add(RepeatVector(MAXLEN))\n",
    "for _ in range(LAYERS):\n",
    "    model.add(LSTM(HIDDEN_SIZE, return_sequences=True))\n",
    "\n",
    "model.add(TimeDistributedDense(input_dim=HIDDEN_SIZE, output_dim=300))\n",
    "model.add(Activation('tanh'))\n",
    "model.add(TimeDistributedDense(input_dim=300, output_dim=len(voc)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(optimizer='RMSprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 135000 samples, validate on 15000 samples\n",
      "Epoch 1/50\n",
      "135000/135000 [==============================] - 344s - loss: 0.3989 - acc: 0.8561 - val_loss: 0.7429 - val_acc: 0.7584\n",
      "Epoch 2/50\n",
      "135000/135000 [==============================] - 346s - loss: 0.3917 - acc: 0.8586 - val_loss: 0.6927 - val_acc: 0.7695\n",
      "Epoch 3/50\n",
      "135000/135000 [==============================] - 347s - loss: 0.3840 - acc: 0.8610 - val_loss: 0.7462 - val_acc: 0.7602\n",
      "Epoch 4/50\n",
      "135000/135000 [==============================] - 314s - loss: 0.3772 - acc: 0.8633 - val_loss: 0.7306 - val_acc: 0.7642\n",
      "Epoch 5/50\n",
      "135000/135000 [==============================] - 374s - loss: 0.3719 - acc: 0.8659 - val_loss: 0.6538 - val_acc: 0.7814\n",
      "Epoch 6/50\n",
      "135000/135000 [==============================] - 375s - loss: 0.3643 - acc: 0.8683 - val_loss: 0.6740 - val_acc: 0.7783\n",
      "Epoch 7/50\n",
      "135000/135000 [==============================] - 375s - loss: 0.3585 - acc: 0.8703 - val_loss: 0.6755 - val_acc: 0.7765\n",
      "Epoch 8/50\n",
      "135000/135000 [==============================] - 374s - loss: 0.3520 - acc: 0.8729 - val_loss: 0.6586 - val_acc: 0.7831\n",
      "Epoch 9/50\n",
      "135000/135000 [==============================] - 374s - loss: 0.3456 - acc: 0.8752 - val_loss: 0.7156 - val_acc: 0.7725\n",
      "Epoch 10/50\n",
      "135000/135000 [==============================] - 374s - loss: 0.3404 - acc: 0.8774 - val_loss: 0.6814 - val_acc: 0.7811\n",
      "Epoch 11/50\n",
      "135000/135000 [==============================] - 374s - loss: 0.3329 - acc: 0.8792 - val_loss: 0.6708 - val_acc: 0.7829\n",
      "Epoch 12/50\n",
      "135000/135000 [==============================] - 374s - loss: 0.3280 - acc: 0.8813 - val_loss: 0.6510 - val_acc: 0.7894\n",
      "Epoch 13/50\n",
      "135000/135000 [==============================] - 374s - loss: 0.3229 - acc: 0.8835 - val_loss: 0.6582 - val_acc: 0.7878\n",
      "Epoch 14/50\n",
      "135000/135000 [==============================] - 374s - loss: 0.3182 - acc: 0.8850 - val_loss: 0.6494 - val_acc: 0.7912\n",
      "Epoch 15/50\n",
      "135000/135000 [==============================] - 375s - loss: 0.3117 - acc: 0.8874 - val_loss: 0.6571 - val_acc: 0.7905\n",
      "Epoch 16/50\n",
      "135000/135000 [==============================] - 374s - loss: 0.3077 - acc: 0.8894 - val_loss: 0.7217 - val_acc: 0.7765\n",
      "Epoch 17/50\n",
      "135000/135000 [==============================] - 375s - loss: 0.3030 - acc: 0.8911 - val_loss: 0.6773 - val_acc: 0.7866\n",
      "Epoch 18/50\n",
      "135000/135000 [==============================] - 374s - loss: 0.2993 - acc: 0.8929 - val_loss: 0.6765 - val_acc: 0.7872\n",
      "Epoch 19/50\n",
      "135000/135000 [==============================] - 374s - loss: 0.2932 - acc: 0.8944 - val_loss: 0.6420 - val_acc: 0.7966\n",
      "Epoch 20/50\n",
      "135000/135000 [==============================] - 374s - loss: 0.2887 - acc: 0.8961 - val_loss: 0.6617 - val_acc: 0.7933\n",
      "Epoch 21/50\n",
      "135000/135000 [==============================] - 374s - loss: 0.2835 - acc: 0.8981 - val_loss: 0.6858 - val_acc: 0.7894\n",
      "Epoch 22/50\n",
      "135000/135000 [==============================] - 374s - loss: 0.2804 - acc: 0.8989 - val_loss: 0.6557 - val_acc: 0.7959\n",
      "Epoch 23/50\n",
      "135000/135000 [==============================] - 374s - loss: 0.2755 - acc: 0.9010 - val_loss: 0.6453 - val_acc: 0.8000\n",
      "Epoch 24/50\n",
      "135000/135000 [==============================] - 374s - loss: 0.2716 - acc: 0.9019 - val_loss: 0.6686 - val_acc: 0.7956\n",
      "Epoch 25/50\n",
      "135000/135000 [==============================] - 375s - loss: 0.2676 - acc: 0.9037 - val_loss: 0.6911 - val_acc: 0.7904\n",
      "Epoch 26/50\n",
      "135000/135000 [==============================] - 374s - loss: 0.2630 - acc: 0.9052 - val_loss: 0.6802 - val_acc: 0.7964\n",
      "Epoch 27/50\n",
      "135000/135000 [==============================] - 374s - loss: 0.2595 - acc: 0.9069 - val_loss: 0.6757 - val_acc: 0.7958\n",
      "10000/10000 [==============================] - 11s    \n",
      "Test score: 0.670910172462\n",
      "Test accuracy: 0.797108012438\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=7)\n",
    "hist = model.fit(X, y, batch_size=BATCH_SIZE, nb_epoch=50, \n",
    "                 callbacks=[early_stopping],\n",
    "          validation_split = 0.1, shuffle=True)\n",
    "\n",
    "score, acc = model.evaluate(X_test, y_test,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            show_accuracy=True)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/keras/models.py:433: UserWarning: The \"show_accuracy\" argument is deprecated, instead you should pass the \"accuracy\" metric to the model at compile time:\n",
      "`model.compile(optimizer, loss, metrics=[\"accuracy\"])`\n",
      "  warnings.warn('The \"show_accuracy\" argument is deprecated, '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 16s    \n",
      "Test score: 0.0201964445598\n",
      "Test accuracy: 0.993400006294\n"
     ]
    }
   ],
   "source": [
    "score, acc = model.evaluate(X_test, y_test,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            show_accuracy=True)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "json_string = model.to_json()\n",
    "open('sortemd_100_256_150k_model.json', 'w').write(json_string)\n",
    "model.save_weights('sortemd_100_256_150k_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "model = model_from_json(open('sortemd_100_256_150k_model.json').read())\n",
    "model.load_weights('sortemd_100_256_150k_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 135000 samples, validate on 15000 samples\n",
      "Epoch 1/2\n",
      "  2600/135000 [..............................] - ETA: 701s - loss: 0.1208 - acc: 0.9708"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-8e4940d83eaf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m hist = model2.fit(X, y, batch_size=BATCH_SIZE, nb_epoch=2, \n\u001b[0;32m     10\u001b[0m                  \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m           validation_split = 0.1, shuffle=True)\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m score, acc = model2.evaluate(X_test, y_test,\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, **kwargs)\u001b[0m\n\u001b[0;32m    407\u001b[0m                               \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    408\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 409\u001b[1;33m                               sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m    410\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    411\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight)\u001b[0m\n\u001b[0;32m   1050\u001b[0m                               \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1051\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1052\u001b[1;33m                               callback_metrics=callback_metrics)\n\u001b[0m\u001b[0;32m   1053\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1054\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, nb_epoch, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics)\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 790\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    791\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    792\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/keras/backend/theano_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    516\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    517\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 518\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    519\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/theano/scan_module/scan_op.pyc\u001b[0m in \u001b[0;36mrval\u001b[1;34m(p, i, o, n, allow_gc)\u001b[0m\n\u001b[0;32m    949\u001b[0m         def rval(p=p, i=node_input_storage, o=node_output_storage, n=node,\n\u001b[0;32m    950\u001b[0m                  allow_gc=allow_gc):\n\u001b[1;32m--> 951\u001b[1;33m             \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    952\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    953\u001b[0m                 \u001b[0mcompute_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/theano/scan_module/scan_op.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(node, args, outs)\u001b[0m\n\u001b[0;32m    938\u001b[0m                         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    939\u001b[0m                         \u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 940\u001b[1;33m                         self, node)\n\u001b[0m\u001b[0;32m    941\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mImportError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgof\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMissingGXX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    942\u001b[0m             \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model2.compile(optimizer='RMSprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "hist = model2.fit(X, y, batch_size=BATCH_SIZE, nb_epoch=2, \n",
    "                 callbacks=[early_stopping],\n",
    "          validation_split = 0.1, shuffle=True)\n",
    "\n",
    "score, acc = model2.evaluate(X_test, y_test,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            show_accuracy=True)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "def get_activations(model, layer, X_batch):\n",
    "    get_activations = K.function([model.layers[0].input, K.learning_phase()], [model.layers[layer].output,])\n",
    "    activations = get_activations([X_batch,0])\n",
    "    return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "        17, 18, 19, 20, 21, 22, 23, 24],\n",
       "       [25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41,\n",
       "        42, 43, 44, 45, 46, 47, 48, 49],\n",
       "       [50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66,\n",
       "        67, 68, 69, 70, 71, 72, 73, 74],\n",
       "       [75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91,\n",
       "        92, 93, 94, 95, 96, 97, 98, 99]], dtype=int32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_voc = np.zeros((4, 25), dtype=np.int32)\n",
    "X_voc[0] = range(0,25)\n",
    "X_voc[1] = range(25,50)\n",
    "X_voc[2] = range(50,75)\n",
    "X_voc[3] = range(75,100)\n",
    "X_voc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.layers.embeddings.Embedding object at 0x7f926f392250>\n",
      "Reshape{3}.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/keras/backend/theano_backend.py:514: UserWarning: theano.function was asked to create a function computing outputs given certain inputs, but the provided input variable at index 1 is not part of the computational graph needed to compute the outputs: keras_learning_phase.\n",
      "To make this warning into an error, you can pass the parameter on_unused_input='raise' to theano.function. To disable it completely, use on_unused_input='ignore'.\n",
      "  **kwargs)\n"
     ]
    }
   ],
   "source": [
    "print(model.layers[0])\n",
    "print(model.layers[0].output)\n",
    "embeddings = get_activations(model2, 0, X_voc)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embed = np.zeros((100, 300))\n",
    "for i in range(25):\n",
    "    embed[i] = embeddings[0][i]\n",
    "    embed[i+25] = embeddings[1][i]\n",
    "    embed[i+50] = embeddings[2][i]\n",
    "    embed[i+75] = embeddings[3][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn import decomposition\n",
    "from sklearn import datasets\n",
    "\n",
    "pca = decomposition.PCA(n_components=2)\n",
    "pca.fit(embed[:40])\n",
    "V = pca.transform(embed[:40])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plt.plot(V[:,0], V[:,1], \"o\")\n",
    "plt.axis([-1.2,1.2,-1.2,1.2])\n",
    "for i in range(0,V.shape[0],4):                                      \n",
    "    ax.annotate(i, xy=V[i], textcoords='data')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.60725524]\n",
      " [-0.5478629 ]\n",
      " [-0.64999643]\n",
      " [-0.59897736]\n",
      " [-0.5413074 ]\n",
      " [-0.64889847]\n",
      " [-0.61972691]\n",
      " [-0.58120159]\n",
      " [-0.56387784]\n",
      " [-0.58660659]\n",
      " [-0.56847293]\n",
      " [-0.54236413]\n",
      " [-0.52597181]\n",
      " [-0.4776906 ]\n",
      " [-0.44901819]\n",
      " [-0.46480098]\n",
      " [-0.37702839]\n",
      " [-0.28938999]\n",
      " [-0.19230272]\n",
      " [-0.15344832]\n",
      " [-0.16317479]\n",
      " [-0.10045423]\n",
      " [-0.00275577]\n",
      " [ 0.06050427]\n",
      " [ 0.1355235 ]\n",
      " [ 0.14400794]\n",
      " [ 0.31399033]\n",
      " [ 0.43521893]\n",
      " [ 0.46121149]\n",
      " [ 0.6210367 ]\n",
      " [ 0.67228949]\n",
      " [ 0.63967616]\n",
      " [ 0.76844663]\n",
      " [ 0.88829104]\n",
      " [ 0.90041505]\n",
      " [ 0.92397774]\n",
      " [ 0.90445206]\n",
      " [ 0.88113638]\n",
      " [ 0.85624798]\n",
      " [ 0.64615786]]\n"
     ]
    }
   ],
   "source": [
    "pca = decomposition.PCA(n_components=1)\n",
    "pca.fit(embed[:40])\n",
    "V = pca.transform(embed[:40])\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plt.plot(V, [0]*40, \"o\")\n",
    "plt.axis([-1.2,1.2,-0.3,0.3])\n",
    "for i in range(0,40,4):                                      \n",
    "    ax.annotate(i, xy=(V[i],0.001), textcoords='data')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
